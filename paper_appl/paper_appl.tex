\documentclass[12pt]{article}

% TODO: Are we, in this setting, immune to confounders?  We are removing
%       all bi-directional edges, so it seems likely that we are?  Can we state
%       any theorem about this?

%% bibliography stuff -- this needs come before the preamble inclusion
\usepackage[backend=bibtex,sorting=none]{biblatex}
\usepackage{enumitem}
\usepackage{etex,etoolbox}
\usepackage{hyperref}
\usepackage{fullpage}
\setlength{\marginparwidth}{2cm}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{multirow}
\bibliography{\string~/Documents/academics/global_academics/global_bib.bib}
% \usepackage{apxproof}

% \bibliography{\string~global_bib.bib}

\def\gc{\overset{\text{GC}}{\rightarrow}}  % Granger causality arrow
\def\ngc{\overset{\text{GC}}{\nrightarrow}}  % Negated Granger causality arrow
\def\pwgc{\overset{\text{PW}}{\rightarrow}}  % Pairwise Granger causality arrow
\def\npwgc{\overset{\text{PW}}{\nrightarrow}}  % Negated pairwise Granger causality arrow
\def\te{\overset{\mathcal{T}}{\rightarrow}}  % Transfer entropy arrow
\def\gcg{\mathcal{G}}  % Granger-causality graph
\def\gcge{\mathcal{E}}  % Graph edges
\def\VAR{\mathsf{VAR}}  % VAR(p) model
\def\B{\mathsf{B}}  % Filter B
\def\wtB{\widetilde{\B}}  % General filter B
\def\A{\mathsf{A}}  % Filter A
\def\H{\mathcal{H}}  % Hilbert space

\newcommand{\linE}[2]{\hat{\E}[#1\ |\ #2]}  % Linear projection
\newcommand{\linEerr}[2]{\xi[#1\ |\ #2]}  % Error of linear projection
\newcommand{\pa}[1]{pa(#1)}  % Parents of a node
\newcommand{\anc}[1]{\mathcal{A}(#1)}  % Ancestors of a node
\newcommand{\ancn}[2]{\mathcal{A}_{#1}(#2)}  % nth ancestors of a node
\newcommand{\gpn}[2]{gp_{#1}(#2)}  % nth generation grandparents
\newcommand{\wtalpha}[2]{\widetilde{\alpha}(#1, #2)}  % Some notation for lem:pwgc_anc
\newcommand{\dist}[2]{\mathsf{d}(#1, #2)}  % Distance between things
\newcommand{\gcgpath}[2]{#1 \rightarrow \cdots \rightarrow #2}  % A shorter path command

\input{\string~/Documents/academics/global_academics/latex_preamble}
% \input{\string~latex_preamble}

\graphicspath{{../figures/}}

\title{A Study in Pairwise Testing Heuristics for Granger-causal Network Estimation}
\author{R. J. Kinnear \\
  \small\href{mailto:ryan@kinnear.ca}{ryan@kinnear.ca} \\
  \small\url{https://github.com/RJTK} \and R. R. Mazumdar \\
  \small\href{mailto:mazum@uwaterloo.ca}{mazum@uwaterloo.ca}}

\begin{document}
\maketitle
\abstract{Following recent theoretical developments of
  Granger-causality graph estimation for ``strongly causal'' causality
  graphs, we develop in detail a computationally and statistically
  efficient heuristic for Granger-causality estimation which is based
  purely on pairwise causality testing.  We perform detailed
  comparisons against state of the art algorithms in the LASSO family
  through a simulation study, and demonstrate the capacity for scaling
  to thousands of nodes in a time series network.  We develop an
  application example for EEG data where we accurately discriminate
  between subjects based purely on the Granger-causality graphs
  inferred from data by our algorithms, demonstrating that meaningful
  features are being captured in the Granger-causalilt graph topology,
  as well as providing evidence that individuals exhibit meaningfully
  distinct EEG causality structures, which may be a result of
  independent scientific interest.}

\keywords{causality graph, EEG, Granger-causality, LASSO, network
  learning, time series, vector autoregression}

\paragraph{Acknowledgement}
We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), [funding reference number 518418-2018].  Cette recherche a été financée par le Conseil de recherches en sciences naturelles et en génie du Canada (CRSNG), [numéro de référence 518418-2018].

\clearpage

% \tableofcontents
% \clearpage

\section{Introduction and Review}
\label{sec:introduction}
Following recent theoretical developments \cite{my_GC_paper}
establishing that, under certain topological assumptions on the
underlying causality graph, it is possible to accurately recover the
true causality graph by way of pairwise causality tests alone.  That
is, there are algorithms which are immune to the effects of
confounders when applied to certain causality graph structures.  See
\cite{tam2013gene_pwgc} for earlier work on pairwise Granger-causality.
In this paper we provide simulation evidence that, given structural
assumptions, such algorithms can dramatically outperform state of the
art LASSO-type estimators.  Moreover, even in cases where structural
assumptions do not in fact hold, using similar algorithms as pure
heuristics can still exhibit superior performance.  In addition to the
potential for gains in statistical power, that our algorithms make use
only of pairwise causality testing improves the scalability (both in
terms of the asymptotics, as well as through trivial parallelization)
of Granger-causality testing up to thousands of nodes.

Granger-causality is a notion is leveraged in a variety of
applications e.g. in Neuroscience as a means of recovering
interactions amongst brain regions \cite{bressler2011wiener},
\cite{anna_paper2008}, \cite{david2008identifying}; in the study of
the dependence and connectedness of financial institutions
\cite{NBERw16223}; gene expression networks \cite{Fujita2007},
\cite{methods_for_inferring_gene_regulatory_networks_from_time_series_expression_data},
\cite{grouped_graphical_granger_modelling_for_gene_expression_regulatory_networks_discovery},
\cite{discovering_graphical_Granger_causality_using_the_truncating_lasso_penalty};
and power system design \cite{Misyrlis2016450}, \cite{yuan2014root}.

Granger-causality can generally be formulated by searching for the
``best'' graph structure consistent with observed data, which is in
general an extremely challenging problem (i.e. it may be framed as a
best subset selection problem, see \cite{bss_mio},
\cite{hastie_bss_comp}), moreover, the comparison of quality between
different structures, and hence the notion of ``best'' needs
qualification.  In applications where we are interested merely in
minimizing the mean squared error of a linear one-step-ahead
predictor, then we will be satisfied with an entirely dense graph of
connections, since each edge can only serve to reduce estimation
error.  However, since the number of edges scales quadratically in $n$
(the number of nodes) it becomes imperative to infer a sparse
causality graph for large systems, both to avoid overfitting observed
data, as well as to aid the interpretability of the results.

A fairly early approach to the problem in the context of large systems
is provided by \cite{bach2004learning}, where the authors apply a
local search heuristic to the Whittle likelihood with an AIC
penalization.  The local search heuristic where at each iteration an
edge is either added, removed, or reversed is a common approach to
combinatorial optimization due to it's simplicity, but is liable to
get stuck in shallow local minima.

A second and wildly successful heuristic is the LASSO regularizer
\cite{tibshirani1996regression}, which can be understood as a natural
convex relaxation to penalizing the count of the non-zero edges.  The
LASSO enjoys fairly strong theoretical guarantees
\cite{wainwright2009sharp}, extending largely to the case of
stationary time series data with a sufficiently fast rate of
dependence decay \cite{basu2015} \cite{wong2016lasso}
\cite{autoregressive_process_modelling_via_the_lasso_procedure}, and
variations on the LASSO have been applied in a number of different
time series contexts as well as Granger-causality
\cite{DBLP:journals/corr/HallacPBL17} \cite{haufe2008sparse}
\cite{bolstad2011causal} \cite{he2013stationary}
\cite{grouped_graphical_granger_modelling_for_gene_expression_regulatory_networks_discovery}.
One of the key improvements to the original LASSO algorithm is the
adaptive (i.e. weighted) ``adaLASSO'' \cite{adaptive_lasso_zou2006},
for which oracle results (i.e. asymptotic support recovery) are
established under less restrictive conditions than for the vanilla
LASSO.  Our experimental comparisons in Section
\ref{sec:empirical_evaluation} are against the
adaLASSO. % Success of the LASSO has lead to
% the famous ``bet on sparsity'' principle ``Use a procedure that does
% well in sparse problems, since no procedure does well in dense
% problems.''  \cite{tibshirani2015statistical}.

% \subsection{Contributions}
% In the context of time series data, sparsity assumptions remain
% important, but there is significant additional structure that may
% arise as a result of considering the topology of the underlying
% Granger-causality graph, which to our knowledge remains largely
% unexplored.  The focus of this paper is to shed light on some of these
% topological questions, in particular, we study a particularly simple
% notion of causality graph topology which we term ``strongly causal''
% and show that stationary times series whose underlying causality graph
% has this structure satisfy natural intuitive notions of ``information
% flow'' through the graph.  Moreover, we show that such graphs are
% perfectly recoverable with only \textit{pairwise} Granger-causality
% tests, which would otherwise suffer from serious confounding problems.
% Our finite sample results are based on simulations, where we show
% promising results for these graph topologies where our algorithm
% performs substantially better in our simulation setup than do
% competing LASSO algorithms, even for graphs that do not exactly
% satisfy our strongly-causal topology assumptions.

% In the case of gene expression networks, we show examples from the
% literature which suggest our concept of a ``strongly causal graph''
% topology may have application in this field (see Section
% \ref{sec:strongly_causal_graphs}).

% The principle contributions of this paper are as follows: firstly, in
% section \ref{sec:theory} we study \textit{pairwise} Granger-causality
% relations, providing novel theorems connecting the structure of the
% causality graph to the pairwise ``causality flow'' in the system, as
% well as an interpretation in terms of the graph topology of the
% sparsity pattern of matrices arising in the Wold decomposition,
% generalizing in some sense the notion of ``feedback-free'' processes
% studied by \cite{caines1975feedback} in close connection with
% Granger-causality.  We establish sufficient conditions (sections
% \ref{sec:strongly_causal_graphs}, \ref{sec:persistent_systems}) under
% which a fully conditional Granger-causality graph can be recovered
% from pairwise tests alone (sec \ref{sec:pairwise_algorithm}).
% Secondly, we propose in section \ref{sec:structure_learning} a graph
% search heuristic which implements our theoretical results to finite
% data samples, specifying and summarizing appropriate methods for
% hypothesis testing (section \ref{sec:pairwise_hypothesis_testing}),
% model order selection (section \ref{sec:model_order_selection}),
% computationally efficient estimation (section
% \ref{sec:efficient_model_estimation}), and error rate controls
% (section \ref{sec:error_rate_control}).  Our heuristics are compared
% against the adaLASSO algorithm in Section
% \ref{sec:empirical_evaluation}.  We stress the scalability of our
% algorithm which is capable of comfortably handling hundreds or
% thousands of nodes on a single machine, as opposed to standard LASSO
% algorithms which do not take advantage of the special structure
% associated with stationary time series data.  In section
% \ref{sec:application} we develop an example application where a
% classifier is constructed to accurately discriminate between subjects
% in an EEG study based only on the Granger-causality graph topology
% inferred by our algorithms.  Concluding remarks on further open
% problems and extentions are provided in Section \ref{sec:conclusion}.

\section{Granger Causality}
\label{sec:theory}
For the purposes of this paper, we provide an expediant and simplified
definition of Granger-causality.  For a more detailed exposition see
e.g. \cite{my_GC_paper}, \cite{granger1969investigating},
\cite{geweke1982measurement}, \cite{lutkepohl2005new}.  Suppose that $x(t)$ is an
$n-$dimensional wide-sense stationary time series generated by the
following (stable) $VAR(\infty)$ model

\begin{equation}
  \label{eq:var_model}
  x(t) = \sum_{\tau = 1}^\infty B(\tau) x(t - \tau) + v(t).
\end{equation}

\begin{definition}[Granger-causality]
  \label{def:granger_causality}
  We will say that the $j$th component of $x$, namely $x_j(t)$,
  Granger-causes component $x_i(t)$ (with respect to $x$) and write
  $j \gc i$ if $\exists \tau \in \Z_{++}$ such that $B_{ij}(\tau) \ne 0$.
\end{definition}

This definition captures the notion that $x_j$ provides information
about $x_i$ which is not available through any of the other components
of $x$.  The motivation being that, since an effect must follow it's
cause, that $x_j$ ``causes'' $x_i$.  This intuition can also be given
interpretation as the ``flow'' of ``information'' or ``energy''.

\begin{definition}[Granger-causality Graph]
  We define the Granger-causality graph $\gcg = ([n], \gcge)$ to be
  the directed graph formed on $n$ vertices where an edge
  $(j, i) \in \gcge$ if and only if $x_j$ Granger-causes $x_i$ (with
  respect to $x$).  That is,
  $$(j, i) \in \gcge \iff j \in \pa{i} \iff x_j \gc x_i.$$
\end{definition}

Referring back to the definition (Def. \ref{def:granger_causality}) of
Granger-causality it is important that the concept be understood with
respect to a particular universe of observations.  If $x_j \gc x_i$
with respect to $x_{-k}$, it may not hold with respect to $x$.  For
example, $x_k$ may be a common ancestor which when observed,
completely explains the connection from $x_j$ to $x_i$.  We introduce
the pairwise definition of Granger-causality as a special case:

\begin{definition}[Pairwise Granger-causality]
  We will say that $x_j$ pairwise Granger-causes $x_i$ and write
  $x_j \pwgc x_i$ if $x_j$ Granger-causes $x_i$ with respect only to
  $(x_i, x_j)$.
\end{definition}

This notion is of interest for a variety of reasons.  From a purely
conceptual standpoint, we will see how the notion can in some sense
capture the idea of ``flow of information'' in the underlying graph,
in the sense that if $j \in \anc{i}$ we expect that $j \pwgc i$.  It
may also be useful for reasoning about the conditions under which
\textit{unobserved} components of $x(t)$ may or may not interfere with
inference in the actually observed components.  Finally, motivated
from a practical standpoint to analyze causation in large systems, we
will seek to construct practical estimation procedures based purely on
pairwise causality tests since the computation of such pairwise
relations is somewhat easier.

\subsection{Exact Recovery From Pairwise Tests}
\label{sec:strongly_causal_graphs}
In this section we define the notion of a strongly causal graph, and
review the implications for pairwise causality testing.

\begin{definition}[Strongly Causal \cite{my_GC_paper}]
  \label{def:strongly_causal}
  We will say that a Granger-causality graph $\gcg$ is
  \textit{strongly causal} if it is acyclic and there is at most 1
  directed path between any two nodes.  Strongly Causal Graphs will be
  referred to as SCGs.
\end{definition}

  Examples of strongly causal graphs include directed trees (or
  forests), DAGs where each node has at most one parent, and complete
  bipartite graphs.  Examples of graphs arising in practice that appear
  to be ``close'' to strongly causal (i.e. few edges violate the condition)
  appear to be available in biology, in particular, the authors of
  \cite{discovering_graphical_Granger_causality_using_the_truncating_lasso_penalty}
  cite an example of the so called ``transcription regulatory network
  of \textit{E.coli}'', and
  \cite{learning_genome_scale_regulatory_networks} study a much larger
  regulatory network of \textit{Saccharomyces cerevisiae}.  These
  networks, which we reproduce\footnote{Figure \ref{fig:gene_network1}
    is reproduced under the Creative Commons Attribution
    Non-Commercial License
    (\url{http://creativecommons.org/licenses/by-nc/2.5}) and Figure
    \ref{fig:gene_network2} under the Creative Commons Attribution
    License (\url{https://creativecommons.org/licenses/by/4.0/})} in
  figure \ref{fig:gene_networks} appear to have at most a small number
  of edges which violate the strong-causality condition.

  \begin{figure}[h]
    \centering
    \caption{Transcription Regulatory Networks}
    \label{fig:gene_networks}
    \begin{subfigure}[b]{0.45\textwidth}
      \caption{\textit{E.Coli} Network of
        \cite{discovering_graphical_Granger_causality_using_the_truncating_lasso_penalty}}
      \label{fig:gene_network1}
      \includegraphics[width=\linewidth, height=\linewidth]{ecoli_regulatory_network.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \caption{\textit{Saccharomyces cerevisiae} Network of
        \cite{learning_genome_scale_regulatory_networks}}
      \label{fig:gene_network2}
      \includegraphics[width=\linewidth, height=\linewidth]{huge_gene_network.png}
    \end{subfigure}
  \end{figure}

  There is another technical condition (detailed in
  \cite{my_GC_paper}) required for the next theorem which eliminates
  the possibility of certain kinds of pathological cancellation when
  confounding nodes are present.  However, $\VAR$ systems which do not
  satisfy the technical condition are pathological.

  \begin{theorem}[Pairwise Recovery \cite{my_GC_paper}]
    \label{thm:scg_recovery}
    Suppose that the Granger-causality graph $\gcg$ for the process
    $x(t)$ is a strongly causal DAG.  Then, except for certain
    pathological cases \cite{my_GC_paper}, $\gcg$ can be inferred from
    pairwise causality tests, given an oracle for pairwise causation.
  \end{theorem}

  The proof of this theorem proceeds by proving the correctness of an
  algorithm for implementing exact recovery through pairwise testing.
  This algorithm inspires the heuristic\footnote{Recall that
    Granger-causality is necessarily based on heuristics when $n$ is
    modestly large, as the complexity of searching over causality
    graphs scales as $O(2^{n^2})$} of Section
  \cite{sec:structure_learning}.

\section{Laten Structure Recovery Performance Metric}
The Granger causal graph structure described in section
\ref{sec:theory} is entirely latent.  That is, we can never observe
the true causality graph (assuming such a structure \textit{exists}),
which makes evaluating different support recovery algorithms
difficult.  Broadly speaking, most work on the topic evaluates the
performance of competing causality graph recovery algorithms in one of
two ways (or both): (1) theoretical proofs or (2) simulations where
the true graph is known by construction.  In this work our goal is to
establish as valid a third possibility: the performance of a second
step classifier.

\subsection{Introduction and Motivation}
More precisely, we suppose we have a two distinct data generating
processes (``subjects'') labeled $S^{(1)}$ and $S^{(2)}$.  Each
subject has their own fixed, distinct, and unobservable Granger
causality graph $\gcg^{(i)}$.  And, when queried, the subject
returns% \footnote{Formally, we could define a distribution $t_0$ on
  % $\N$ and take $X^{(i)} = \{x^{(i)}(t)\}_{t = t_0}^{t_0 + T}$.  Since
  % $x(t)$ is stationary, this doesn't effect the statistics of the
  % sample}
a finite length $T$ sample of time series data
$X^{(i)} \defeq \{x^{(i)}(t)\}_{t = 1}^{T}$ from a stationary process
whose temporal dependence structure corresponds to the causality graph
structure of $\gcg^{(i)}$.  We suppose that we collect $N$ independent
samples $\{X^{(i)}_n\}_{n = 1}^N$ from each subject.

In addition to the subjects, we have $K$ algorithms $A_1, \ldots, A_K$
which consume such a dataset $X_n^{(i)}$ and return an estimate
$\widehat{\gcg}_{k,n}^{(i)}$ of graph $\gcg^{(i)}$.  That is, if we
view graph estimates as points in $\{0, 1\}^{n^2}$ we have
$A_k: \R^{T \times n} \rightarrow \{0, 1\}^{n^2}$.

Applying each algorithm to each sample of time series data allows us
to construct $K$ datasets $\mathcal{D}_1, \ldots, \mathcal{D}_K$, each
of which contains $2N$ labeled graph estimates
$(\widehat{\gcg}_{k, n}^{(i)}, i)$:

\[
  \mathcal{D}_k = \bigcup_{i \in \{1, 2\}} \bigcup_{n = 1}^N \{(\widehat{\gcg}_{k, n}^{(i)}, i)\}.
\]

Finally, we fit a classifier
$C_k: \{0, 1\}^{n^2} \rightarrow \{1, 2\}$ to the dataset
$\mathcal{D}_k$ which consumes an estimated causality graph and
attempts to predict the subject label $i \in \{1, 2\}$ that generated
the data from which the graph was estimated.  If classifier $C_{k_1}$
has superior out-of-sample test accuracy in comparison to $C_{k_2}$,
we consider this as evidence that Algorithm $A_{k_1}$ has made more
accurate estimates of $\gcg^{(i)}$ than has Algorithm $A_{k_2}$.  That
is, if for new graph estimates
$\widehat{\gcg}_{k_1}^{(i)}, \widehat{\gcg}_{k_2}^{(i)}$ we have

\begin{equation}
  \label{eqn:comparison_metric}
  \P\big\{C_{k_1}(\widehat{\gcg}_{k_1}^{(i)}) = i\big\} > \P\big\{C_{k_2}(\widehat{\gcg}_{k_2}^{(i)}) = i\big\}.
\end{equation}

\begin{remark}
We will provide mathematical justification for this evaluation
strategy below, but some remarks are in order.

Firstly, if Equation \ref{eqn:comparison_metric} holds, it does not
constitute \textit{proof} that $A_{k_1}$ is superior to $A_{k_2}$ for
the task of estimating Granger causality graphs.  Indeed, it would be
possible to construct an algorithm $A$ which attempts to encode the
idiosyncracies of $s^{(i)}$ present in the data $x^{(i)}(t)$, making
no attempt at actually recovering $\gcg^{(i)}$.  We therefore
formulate our theoretical justification such that we make no reference
to the actual recovery algorithm.

Secondly, we claim that this evaluation strategy is distinct from the
above evaluation metrics (1) and (2) (i.e. mathematical proof, and
simulation), but we do not claim it is \textit{better}.  Merely, we
propose it as a third mechanism for comparing different recovery
algorithms that may provide further insight and quantify performance
along another axis.
\end{remark}

\subsection{Theoretical Analysis}

\section{Finite Sample Graph Recovery}
\label{sec:structure_learning}
In this section we detail our proposed pairwise testing heuristics.
In particular, Algorithm \ref{alg:finite_pwgc} proceeds first by a
simple thresholding of pairwise relations.  The algorithm next
proceeds by an iterative procedure which ``peels away layers'' of the
causality graph eminating from parentless nodes, while maintaining the
strong causality property.  We present simulation evidence that this
heuristic can outperform LASSO-based heuristics in \cite{my_GC_paper}.
% We will see in Section \ref{sec:empirical_evaluation} that Algorithm
% \ref{alg:finite_pwgc} outperforms LASSO-based heuristics, even when
% $\gcg$ does not exactly satisfy the strong causality assumption.

In the appendix Section \ref{sec:pairwise_hypothesis_testing} we describe
straightforward methods for implementing Algorithm
\ref{alg:finite_pwgc}.  In particular, we consider it important to be
able to test pairwise causation in a computationally efficient manner,
since $O(n^2)$ pairwise tests are needed.  In practice, carrying out
each pairwise test consumes the vast majority of computational time,
though since this step is trivially parallelizable, we are able to
scale the estimation procedure to over $1000$ nodes on a personal
computer.

\begin{algorithm}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \SetKwInOut{Initialize}{initialize}
    \DontPrintSemicolon

    \BlankLine
    \caption{Finite Sample Pairwise Graph Recovery (PWGC)}
    \label{alg:finite_pwgc}

    \Input{Estimates of pairwise Granger-causality statistics $F_{ij}$
      (eqn \ref{eqn:gc_statistics}).  Matrix of edge probabilities $P_{ij}$ (eqn \ref{eqn:edge_inclusion_probability}).  Hypothesis testing threshold $\delta$ chosen via the Benjamini-Hochberg criterion (Section \ref{sec:error_rate_control})}
    \Output{A strongly causal graph $\widehat{\gcg}$}
    \Initialize{$S = [n]$  \texttt{\# unprocessed nodes}\\
      $E = \emptyset$  \texttt{\# edges of }$\widehat{\gcg}$\\
      $k = 1$ \texttt{\# a counter used only for notation}}

    \BlankLine

    $W_\delta \leftarrow \{(i, j)\ |\ P_{ji} > 1 - \delta, F_{ji} > F_{ij}\}$  \texttt{\# candidate edges}\\
    $\mathcal{I}_0 \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# total node incident probability}\\
    $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) < \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Nodes with fewest incident edges}\\
    \If{$P_0 = \emptyset$}{
      $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) \le \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Ensure non-empty}
    }
    \BlankLine

    \While{$S \ne \emptyset$}{
      $S \leftarrow S \setminus P_{k - 1}$ \texttt{\# remove processed nodes}\\
      % $\mathcal{I}_k \leftarrow \big(\sum_{j \in S: (j, i) \in W_\delta} F_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# intra-}$S$ \texttt{incident strength}\\
      $\mathcal{I}_k \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$\\
      $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) < \ceil{\text{min}(\mathcal{I}_k)}\}$\\
      \If{$P_k = \emptyset$}{
        $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) \le \ceil{\text{min}(\mathcal{I}_k)}\}$
      }
      \;
      \texttt{\# add strongest edges, maintaining strong causality}\\
      $U_k \leftarrow \bigcup_{r = 1}^k P_{k - r}$ \texttt{\# Include all forward edges}\\
      \For{$(i, j) \in \mathsf{sort}\Big(\{(i, j) \in U_k \times P_k\ |\ (i, j) \in W_\delta\} \mathsf{\ by\ descending\ } F_{ji}\Big)$} {
        \If{$\mathsf{is\_strongly\_causal}(E \cup \{(i, j)\})$} {
          \texttt{\# }$\mathsf{is\_strongly\_causal}$ \texttt{can be implemented by keeping}\\
          \texttt{\# track of ancestor / descendant relationships}\\
          $E \leftarrow E \cup \{(i, j)\}$
        }
      }
      $k \leftarrow k + 1$\\
    }
    \Return{$([n], E)$}
\end{algorithm}

In order to adapt Algorithm \ref{alg:finite_pwgc} to graphs which may
not be strongly causal, we consider an iterative application as
described in Algorithm \ref{alg:iterated_pwgc}.  We apply Algorithm
\ref{alg:finite_pwgc} first to the data itself, and then subsequently
to the resulting residuals before performing a final estimate on the
combined graph.  We evaluate these algorithms, as well as simpler
pairwise heuristics and LASSO-type heuristics in the next section.  A
theoretical analysis of Algorithm \ref{alg:iterated_pwgc} is deferred
to future work.

\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Initialize}{initialize}
  \DontPrintSemicolon

  \BlankLine
  \caption{Iterated PWGC Heuristic}
  \label{alg:iterated_pwgc}
  
  \Input{Data $x(t)$, maximum number of iterations $N$,
    threshold $r \in (0, 1)$.}
  \Output{Granger-causality graph estimate $\widehat{G}$ obtaind from
    iterated application of PWGC Algorithm \ref{alg:finite_pwgc}}
  \Initialize{$i = 1$, $\widehat{x}_0(t) \leftarrow x(t)$,
    $\sigma_0 \leftarrow Var\ x(t)$}

  \While{$i \le N$} {
    Estimate $\VAR$ model via PWGC on $x_{i - 1}(t)$ to obtain $\widehat{x}_{i - 1}(t)$ and $\widehat{G}_{i - 1}$\\
    $\epsilon_i(t) \leftarrow x_{i - 1}(t) - \widehat{x}_{i - 1}(t)$ \texttt{\# Compute residuals}\\
    $\sigma_i^2 \leftarrow \text{MSE}\big(\epsilon_i(t)\big)$ \texttt{\# Compute MSE}\\
    \If{$\sigma_i^2 > r \sigma_{i - 1}^2$}{
      \texttt{break  \# Insignificant Improvement}
    }
    $i \leftarrow i + 1$
  }
  $\widehat{G} \leftarrow \bigcup_{i} \widehat{G}_{i}$ \texttt{\# Combine PWGC graph estimates}\\
  \Return{$\widehat{G}$}
\end{algorithm}

% \section{Empirical Evaluation}
% \label{sec:empirical_evaluation}
Our implementations are in Python \cite{scipy}, in particular we
leverage the LASSO implementation from \texttt{sklearn} \cite{sklearn}
and the random graph generators from \texttt{networkx}
\cite{networkx}.  Our paper \cite{my_GC_paper} provides simulation
evidence that our PWGC heuristic can outperform LASSO-based heuristics
for simulated data.

% We run experiments using two separate graph topologies having $n$
% nodes: a strongly causal graph (SCG) and a directed acyclic graph
% (DAG).  These are generated respectively by drawing a random tree
% and a random Erdos Renyi graph (with edge probability
% $q = \frac{2}{n}$ resulting in approximately the same number of
% edges for the SCG as for the DAG), then creating a directed graph by
% directing edges from lower numbered nodes to higher numbered nodes.

% We populate each of the edges (including self loops) with random
% linear filters constructed by placing $5$ transfer function poles
% (i.e. $p = 5$) uniformly at random in a disc of radius $3 / 4$ (which
% guarantees stability for acyclic graphs).  The resulting system is
% driven by i.i.d. Gaussian random noise, each component having random
% variance $\sigma_i^2 = 1/2 + r_i$ where $r_i \sim \text{exp}(1/2)$.
% We set $p_{\text{max}} = 15$.  Results and representative graphs are
% collected in Figures \ref{fig:random_graph_topologies},
% \ref{fig:simulation_results_comparison1},
% \ref{fig:simulation_results_comparison2},
% \ref{fig:simulation_results_scaling_and_small_T},
% \ref{fig:simulation_results_dense}.

% We compare our results against the adaptive LASSO
% \cite{adaptive_lasso_zou2006}, which outperformed substantially both
% the LASSO and the grouped LASSO.  Motivated by scaling, we split the
% squared error term into separate terms, one for each group of incident
% edges on a node, and estimate the collection of $n$ incident filters
% $\big\{\B_{ij}(z)\big\}_{j = 1}^n$ that minimizes
% $\xi_i^{\text{LASSO}}$ in the following:

% \begin{equation}
%   \begin{aligned}
%   \xi_i^{\text{LASSO}}(\lambda) &= \underset{B}{\text{min}}\ \frac{1}{T}\sum_{t = p + 1}^T\big(x_i(t) - \sum_{\tau = 1}^p\sum_{j = 1}^n B_{i, j}(\tau) x(t - \tau)\big)^2 + \lambda \sum_{\tau = 1}^p \sum_{j = 1}^n |B_{ij}(\tau)|\\
%   \xi_i^{\text{LASSO}} &= \underset{\lambda \ge 0}{\text{min}}\ \xi_i^{\text{LASSO}}(\lambda) + \mathsf{BIC}\big(B_i^{\text{LASSO}}(\lambda)\big)\\
%   \end{aligned}
% \end{equation}

% where we are choosing $\lambda$, the regularization parameters, via the BIC.

% \begin{figure}
%   \centering
%   \caption{Representative Random Graph Topologies}
%   \label{fig:random_graph_topologies}
%   \begin{subfigure}[b]{0.3\textwidth}
%     \caption{Random SCG}
%     \includegraphics[width=\linewidth]{example_scg.pdf}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.3\textwidth}
%     \caption{Random DAG $(q = \frac{2}{n})$}
%     \includegraphics[width=\linewidth]{example_dag.pdf}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.3\textwidth}
%     \caption{Random DAG $(q = \frac{4}{n})$}
%     \includegraphics[width=\linewidth]{example_dag_q.pdf}
%   \end{subfigure}
% \end{figure}

% \begin{remark}[Graph Topologies]
%   We depict in Figure \ref{fig:random_graph_topologies} the topologies
%   of random graphs used in our empirical evaluation.  For values of
%   $q$ close to $\frac{2}{n}$, the resulting random graphs tend to have
%   a topology which is, at least qualitatively, close to the SCG.  As
%   the value of $q$ increases, the random graphs deviate farther from
%   the SCG topology, and we therefore expect the LASSO to outperform
%   PWGC for larger values of $q$.  This can be observed in Figure
%   \ref{fig:simulation_results_dense}, at least where performance is
%   measured by the support recovery (i.e. via the MCC).
% \end{remark}

% \begin{remark}[MCC as a Support Recovery Measurement]
%   We apply Matthew's Correlation Coefficient (MCC)
%   \cite{matthews1975comparison} as a statistic for measuring support
%   recovery performance.  This statistic synthesizes the confusion
%   matrix into a single score appropriate for unbalanced labels and is
%   calibrated to fall into the range $[-1, 1]$ with $1$ being perfect
%   performance, $0$ being the performance of random guessing, and $-1$
%   being perfectly opposed.
% \end{remark}

% \begin{remark}[Error Measurement]
%   We estimate the 1-step ahead prediction error by forming the variance matrix estimate

%   \begin{equation*}
%     \widehat{\Sigma}_v \defeq \frac{1}{T_{\text{out}}} \sum_{t = 1}^{T_{\text{out}}} (x(t) - \widehat{x}(t))(x(t) - \widehat{x}(t))^\T
%   \end{equation*}

%   on a long stream of out-of-sample data.  We then report the quantity

%   \begin{equation*}
%     \frac{\ln \tr \widehat{\Sigma}_v}{\ln \tr \Sigma_v}
%   \end{equation*}

%   where $\widehat{\Sigma}_v = \Sigma_v$ is the best possible performance.
% \end{remark}

% \begin{figure}
%   \centering
%   \caption{PWGC Compared Against AdaLASSO \cite{adaptive_lasso_zou2006} (SCG)}
%   \label{fig:simulation_results_comparison1}
%   \includegraphics[width=0.95\textwidth]{new_lasso_comparison_scg_pmax15_simulation.pdf}

%   {\scriptsize Comparison of PWGC and LASSO for $\VAR(p)$ model
%     estimation.  We make comparisons against both the MCC and the
%     relative log mean-squared prediction error
%     $\frac{\ln\tr \widehat{\Sigma}_v}{\ln\tr \Sigma_v}$.  Results
%     in Figure \ref{fig:simulation_results_comparison1} are for systems
%     guaranteed to satisfy the assumptions required for Theorem
%     \ref{thm:scg_recovery}.}
% \end{figure}

% \begin{figure}
%   \caption{PWGC vs adaLASSO (DAG, $q = \frac{2}{n}$)}
%   \label{fig:simulation_results_comparison2}
%   \includegraphics[width=0.95\textwidth]{new_lasso_comparison_dag_pmax15_simulation.pdf}

%   {\scriptsize Figure \ref{fig:simulation_results_comparison2}
%     provides results for systems which do not guarantee the
%     assumptions of Theorem \ref{thm:scg_recovery}, though the graph
%     has a similar level of sparsity.}
% \end{figure}

% \begin{figure}
%   \centering
%   \caption{PWGC Scaling and Small Sample Performance}
%   \label{fig:simulation_results_scaling_and_small_T}
%   \begin{subfigure}[b]{0.45\textwidth}
%     \caption{Fixed $T$, increasing $n$ (SCG)}
%     \label{fig:simulation_results_scaling}
%     \includegraphics[width=\linewidth]{new_increasing_n_simulation.pdf}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.45\textwidth}
%     \caption{MCC Comparison for $T \le 100$}
%     \label{fig:small_T_comparison}
%     \includegraphics[width=\linewidth]{new_mcc_comparison001.pdf}
%   \end{subfigure}

%   {\scriptsize Figure \ref{fig:simulation_results_scaling} measures
%     support recovery performance as the number of nodes $n$ increases,
%     and the edge proportion as well as the number of samples $T$ is
%     held fixed.  Remarkably, the degradation as $n$ increases is
%     limited, it is primarily the graph topology (SCG or non-SCG) as
%     well as the level of sparsity (measured by $q$) which are the
%     determining factors for support recovery performance.

%     Figure \ref{fig:small_T_comparison} provides a support recovery
%     comparison for very small values of, $T$ typical for many
%     applications.}
% \end{figure}

% \begin{figure}
%   \caption{Fixed $T, n$, increasing edges $q$ (DAG)}
%   \label{fig:simulation_results_dense}
%   \includegraphics[width=\linewidth]{dag_increasing_q_small_T_alasso_simulation.pdf}

%   {\scriptsize Figure \ref{fig:simulation_results_dense} provides a
%     comparison between PWGC and AdaLASSO as the density of graph edges
%     (as measured by $q$) increases.  For reference,
%     $\frac{2}{n} = 0.04$ has approximately the same level of sparsity
%     as the SCGs we simulated.  As $q$ increases, the AdaLASSO
%     outperforms PWGC as measured by the MCC.  However, PWGC maintains
%     superior performance for 1-step-ahead prediction.  We speculate
%     that this is a result of fitting the sparsity pattern recovered by
%     PWGC via OLS which directly seeks to optimize this metric, whereas
%     the LASSO is encumbered by the sparsity inducing penalty.}
% \end{figure}

% In reference to figure \ref{fig:simulation_results_comparison1} it
% should not be overly surprising that our PWGC algorithm performs
% better than the LASSO for the case of a strongly causal graph, since
% in this case the assumptions which guarantee the correctness of
% Algorithm \ref{alg:finite_pwgc} hold.  However, the performance is
% still markedly superior in the case of a more general DAG.  We would
% conjecture that a DAG having a similar degree of sparsity as an SCG is
% likely to be ``close'' to an SCG.  Figure
% \ref{fig:simulation_results_dense} illustrates the severe (expected)
% degradation in performance as the number of edges increases while
% the number of data samples $T$ remains fixed.  For larger values $q$
% in this plot, the number of edges in the graph is comparable to the
% number of data samples.

% We have also paid close attention to the performance of PWGC in the
% very small sample ($T \le 100$) regime (see Figure
% \ref{fig:small_T_comparison}), as this is the regime many applications
% must contend with.

% In regards scalability, we have observed that performing the $O(n^2)$
% pairwise Granger-causality calculations consumes the vast majority
% ($> 90\%$) of the computation time.  Since this step is trivially
% parallelizable, our algorithm also scales well with multiple cores or
% multiple machines.  Figure \ref{fig:simulation_results_scaling} is a
% demonstration of this scalability, where we are able to estimate
% graphs having over $1500$ nodes (over $2.25 \times 10 ^6$ possible edges)
% using only $T = 500$ data points, granted, an SCG on this many nodes
% is extremely sparse.

% \begin{table}
%   \centering
%   \caption{Simulation Results: PWGC vs AdaLASSO}
%   \label{taab:simulation_table}

%   \begin{tabular}{|ll||ll|ll|ll|}
%     \toprule
%     &{Algorithm}&alasso&pwgc&alasso&pwgc&alasso&pwgc\\
%     &\textbf{Metric}&Err&Err&FDR&FDR&MCC&MCC\\
%     \textbf{T}&\textbf{q}&&&&&&\\
%     \midrule
%     \multirow{4}{*}{\textbf{50}}
%     &\textbf{SCG}&1.83&\textbf{1.57}&0.48&\textbf{0.07}&0.47&\textbf{0.57}\\
%     &\textbf{0.06}&1.91&\textbf{1.69}&0.54&\textbf{0.09}&0.43&\textbf{0.53}\\
%     &\textbf{0.11}&2.89&\textbf{2.58}&0.46&\textbf{0.22}&0.38&0.40\\
%     &\textbf{0.46}&8.49&\textbf{7.72}&0.42&0.42&\textbf{0.16}&0.11\\
%     \midrule
%     \multirow{4}{*}{\textbf{250}}
%     &\textbf{SCG}&1.32&\textbf{1.22}&0.27&\textbf{0.07}&0.71&\textbf{0.80}\\
%     &\textbf{0.06}&1.44&\textbf{1.28}&0.32&\textbf{0.09}&0.66&\textbf{0.76}\\
%     &\textbf{0.11}&2.57&\textbf{2.17}&0.32&\textbf{0.16}&0.53&\textbf{0.57}\\
%     &\textbf{0.46}&8.09&\textbf{7.11}&0.39&0.40&\textbf{0.20}&0.14\\
%     \midrule
%     \multirow{4}{*}{\textbf{1250}}
%     &\textbf{SCG}&1.18&\textbf{1.09}&0.40&\textbf{0.06}&0.69&\textbf{0.89}\\
%     &\textbf{0.06}&1.24&1.17&0.43&\textbf{0.05}&0.65&\textbf{0.86}\\
%     &\textbf{0.11}&2.02&2.10&0.33&\textbf{0.14}&0.62&0.63\\
%     &\textbf{0.46}&7.44&\textbf{7.04}&0.40&\textbf{0.30}&\textbf{0.22}&0.19\\
%     \bottomrule
%   \end{tabular}

  % \begin{tabular}{|ll||ll|ll|ll|}
  %   \toprule
  %   &\textbf{Algorithm}&alasso&pwgc&alasso&pwgc&alasso&pwgc\\
  %   &\textbf{Metric}&Err&Err&FDR&FDR&MCC&MCC\\
  %   \textbf{T}&\textbf{q}&&&&&&\\
  %   \midrule
  %   \multirow{4}{*}{\textbf{50}}
  %   &\textbf{SCG}&1.86&\textbf{1.51}&0.49&\textbf{0.16}&0.47&\textbf{0.63}\\
  %   &\textbf{0.06}&1.92&\textbf{1.67}&0.52&\textbf{0.17}&0.44&\textbf{0.60}\\
  %   &\textbf{0.11}&3.06&\textbf{2.81}&0.47&\textbf{0.34}&0.37&\textbf{0.41}\\
  %   &\textbf{0.46}&8.90&\textbf{7.74}&\textbf{0.42}&0.52&\textbf{0.16}&0.10\\
  %   \cline{1-8}
  %   \multirow{4}{*}{\textbf{250}}
  %   &\textbf{SCG}&1.35&\textbf{1.20}&0.28&\textbf{0.21}&0.70&\textbf{0.77}\\
  %   &\textbf{0.06}&1.40&\textbf{1.27}&0.33&\textbf{0.22}&0.66&\textbf{0.73}\\
  %   &\textbf{0.11}&2.48&\textbf{2.18}&0.31&0.30&\textbf{0.55}&0.52\\
  %   &\textbf{0.46}&7.96&\textbf{6.89}&0.38&0.38&\textbf{0.20}&0.15\\
  %   \cline{1-8}
  %   \multirow{4}{*}{\textbf{1250}}
  %   &\textbf{SCG}&1.15&1.12&0.42&\textbf{0.24}&0.67&\textbf{0.79}\\
  %   &\textbf{0.06}&1.28&1.24&0.45&\textbf{0.24}&0.64&\textbf{0.75}\\
  %   &\textbf{0.11}&2.11&2.05&0.34&\textbf{0.29}&\textbf{0.60}&0.56\\
  %   &\textbf{0.46}&7.12&\textbf{6.79}&0.39&\textbf{0.32}&\textbf{0.24}&0.18\\
  %   \bottomrule
  % \end{tabular}

  % \begin{tabular}{|ll||ll|ll|ll|}
  %   \toprule
  %   &\textbf{Algorithm}&alasso&pwgc&alasso&pwgc&alasso&pwgc\\
  %   &\textbf{metric}&RLMSE&&FDR&&MCC&\\
  %   \textbf{T}&\textbf{q}&&&&&&\\
  %   \midrule
  %   \multirow{4}{*}{\textbf{50}}
  %   &\textbf{SCG}&2.38&\textbf{1.56}&0.49&\textbf{0.16}&0.48&\textbf{0.63}\\
  %   &\textbf{0.06}&2.66&\textbf{1.68}&0.52&\textbf{0.19}&0.45&\textbf{0.60}\\
  %   &\textbf{0.11}&3.91&\textbf{2.68}&0.46&\textbf{0.32}&0.38&\textbf{0.42}\\
  %   &\textbf{0.46}&10.24&\textbf{7.94}&\textbf{0.41}&0.49&\textbf{0.17}&0.11\\
  %   \cline{1-8}
  %   \multirow{4}{*}{\textbf{250}}
  %   &\textbf{SCG}&2.14&\textbf{1.21}&0.30&\textbf{0.20}&0.69&\textbf{0.78}\\
  %   &\textbf{0.06}&2.26&\textbf{1.29}&0.32&\textbf{0.24}&0.66&\textbf{0.73}\\
  %   &\textbf{0.11}&3.60&\textbf{2.12}&0.31&0.29&0.55&0.53\\
  %   &\textbf{0.46}&10.17&\textbf{7.27}&0.40&0.41&\textbf{0.19}&0.14\\
  %   \cline{1-8}
  %   \multirow{4}{*}{\textbf{1250}}
  %   &\textbf{SCG}&1.88&\textbf{1.12}&0.39&\textbf{0.22}&0.70&\textbf{0.81}\\
  %   &\textbf{0.06}&1.88&\textbf{1.21}&0.41&\textbf{0.25}&0.66&\textbf{0.75}\\
  %   &\textbf{0.11}&3.24&\textbf{2.17}&0.34&\textbf{0.28}&\textbf{0.59}&0.56\\
  %   &\textbf{0.46}&9.74&\textbf{6.93}&0.40&\textbf{0.35}&\textbf{0.22}&0.17\\
  %   \bottomrule
  % \end{tabular}

%   {\scriptsize Results of Monte Carlo simulations comparing PWGC and
%     AdaLASSO for small samples and when the SCG assumption doesn't
%     hold.  The superior result is bolded when the difference is
%     statistically significant, as measured by
%     \texttt{scipy.stats.ttest\_rel}.  100 iterations are run for each
%     set of parameters.}
% \end{table}

\section{Application}
\label{sec:application}
In this section we apply our methods, as well as a number of
previously known methods, to a real set of EEG data obtained from the
``EEG Database Data
Set''\footnote{http://archive.ics.uci.edu/ml/datasets/EEG+Database}
\cite{zhang1995event} on the UCI machine learning repository
\cite{uci_mlr}.  This dataset contains 1 second long measurements of
(64 channel) EEG signals from patients who are given visual stimuli.

More precisely, our dataset is described by
$\mathcal{D} = \{\big(x^{(i)}(t)\big)_{t = 1}^T, y^{(i)} \}_{i = 1}^N$
where $T = 256$, $x^{(i)}(t) \in \R^{64}$,
$y^{(i)} \in [N_{subjects}]$ (with $N_{subjects} = 119$), and
$N = 10723$.  There are on average 90 trials for each subject, ranging
between 30 and 119.

We have constructed a simple pipeline for discriminating between
subjects by first applying a Granger-causality algorithm directly to
the EEG data $x^{(i)}$ to obtain a Granger-causality graph
$\widehat{G}^{(i)}$.  We then feed the vectorized adjacency matrix of
$\widehat{G}^{(i)}$ through a polynomial Kernel multinomial logistic
regression model with parameters fit by cross validation.

\subsection{Explanation}
The subjects in the study are labeled as being either ``control'' or
``alcoholic'', however, we will ignore this label and instead focus on
\textit{distinguishing between subjects} based on the
Granger-causality graphs inferred from the subject's trials.  Our
reasoning is to focus on the underlying question: ``Does our PWGC
algorithm uncover meaningful Granger-causal connections from EEG
data?''.  Focusing only on the subject label allows us to answer this
question without simultaneously grappeling with the physiological
question of whether alcoholic subjects as a group have discernable
differences in their EEG readings\footnote{Some exploratory analysis
  actually suggests that alcoholic Granger-causality graphs are not
  substantively different}, which is a stronger requirement than
simply that there are meaningful distinctions between the EEG readings
of subjects generally.

We will provide evidence in the affirmative for the question posed.
Moreover, we will compare the classification accuracy resulting from
training the same classifier on the causality graphs estimated by our
PWGC algorithm, as well as graphs estimated by previously studied
methods.

Since Granger-causality graphs are latent structures, it is difficult
to access the quality of graph recovery algorithms when applied to
real-world data, since the ground truth causality graph cannot be
known.  Our reasoning for comparing classification accuracy is the
following: if causality graph estimates are provided by two methods,
say $M_1$ and $M_2$, it stands to reason that if a classifier trained
on $M_1$ significantly outerpforms a classifier trained on $M_2$, then
the latent structure inferred from $M_1$ is more effectively capturing
the nature of the ground truth causality graph.

While it is almost certainly possible to achieve much greater
classification accuracy on this dataset by constructing a classifier
to act directly on $x^{(i)}(t)$, we stress that our purpose is to
demonstrate that relevant information is being captured by the graph
estimates $\widehat{G}^{(i)}$.  That is, evidence that the latent
structure $\widehat{G}^{(i)}$ contains scientifically relevant
insights, as opposed to simply being a feature for classification
tasks.

\subsection{Preliminary Analysis}
Figures \ref{fig:ar_eeg_example} and \ref{fig:eeg_oos_error}
illustrate the appropriateness of modelling $x(t)$ with $\VAR$ models,
as opposed simply to a collection of $n$ unidimensional
autoregressiive models.  The conclusion being that the
Granger-causality graph estimates are not purely spurious.  If there
were no gains in prediction power resulting from the application of a
$\VAR$ model, as opposed to a collection of unidimensional
$\mathsf{AR}$ models, it would be difficult to justify the use of
linear Granger-causality for EEG data at all.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{$\mathsf{AR}(p)$ Model Example}
    \label{fig:ar_eeg_example}
    \includegraphics[width=\linewidth]{eeg_estimate_example.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{OOS Error for a Single Subject}
    \label{fig:eeg_oos_error}
    \includegraphics[width=\linewidth]{eeg_oos_demonstration.pdf}
  \end{subfigure}

  {\scriptsize Figure \ref{fig:ar_eeg_example} provides an example of
    an autoregressive model fir via the Levinson-Durbin algorithm
    (choosing the system order via the BIC) on a single EEG channel.
    Linear autoregressive models appear qualitatively to be adequate
    for this data.  Figure \ref{fig:eeg_oos_error} demonstrates the
    improvement of a unified $\VAR$ model over independent
    $\mathsf{AR}$ models, providing evidence that intra-node edges
    inferred by PWGC are not simply spurious.

  Out-of-Sample estimates are performed on data that were not used in
  fitting the models.}
\end{figure}

\subsection{Results}
We provide the final results in Figure
\ref{fig:logistic_regression_results} where the classification
accuracy is quantified by the (multiclass) MCC of $\approx 0.20$ on a held
out validation set consisting of $20\%$ of the data.  This MCC score
exceeds by a modest margin the performance of randomly guessing,
lending strong evidence to the assertion that PWGC successfully
recovers some meaningful differences between the Granger-causality
graphs of different subjects, particularly if we recall that there are
generally fewer examples for each subject than there are subjects
overall.  Moreover, distinguishing only between two particular
subjects (as opposed to distinguishing between one subject and $118$
others) is substantially easier -- an illustration is provided in
Figure \ref{fig:logistic_regression_2pair} where we embed the data
into the plane through supervised dimensionality reduction.  In
general, fitting a simple classifier between two subjects achieves an
$MCC$ in excess of $0.6$ and up to $0.9$, even when the same
hyperparameters are carried over for different pairs of subjects.

\begin{figure}
  \centering
  \caption{Subject Classification from Granger-causality Graphs}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{Distinguishing Individual Subjects}
    \label{fig:logistic_regression_2pair}
    \includegraphics[width=\linewidth]{logistic_regression_separation.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{Complete Multiclass Classifier}
    \label{fig:logistic_regression_results}
    \includegraphics[width=\linewidth]{logistic_regression_confusion_matrix.pdf}

    {\scriptsize }
  \end{subfigure}

  {\scriptsize Figure \ref{fig:logistic_regression_2pair} illustrates
    discriminating between two particular subjects based on their
    EEG Granger-causality graphs.  In this case $MCC = 0.75$ on held
    out data.  Visualization is constructed by supervised
    dimensionality reduction and is purely illustrative.

    Figure \ref{fig:logistic_regression_results} provides the
    row-normalized confusion matrix (computed on held out validation
    data) of a multiclass logistic regression classifier used to
    classify subjects based on their EEG Granger-causality graphs.  $MCC = 0.20$}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper we have applied the theoretical insights developed in
\cite{my_GC_paper} into a heuristic for estimating Granger-causality
graphs.  We have leveraged the notion of a strongly-causal graph which
enables the recover of causality graphs through simple pairwise
causality testing (Theorem \ref{thm:scg_recovery}), and demonstrated
that substantial improvements (both statistically and computationally)
over LASSO-based heuristics can be obtained.

We emphasize that the improvements are a result of topological
assumptions placed on the causality graph of the system, though
examples (see Figure \ref{fig:gene_networks}) from the literature
suggest that many graphs in practice may have special topological
properties that can be exploited as prior knowledge.  Further research
on the implications of Network properties (i.e. degree distributions,
or small-world properties \todo{cite some network theory}) for
Granger-causality is of interest.  Algorithms exploiting graphical
assumptions may, for example, take the form of specially developed
heuristics (e.g. Algorithm \ref{finite_pwgc}), cunning adaptive
weighting schemes for the LASSO, or formal Bayesian priors.

Our application in Section \ref{sec:application} provides strong
evidence that PWGC (Algorithm \ref{finite_pwgc}) is capable of
uncovering meaningful features from networks of time series data by
constructing a classifier which accurately discriminates between
subjects in an EEG experiment based purely on the Granger-causality
graph topology inferred by the iterated application of PWGC.  This is
a result which may be of independent scientific interest for EEG.

There are a number of potential extensions to this work.  Firstly, it
is known that the information theoretic notion of transfer entropy
reduces to Granger-causality when the data is Gaussian
\cite{barnett2009granger}, can Theorem \ref{thm:scg_recovery} be
generalized in a useful way to causality networks defined by transfer
entropy?  As well, the work of \cite{barnett2015granger} has
established the superiority of Granger-causality testing in state
space models (as opposed to pure autoregressions) in many cases.
Since, our pairwise recovery results do not require $x(t)$ to be
generated by a $\VAR$ model, such results can replace the Hypothesis
tests of Section \ref{sec:pairwise_hypothesis_testing} to enable
application to systems which are not well modelled by finite $\VAR$
models.

\clearpage
\printbibliography
\clearpage
\appendix

\section{Pairwise Causality Graph Estimation}
This section details efficient pairwise causality testing methodology
used in constructing the input data $F, P, \delta$ for Algorithm
\ref{alg:finite_pwgc}.  We employ the simplest reasonable methods in
order to maintain a focus on the Graph topological aspects.

\subsection{Pairwise Hypothesis Testing}
\label{sec:pairwise_hypothesis_testing}
In performing pairwise checks for Granger-causality $x_j \pwgc x_i$ we
follow the simple scheme of estimating the following two linear models:

\begin{align}
  H_0:&\ \widehat{x}_i^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau),\\
  H_1:&\ \widehat{x}_i^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau) + \sum_{\tau = 1}^pb_{ij}(\tau)x_j(t - \tau).
\end{align}

We formulate the statistic 

\begin{equation}
  \label{eqn:gc_statistics}
  F_{ij}(p) = \frac{T}{p}\Big(\frac{\xi_i(p)}{\xi_{ij}(p)} - 1\Big),
\end{equation}

where $\xi_i(p)$ is the sample mean square of the
residuals\footnote{This quantity is often denoted $\widehat{\sigma}$,
  but we maintain notation from Definition
  \ref{def:granger_causality}.}  $x_i(t) - \widehat{x}^{(p)}_i(t)$,

\begin{equation*}
  \xi_i(p) = \frac{1}{T - p}\sum_{t = p + 1}^T (x_i(t) - \widehat{x}_i^{(p)}(t))^2,
\end{equation*}

and similarly for $\xi_{ij}(p)$.  We test $F_{ij}(p)$ against a
$\chi^2(p)$ distribution.

If the estimation procedure is consistent, we will have the following
convergence (in $\P$ or a.s.):

\begin{equation}
  F_{ij}(p) \rightarrow
  \left\{
    \begin{array}{ll}
      0;\ x_j \npwgc x_i\\
      \infty;\ x_j \pwgc x_i
    \end{array}
  \right. \text{ as } T \rightarrow \infty.  % the '.' after \right is necessary.
\end{equation}

In our finite sample implementation (see Algorithm
\ref{alg:finite_pwgc}) we add edges to $\widehat{\gcg}$ in order of
the decreasing magnitude of $F_{ij}$ instead of proceeding backwards
through $P_{k - r}$ in Algorithm \ref{alg:pwgr}.  This makes greater
use of the information provided by the test statistic $F_{ij}$,
moreover, if $x_i \gc x_j$ and $x_j \gc x_k$, it is expected that
$F_{kj} > F_{ji}$, thereby providing the same effect as proceeding
backwards through $P_{k - r}$.
\subsection{Model Order Selection}
\label{sec:model_order_selection}
There are a variety of methods to choose the filter order $p$ (see
e.g. \cite{lutkepohl2005new}), but we will focus in particular on the
Bayesian Information Criteria (BIC).  The BIC is substantially more
conservative than the popular alternative Akaiake Information Criteria
(the BIC is also asymptotically consistent), and since we are
searching for \textit{sparse graphs}, we therefore prefer the BIC,
where we seek to \textit{minimize} over $p$:

\begin{equation}
  \label{eqn:bic}
  \begin{aligned}
    BIC_{\text{univariate}}(p) &= \ln\ \xi_i(p) + p\frac{\ln T}{T},\\
    BIC_{\text{bivariate}}(p) &= \ln \det \widehat{\Sigma}_{ij}(p) + 4p\frac{\ln T}{T},\\
  \end{aligned}
\end{equation}

where $\widehat{\Sigma}_{ij}(p)$ is the $2 \times 2$ residual
covariance matrix for the $\VAR(p)$ model of $(x_i(t), x_j(t))$.  The
bivariate errors $\xi_{ij}(p)$ and $\xi_{ji}(p)$ are the diagonal
entries of $\widehat{\Sigma}_{ij}(p)$.

We carry this out by a simple direct search on each model order
between $0$ and some prescribed $p_\text{max}$, resulting in a
collection $p_{ij}$ of model order estimates.  In practice, it is
sufficient to pick $p_\text{max}$ ad-hoc or via some simple heuristic
e.g. plotting the sequence $BIC(p)$ over $p$, though it is not
technically possible to guarantee that the optimal $p$ is less than
the chosen $p_\text{max}$ (since there can in general be arbitrarily
long lags from one variable to another).

\subsection{Efficient Model Estimation}
\label{sec:efficient_model_estimation}
In practice, the vast majority of computational effort involved in
implementing our estimation algorithm is spent calculating the error
estimates $\xi_i(p_i)$ and $\xi_{ij}(p_{ij})$.  This requires fitting a
total of $n^2p_{\text{max}}$ autoregressive models, where the most
naive algorithm (e.g. solving a least squares problem for each model)
for this task will consume $O(n^2p_{\text{max}}^4T)$ time, it is
possible to carry out this task in a much more modest
$O(n^2p_{\text{max}}^2 ) + O(n^2p_{\text{max}}T)$ time via the
autocorrelation method
\cite{hayes_statistical_digital_signal_processing} which substitutes
the following autocovariance estimates in the Yule-Walker
equations:\footnote{The particular indexing and normalization given in
  equation \ref{eqn:covariance_estimate} is critical to ensure
  $\widehat{R}$ is positive semidefinite.  The estimate can be viewed
  as calculating the covariance sequence of a signal multiplied by a
  rectangular window.}

\begin{equation}
  \label{eqn:covariance_estimate}
  \widehat{R}_x(\tau) = \frac{1}{T}\sum_{t = \tau + 1}^T x(t) x(t - \tau)^\T;\ \tau = 0, \ldots, p_{\text{max}},
\end{equation}

It is imperative that the first index in the summation is $\tau + 1$, as
opposed perhaps to $p_\text{max}$ and that the normalization is
$1 / T$, as opposed perhaps to $1 / (T - p_\text{max})$, in order to
guarantee that $\widehat{R}_x(\tau)$ forms a valid (i.e. positive
definite) covariance sequence.  This results in some bias, however the
dramatic computational speedup is worth it for our purposes.

These covariance estimates constitute the $O(n^2p_{\text{max}}T)$
operation.  Given these particular estimates, the variances $\xi_i(p)$
for $p = 1, \ldots, p_{\text{max}}$ can be evaluated in
$O(p_{\text{max}}^2)$ time each by applying the Levinson-Durbin
recursion to $\widehat{R}_{ii}(\tau)$, which effectively estimates a
sequence of $AR$ models, producing $\xi_i(p)$ as a side-effect (see
\cite{hayes_statistical_digital_signal_processing} and
\cite{levinson_durbin_recursion}).

Similarly, the variance estimates $\widehat{\Sigma}_{ij}(p)$ (which
include $\xi_{ij}$ and $\xi_{ji}$) can be obtained by estimating
$\frac{(n + 1)n}{2}$ bivariate AR models, again in
$O(p_{\text{max}}^2)$ time via Whittle's generalized Levinson-Durbin
recursion\footnote{We have made use of standalone tailor made
  implementations of these algorithms, available at
  \textsf{github.com/RJTK/Levinson-Durbin-Recursion}.}
\cite{whittle_generalized_levinson_durbin}.

\subsection{Edge Probabilities and Error Rate Controls}
\label{sec:error_rate_control}
Denote $F_{ij}$ the Granger-causality statistic of equation
\ref{eqn:gc_statistics} with model orders chosen by the methods of
Section \ref{sec:model_order_selection}.  We assume that this
statistic is asymptotically $\chi^2(p_{ij})$ distributed (the
disturbances are Gaussian), and denote by $G$ the cumulative
distribution function thereof.  We will define the matrix

\begin{equation}
  \label{eqn:edge_inclusion_probability}
  P_{ij} = G(F_{ij}),
\end{equation}

to be the matrix of pairwise edge inclusion P-values.  This is
motivated by the hypothesis test where the hypothesis $H_0$ will be
rejected (and thence we will conclude that $x_j \pwgc x_i$) if
$P_{ij} > 1 - \delta$.

The value $\delta$ can be chosen by a variety of methods, in our case
we apply the Benjamini Hochberg criteria \cite{benjamini_hochberg}
\cite{all_of_statistics} to control the false discovery rate of
pairwise edges to a level $\alpha$ (where we generally take
$\alpha = 0.05$).

\subsection{Finite Sample Recovery Algorithm}
\label{sec:finite_pwgc}

After the graph topology $\widehat{\gcg}$ has been estimated via
Algorithm \ref{alg:finite_pwgc}, we refit the entire model with the
specified sparsity pattern directly via ordinary least squares.

We note that producing graph estimates which are not strongly causal
can potentially be achieved by performing sequential estimates
$\widehat{x}_1(t), \widehat{x}_2(t), \ldots$ estimating a strongly causal
graph with the residuals of the previous model as input, and then
refitting on the combined sparsity pattern.  We experiment with this
heuristic in our example application of Section \ref{sec:application},
but reserve theoretical analysis for future work.

\clearpage
\printbibliography  
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
