\documentclass[12pt]{article}

%% bibliography stuff -- this needs come before the preamble inclusion
\usepackage[backend=bibtex,sorting=none]{biblatex}
\bibliography{\string~/Documents/academics/global_academics/global_bib.bib}
\usepackage{hyperref}

\def\gc{\overset{\text{GC}}{\rightarrow}}
\def\te{\overset{\mathcal{T}}{\rightarrow}}

\input{\string~/Documents/academics/global_academics/latex_preamble}

\graphicspath{{./figures/}}

\title{Structure Learning for $VAR(p)$ Models}
\author{R. J. Kinnear, R. R. Mazumdar}

\begin{document}
\maketitle
\abstract{We study Granger Causality and propose a structure learning
  heuristic for uncovering a parsimonious representation of large
  $\mathsf{VAR}(p)$ models.}

\section{Introduction and Review}
\label{sec:introduction}
Consider a collection of stochastic processes producing observations
at discrete time intervals.  Are the underlying processes dependent?
Can we quantify any of the underlying relationships?  Can the arrow of
time help us to distinguish a directionality or flow of dependence
among our observed series?  In this paper we contribute to the
understanding of the notion of Granger-Causality
\cite{granger1969investigating} as a tool for answering these questions.

Though the notion of causality is a philosophically slippery concept,
it is fundamental to the way we understand the world and to the
progress of science in general.  Indeed, without faith in the
consistency of causal interactions the results of experimental science
could not be generalized or applied in any meaningful way.  In the
case of Granger-Causality, we state that if an event '$A$' provides us
with unique (that is, not available anywhere else) information about a
later event $B$, then $A$ must have a causal impact on $B$.  As
opposed to the notion of Causation promoted by Pearl
\cite{pearl2000art}, this is an entirely model-free notion of cause,
and instead leverages the intuition that a cause must precede it's
effect.

In practice, Granger's notion of causation it not a convincing test
for \textit{true} causation, since our statements about causation are
highly dependent upon the data that we are able to observe.  We prefer
instead to interpret Granger causality as a means of uncovering a flow
of ``information'' or ``energy'' through some underlying graph of
interactions.  Though this graph cannot be observed directly, we will
infer it's presence as a latent structure among our observed time
series data.

Finding the ``best'' graph structure consistent with observed data is
generally an extremely challenging problem, though the comparison of
quality between different structures, and hence the notion of
``best'', needs to be quantified.  In applications where we are
interested merely in minimizing the mean squared error of a linear
one-step-ahead predictor, then we will naturally desire an entirely
dense graph of connections, since each edge can only serve to reduce
estimation error.  However, since the number of edges scales
quadratically in $n$, it is imperative to infer a sparse causality
graph for large systems, both to avoid overfitting observed data, as
well as to aid the interpretability of the results.

In \cite{bach2004learning} the authors apply a local search heuristic
to an AIC penalized approximation of the likelihood where at each
iteration an edge is either added, removed, or reversed.  This is a
common approach to combinatorial optimization due to it's simplicity,
but is liable to get stuck in shallow local minima.

A second and wildly successful heuristic is the LASSO regularizer
\cite{tibshirani1996regression}, which can be understood as a natural
convex relaxation to penalizing the count of the non-zero edges.  The
LASSO enjoys strong theoretical guarantees \cite{wainwright2009sharp},
even in the case of time series data \cite{basu2015}
\cite{wong2016lasso}.

From a Bayesian perspective, spike-and-slab priors can be used to
estimate the probability of an edge's inclusion, after which a graph
can be chosen heuristically by thresholding or searching for the MAP
graph structure.

Our contribution is to provide an alternative heuristic based on
minimum spanning trees, inspired by our result in section
\ref{sec:theory} that the true causality graph can be recovered from
pairwise information alone in the case that the underlying graph is a
directed forest.  We compare this algorithm to a grouped LASSO
\cite{yuan2006model}, local search, and a Bayesian posterior
probability thresholding scheme.

%% -Can we also formulate an SDP relaxation?
%% -Fit trees jointly or via coordinate descent?

\section{Theory}
\label{sec:theory}
Consider a zero mean wide sense stationary, vector valued and discrete
time stochastic process $x(t) \in L^n_2(\Omega);\ t \in \Z$, having the matrix
valued covariance sequence
$R(\tau) \overset{\Delta}{=} \E x(t)x(t - \tau)^\T$ and spectra $S(\omega)$:

\begin{equation}
  \label{eqn:fourier_pair}
  \begin{aligned}
    R(\tau) &= \frac{1}{2\pi}\int_{-\pi}^\pi S(\omega) e^{j\tau\omega}\d \omega,\\
    S(\omega) &= \sum_{\tau=-\infty}^\infty R(\tau)e^{-j\tau\omega}.
  \end{aligned}
\end{equation}

\begin{definition}[Granger Causality]
  For the WSS series $x(t)$ we say that component $x_j$
  \textit{Granger-Causes} (GC) component $x_i$ (with respect to
  $x(t)$) and write $x_j \gc x_i$ if given Hilbert spaces
  $\mathcal{H}^{(i)}_{t - 1}$, $\mathcal{H}^{(i, j)}_{t - 1}$
  generated by $\{x_i(t - \tau)\}_{\tau = 1}^\infty$ and
  $\{x_i(t - \tau), x_j(t - s\}_{\tau, s = 1}^\infty$ respectively we have

\begin{equation}
  \xi[x_i(t) \ |\ \mathcal{H}^{(i)}_{t - 1}] > \xi[x_i(t) \ |\ \mathcal{H}^{(i, j)}_{t - 1}],
\end{equation}

where
$\xi[x \ |\ \mathcal{H}] = \E (x - \hat{\E}[x \ |\ \mathcal{H}])^2$ and
$\hat{\E}[x \ |\ \mathcal{H}] = \text{proj}_{\mathcal{H}}(x)$ denotes
the (unique) projection onto the Hilbert space $\mathcal{H}$.
\end{definition}

This notion captures the idea that the process $x_j$ provides
information about $x_i$ that is not available from elsewhere.  The
caveat ``with respect to $x(t)$'' is important in that GC relations
can change when components are added to or removed from $x(t)$,
e.g. new GC relations can arise if we remove the observations of a
common cause, and existing GC relations can disappear if we observe a
new mediating series.

The notion is closely related to the information theoretic measure of
transfer entropy, indeed, if everything in Gaussian then they are
equivalent \cite{barnett2009granger}.

We continue to review some pertinent facts.

As a consequence of the Wold decomposition theorem \cite{lindquist},
every such sequence has the ``moving average'' $MA(\infty)$ representation

\begin{equation}
\label{eqn:wold}
  x(t) = c(t) + \sum_{\tau = 0}^\infty A(\tau) z(t - \tau);\ z(t) \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, I),
\end{equation}

where $c(t)$ is a perfectly predictable sequence\footnote{$c(t)$ is
  perfectly predictable if any sample $c(t_0)$ is enough to determine
  $c(t)$ for every $t$.  For example,
  $c(t) = \text{sin}(2\pi t + \Theta);\; \Theta \sim \mathcal{U}[-\pi, \pi]$}, and
$z(t) \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, I)$.  We will assume
that $c(t)$ can be effectively estimated and removed and take
$c(t) = 0$.  Furthermore, we assume that the spectra is uniformly
bounded away from $0$:
$\exists \delta:\ \delta I \preceq S(\omega) \preceq \frac{1}{\delta} I\ \forall \omega$ so that the Wold
representation can be inverted to yield the $VAR(\infty)$ form

\begin{equation}
  x(t) = \sum_{\tau = 1}^\infty B(\tau) x(t - \tau) + v(t);\ v(t) \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \Sigma_v).
\end{equation}

\begin{theorem}[Granger Causality Equivalences]
  Given W.S.S. vector valued process $x(t)$ the following are equivalent:

  \begin{enumerate}
    \item{$x_j \gc x_i$}
    \item{$\forall \tau \in \N_+\ B_{ij}(\tau) = 0$}
    \item{$\mathcal{H}_t^{(i)} \perp \mathcal{H}_{t - 1}^{(j)}\ |\ \mathcal{H}_{t - 1}^{(i)}$}
    \item{$\mathcal{F}_{j \rightarrow i} \defeq \ln\frac{\xi_i}{\xi_{ij}} > 0$}
    \item{\hl{Wold $A(\tau)$ condition}}
    \item{Tie in the 0s of $S(\omega)^{-1}$, though this isn't immediately directed.}
  \end{enumerate}
\end{theorem}

The following propositions justify various modifications of $x(t)$
applied in practice to massage $x(t)$ into a form amenable to more
standard tools.

\begin{theorem}[Invariance Under Invertible and Deterministic Modifications]
  Let $F(z), G(z)$ describe univariate and invertible
  linear-time-invariant filters.  And, let $f(t), g(t)$ be perfectly
  predictable processes.  Then,

  \begin{equation}
    x_j \gc x_i \iff F(x_j - f)(t) \gc G(x_i - g)(t)
  \end{equation}
\end{theorem}

\begin{theorem}[General Invariance]
  \hl{Under what conditions is this actually true?}

  Let $\zeta(t)$ be a stationary discrete time stochastic process.  Let
  $F, G$ be (possibly nonlinear and time varying) invertible filtering
  operations and $f(t)$, $g(t)$ be perfectly predictable and such that
  $x_j(t) \defeq F(\zeta_j - f)(t), x_i(t) \defeq G(\zeta_i - g)(t)$ are W.S.S.  Then,

\begin{equation}
    x_j \gc x_i \iff \zeta_j \te \zeta_i
  \end{equation}
\end{theorem}

\begin{theorem}[Tree Recovery]
  \label{thm:tree_recovery}
  If the Granger-Causality graph $\mathcal{G}$ for $x(t)$ is a
  directed forest, then $\mathcal{G}$ can be inferred from pairwise
  causality tests alone.
\end{theorem}

\section{Structure Learning}
\subsection{Edge-Wise Grouped LASSO}
\subsection{Bayesian Posterior Thresholding}
Consider the Bayesian model

\begin{equation}
  \begin{aligned}
    (x(t)\ |\ B, \sigma_v^2, \{x(t - \tau)\}_{\tau = 1}^p) &\sim \mathcal{N}(\sum_{\tau = 1}^pB(\tau)x(t - \tau), \sigma_v^2)\\
    (B_{ij}(\tau)\ |\ G_{ij}(\tau)) &\sim G_{ij}(\tau)\mathcal{N}(0, \sigma_\beta^2) + (1 - G_{ij}(\tau))\delta_0(B_{ij}(\tau))\\
    (G_{ij}(\tau)) &\sim \mathsf{BER}(p_{ij}(\tau))\\
    \sigma_v^2 &\sim \Gamma(a_v, b_v)\\
    \sigma_\beta^2 &\sim \Gamma(a_\beta, b_\beta)
  \end{aligned}
\end{equation}

Similar models have been studied by various authors in the context of the linear regression model \hl{[(cite them)]} where it is referred to as \textit{stochastic variable selection}, or referred to as a model for Bayesian variable selection.

Since we have in mind applications where $n$ is large, sampling posterior probabilities can be prohibitively burdensome, so we can instead fit the mean-field variational approximation $p(B, G, \sigma\ |\ X) \approx q_Gq_Bq_\sigma$ and subsequently apply a thresholding operation to the posterior edge inclusion probabilities under $q_G$.

\subsection{Pairwise Minimum Error Spanning Trees}
Inspired by theorem \ref{thm:tree_recovery}, we propose the following
structure learning heuristic.

%% This algorithm should perform coordinate descent or jointly refit all the trees.

\begin{enumerate}
  \item{Set $k = 0$ and $x^0(t) = x(t)$}
  \item{Compute all pairwise causality measures $\Xi \defeq \big[\ln \frac{\xi_i}{\xi_{ij}} \big]_{i, j}$}
  \item{Find the maximum spanning arborescence\footnote{An ``arborescence'' is a french word for a tree diagram, and refers to a \textit{directed} tree in graph theory} $\mathcal{T}_k$ of a graph having edge weights $\Xi_{ij}$.}
  \item{Fit a vector LTI filter $F_k$ having graph defined by $\mathcal{T}_k$ to $x^{k - 1}(t)$ and set $x^k(t) = x^{k - 1}(t) - \hat{x}^{k - 1}(t)$}
  \item{Repeat over $k$ until a stopping criteria is met}
\end{enumerate}


\section{Application}
\section{Conclusion}

\printbibliography
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
