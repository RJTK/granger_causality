\documentclass[12pt]{article}

% TODO: read this https://lemire.me/blog/rules-to-write-a-good-research-paper/

% TODO:
% -- define "DAG"
% -- remove speculative statements
% -- 

%% bibliography stuff -- this needs come before the preamble inclusion
\usepackage[backend=bibtex,sorting=none]{biblatex}
\bibliography{\string~/Documents/academics/global_academics/global_bib.bib}
\usepackage{hyperref}

\def\gc{\overset{\text{GC}}{\rightarrow}}  % Granger causality arrow
\def\ngc{\overset{\text{GC}}{\nrightarrow}}  % Negated Granger causality arrow
\def\pwgc{\overset{\text{PW}}{\rightarrow}}  % Pairwise Granger causality arrow
\def\npwgc{\overset{\text{PW}}{\nrightarrow}}  % Negated pairwise Granger causality arrow
\def\te{\overset{\mathcal{T}}{\rightarrow}}  % Transfer entropy arrow
\def\gcg{\mathcal{G}}  % Granger-causality graph
\def\VAR{\mathsf{VAR}}  % VAR(p) model
\def\B{\mathsf{B}}  % Filter B
\def\wtB{\widetilde{\B}}  % General filter B
\def\A{\mathsf{A}}  % Filter A
\def\H{\mathcal{H}}  % Hilbert space

\newcommand{\linE}[2]{\hat{\E}[#1\ |\ #2]}  % Linear projection
\newcommand{\linEerr}[2]{\xi[#1\ |\ #2]}  % Error of linear projection
\newcommand{\pa}[1]{pa(#1)}  % Parents of a node
\newcommand{\anc}[1]{\mathcal{A}(#1)}  % Ancestors of a node
\newcommand{\ancn}[2]{\mathcal{A}_{#1}(#2)}  % nth ancestors of a node
\newcommand{\gpn}[2]{gp_{#1}(#2)}  % nth generation grandparents
\newcommand{\wtalpha}[2]{\widetilde{\alpha}(#1, #2)}  % Some notation for lem:pwgc_anc
\newcommand{\dist}[2]{\mathsf{d}(#1, #2)}  % Distance between things
\newcommand{\gcgpath}[2]{#1 \rightarrow \cdots \rightarrow #2}  % A shorter path command

\input{\string~/Documents/academics/global_academics/latex_preamble}

\graphicspath{{../figures/}}

\title{Structure Learning for $\VAR(p)$ Models}
\author{R. J. Kinnear, R. R. Mazumdar}

\begin{document}
\maketitle
\abstract{We study Granger Causality and propose a structure learning
  heuristic for uncovering a parsimonious representation of large
  $\mathsf{VAR}(p)$ models.}

\section{Introduction and Review}
\label{sec:introduction}
Consider a collection of stochastic processes producing observations
at discrete time intervals.  Are the underlying processes dependent?
Can we quantify any of the underlying relationships?  Can the arrow of
time help us to distinguish a directionality or flow of dependence
among our observed series?  In this paper we contribute to the
understanding of the notion of Granger-Causality
\cite{granger1969investigating} as a tool for answering these questions.

Though the notion of causality is a philosophically slippery concept,
it is fundamental to the way we understand the world and to the
progress of science in general.  Indeed, without faith in the
consistency of causal interactions the results of experimental science
could not be generalized or applied in any meaningful way.  In the
case of Granger-Causality, we state that if an event '$A$' provides us
with unique (that is, not available anywhere else) information about a
later event $B$, then $A$ must have a causal impact on $B$.  As
opposed to the notion of Causation promoted by Pearl
\cite{pearl2000art}, this is an entirely model-free notion of cause,
and instead leverages the intuition that a cause must precede it's
effect.

In practice, Granger's notion of causation it not a convincing test
for \textit{true} causation, since our statements about causation are
highly dependent upon the data that we are able to observe.  We prefer
instead to interpret Granger causality as a means of uncovering a flow
of ``information'' or ``energy'' through some underlying graph of
interactions.  Though this graph cannot be observed directly, we will
infer it's presence as a latent structure among our observed time
series data.

Finding the ``best'' graph structure consistent with observed data is
generally an extremely challenging problem, though the comparison of
quality between different structures, and hence the notion of
``best'', needs to be quantified.  In applications where we are
interested merely in minimizing the mean squared error of a linear
one-step-ahead predictor, then we will naturally desire an entirely
dense graph of connections, since each edge can only serve to reduce
estimation error.  However, since the number of edges scales
quadratically in $n$, it is imperative to infer a sparse causality
graph for large systems, both to avoid overfitting observed data, as
well as to aid the interpretability of the results.

In \cite{bach2004learning} the authors apply a local search heuristic
to an AIC penalized approximation of the likelihood where at each
iteration an edge is either added, removed, or reversed.  This is a
common approach to combinatorial optimization due to it's simplicity,
but is liable to get stuck in shallow local minima.

A second and wildly successful heuristic is the LASSO regularizer
\cite{tibshirani1996regression}, which can be understood as a natural
convex relaxation to penalizing the count of the non-zero edges.  The
LASSO enjoys strong theoretical guarantees \cite{wainwright2009sharp},
even in the case of time series data \cite{basu2015}
\cite{wong2016lasso}.

From a Bayesian perspective, spike-and-slab priors can be used to
estimate the probability of an edge's inclusion, after which a graph
can be chosen heuristically by thresholding or searching for the MAP
graph structure.

Our contribution is to provide an alternative heuristic based on
minimum spanning trees, inspired by our result in section
\ref{sec:theory} that the true causality graph can be recovered from
pairwise information alone in the case that the underlying graph is a
directed forest.  We compare this algorithm to a grouped LASSO
\cite{yuan2006model}, local search, and a Bayesian posterior
probability thresholding scheme.

%% -Can we also formulate an SDP relaxation?
%% -Fit trees jointly or via coordinate descent?

\section{Theory}
\label{sec:theory}
\subsection{Formal Setting}
Consider the space $L_2(\Omega)$, the usual Hilbert space of finite
variance random variables over a probability space
$(\Omega, \mathcal{F}, \mathbb{P})$ having inner product
$\inner{x}{y} = \E[xy]$.  We will denote the collection vectors of
such random elements as $L_2^n(\Omega)$ or simply $L_2^n$.

We will work with a discrete time and wide-sense stationary process
$x(t)$ taking elements in $L_2$.  We suppose that $x(t)$ has zero
mean, $\E x(t) = 0$, and the matrix valued covariance sequence
$R(\tau) \overset{\Delta}{=} \E x(t)x(t - \tau)^\T$ is absolutely summable and has a spectra $S(\omega)$:

\begin{equation}
  \label{eqn:fourier_pair}
  \begin{aligned}
    R(\tau) &= \frac{1}{2\pi}\int_{-\pi}^\pi S(\omega) e^{j\tau\omega}\d \omega,\\
    S(\omega) &= \sum_{\tau=-\infty}^\infty R(\tau)e^{-j\tau\omega}.
  \end{aligned}
\end{equation}

We will also work frequently with the spaces spanned by the values of
such a process

\begin{equation}
  \label{eq:hilbert_space_defn}
  \begin{aligned}
    \H^x &= \cl \{\sum_{\tau = -T}^T a_\tau^\T x(t - \tau)\ |\ a_\tau \in \R^n, T \in \N\} \subseteq L_2(\Omega),\\
    \H_t^x &= \cl \{\sum_{\tau = 0}^T a_\tau^\T x(t - \tau)\ |\ a_\tau \in \R^n, T \in \N\} \subseteq L_2(\Omega),
  \end{aligned}
\end{equation}

where the closure is naturally in mean-square.  $\H_t^x$ is the space
spanned by the values of $x(t)$ between the ``infinite past'' and
``now'', we will often omit the superscript $x$ which should be clear
from context.  Evidently these spaces are separable and as closed
subspaces of a Hilbert space they are themselves Hilbert.  We will
denote the spaces generated in analagous ways by particular components
of $x$ as e.g. $\H_t^{(i, j)}$, $\H_t^{i}$ or by all but particular
components as e.g. $\H_t^{-j}$.

As a consequence of the Wold decomposition theorem \cite{lindquist},
every such sequence has the ``moving average'' $MA(\infty)$
representation

\begin{equation}
\label{eqn:wold}
  x(t) = c(t) + \sum_{\tau = 0}^\infty A(\tau) w(t - \tau),
\end{equation}

where $c(t)$ is a perfectly predictable sequence\footnote{$c(t)$ is
  perfectly predictable if any sample $c(t_0)$ is enough to determine
  $c(t)$ for every $t$.  For example,
  $c(t) = \text{sin}(2\pi t + \Theta);\; \Theta \sim \mathcal{U}[-\pi,
  \pi]$}, and $w(t)$ is an uncorrelated sequence having a diagonal
covariance and $A(0) = I$.  We will assume that $c(t) = 0$, which in
practice is to say that it has been accurately estimated and removed.

Furthermore, we assume that the spectra is uniformly bounded away from
$0$:
$\exists \delta:\ \delta I \preceq S(\omega) \preceq \frac{1}{\delta}
I\ \forall \omega$ so that the Wold representation can be inverted to
yield the $VAR(\infty)$ form

\begin{equation}
  \label{eqn:ar_representation}
  x(t) = \sum_{\tau = 1}^\infty B(\tau) x(t - \tau) + v(t),
\end{equation}

where $v(t)$ is an uncorrelated sequence with covariance $\Sigma_v$.

The equations (\ref{eqn:wold}), (\ref{eqn:ar_representation}) can be
represented as $x(t) = \A(z)w(t) = \B(z)x(t) + v(t)$ via the action of
the operators (LTI filters)
$\A(z) \defeq \sum_{\tau = 0}^\infty A(\tau)z^{-\tau}$ and
$\B(z) \defeq \sum_{\tau = 1}^\infty B(\tau)z^{-\tau}$ where the
operator $z^{-1}$ is the backshift operator acting on $\H$.  Since
$||z^{-1}|| = 1$, the inversion formulae

\begin{equation}
  \label{eqn:lsi_inversion}
  \A(z) = (I - \B(z))^{-1} = \sum_{k = 0}^\infty \B(z)^k
\end{equation}

are valid.
\subsection{Granger Causality}

\begin{definition}[Granger Causality]
  For the WSS series $x(t)$ we say that component $x_j$
  \textit{Granger-Causes} (GC) component $x_i$ (with respect to $x$)
  and write $x_j \gc x_i$ if given Hilbert spaces
  $\H_{t - 1}$, $\H^{-j}_{t - 1}$

\begin{equation}
  \linEerr{x_i(t)}{\H_{t - 1}} < \linEerr{x_i(t)}{\H^{-j}_{t - 1}},
\end{equation}

where $\xi[x \ |\ \H] = \E (x - \linE{x}{\H})^2$ is the mean squared
estimation error and $\linE{x}{\H} = \text{proj}_{\H}(x)$ denotes the
(unique) projection onto the Hilbert space $\H$.
\end{definition}

This notion captures the idea that the process $x_j$ provides
information about $x_i$ that is not available from elsewhere.  The
caveat ``with respect to $x$'' is important in that GC relations can
change when components are added to or removed from our collection $x$
of observations, e.g. new GC relations can arise if we remove the
observations of a common cause, and existing GC relations can
disappear if we observe a new mediating series.

The notion is closely related to the information theoretic measure of
transfer entropy, indeed, if everything is Gaussian then they are
equivalent \cite{barnett2009granger}.

\begin{theorem}[Granger Causality Equivalences]
  \label{thm:granger_causality_equivalences}
  Let $x(t)$ be a WSS process with absolutely summable covariance
  sequence, and spectral density uniformly bounded above and below.  Denote by
  $\xi_{ij} \defeq \linEerr{x_i(t)}{\H^{-j}}$ and
  $\xi_i \defeq \linEerr{x_i(t)}{\H_t}$.  Then, the following are equivalent:

  \begin{enumerate}
    \item{$x_j \ngc x_i$}
    \item{$\forall \tau \in \N_+\ B_{ij}(\tau) = 0$}
    \item{$\mathcal{F}_{j \rightarrow i} \defeq \ln\frac{\xi_i}{\xi_{ij}} = 0$}
    \item{$\H_t^{i} \perp \H_{t - 1}^{j}\ |\ \H_{t - 1}^{-j}$}
    \item{$\linE{x_i(t)}{\H_{t - 1}^{-j}} = \linE{x_i(t)}{\H_{t - 1}}$}
    % \item{\hl{Wold $A(\tau)$ condition}}
    % \item{Tie in the 0s of $S(\omega)^{-1}$, though this isn't immediately directed.}
  \end{enumerate}
\end{theorem}

Statement (4) means that
$(y - \linE{y}{\H_{t - 1}^{-j}}) \perp (z - \linE{z}{\H_{t -
    1}^{-j}})\ \forall y \in \H_t^i,\ z \in \H_{t - 1}^j$, which is an
appealing geometric statement that there is no direct feedback from
$x_j$ to $x_i$ \cite{lindquist}.

\begin{proof}
  The equivalence $(1) \iff (3)$ is essentially a restatement of the
  definition, but is in line with the seminal work of Geweke
  \cite{geweke1982measurement}, \cite{geweke1984}.

  The equivalence $(3) \iff (5)$ is also immediate from the
  definitions i.e. the projections are equal if and only if the errors
  are equal.

  $(1) \Rightarrow (2)$: The projection for $x_i(t)$ onto $\H_{t - 1}$ is
  (by definition) given by a form similar to equation
  \ref{eqn:ar_representation}:

\[
  \E|x_i(t) - \linE{x_i(t)}{\H_{t - 1}}|^2 = \E|v_i(t) + \sum_{\tau = 1}^\infty \sum_{k = 1}^n (B_{i, k}(\tau) - \hat{B}_{i, k}(\tau))x_k(t - \tau)|^2.
\]

Since $v(t)$ in equation \ref{eqn:ar_representation} is temporally
uncorrelated, it follows that the optimal projection is given by the
model coefficients themselves.  This holds similarly for the
projection onto $\H_{t - 1}^{-j}$.  Then by $(1)$ we have

\[
  \E|x_i(t) - \sum_{\tau = 1}^\infty \sum_{k = 1}^n B_{i, k}(\tau)x_k(t - \tau)|^2 = \E|x_i(t) - \sum_{\tau = 1}^\infty\sum_{k \ne j}B_{i, k}(\tau)x_k(t - \tau)|^2
\]

By the uniqueness of the projection we must have
$\forall \tau\ B_{i, j}(\tau) = 0$.

$(2) \Rightarrow (4)$: In computing $(y - \linE{y}{\H_{t = 1}^{-j}})$ for
$y \in \H_t^i$ it is sufficient to consider $y = x_i(t)$ since
$\H_{t - 1}^i \subseteq \H_{t - 1}^{-j}$, in which case
$(x_i(t) - \linE{x_i(t)}{\H_{t = 1}^{-j}}) = v_i(t)$.  $(4)$ follows
since $v_i(t) \perp \H_{t - 1}$ and
$\forall z \in \H_{t - 1}^j\ (z - \linE{z}{\H_{t - 1}^{-j}}) \in \H_{t
  - 1}$

\hl{TODO: $(4) \Rightarrow (1)$}
  
\end{proof}

% The following propositions justify various modifications of $x(t)$
% applied in practice to massage $x(t)$ into a form amenable to more
% standard tools.

% \begin{theorem}[General Invariance]
%   \hl{Under what conditions is this actually true?}

%   Let $\zeta(t)$ be a stationary discrete time stochastic process.  Let
%   $F, G$ be (possibly nonlinear and time varying) invertible filtering
%   operations and $f(t)$, $g(t)$ be perfectly predictable and such that
%   $x_j(t) \defeq F(\zeta_j - f)(t), x_i(t) \defeq G(\zeta_i - g)(t)$ are W.S.S.  Then,

% \begin{equation}
%     x_j \gc x_i \iff \zeta_j \te \zeta_i
%   \end{equation}
% \end{theorem}

% \begin{proposition}[Invariance Under Invertible and Deterministic Modifications]
%   Let $x(t)$ be a WSS process with absolutely summable covariance
%   sequence, and spectral density uniformly bounded above and below.  Let
%   $F(z), G$ represent univariate and invertible
%   linear-time-invariant filters.  And, let $f(t), g(t)$ be perfectly
%   predictable processes.  Then,

%   \begin{equation}
%     x_j \gc x_i \iff F(x_j - f)(t) \gc G(x_i - g)(t)
%   \end{equation}

% \begin{proof}
%   \hl{TODO}
% \end{proof}
% \end{proposition}

\subsection{Pairwise Granger Causality}
\label{sec:pwgc}
Recall that Granger-causality in general must be understood with
respect to a particular universe of observations.  If $x_j \gc x_i$
with respect to $x_{-k}$, it may not hold with respect to $x$.  For
example, $x_k$ may be a common ancestor which when observed completely
explains the connection from $x_j$ to $x_i$.  In particular, we will
study \textit{pairwise} Granger-causality.

\begin{definition}[Pairwise Granger-causality]
  We will say that $x_j$ pairwise Granger-causes $x_i$ and write
  $x_j \pwgc x_i$ if $x_i$ Granger-causes $x_j$ with respect only to
  $(x_i, x_j)$.
\end{definition}

This notion is of interest for a variety of reasons.  From a purely
theoretical or conceptual standpoint, we will seek to understand when
and how pairwise causality can capture the idea of ``flow of
information'' in the underlying graph.  It will also be useful to
reason about what the conditions under which \textit{unobserved}
components of $x(t)$ may or may not interfere with inference in the
actually observed components.  Finally, we will seek to construct
practical estimation procedures based purely on pairwise causality
tests and understand their properties.

We first need to establish some graph theoretic notation and
terminology, collected formally in definitions for the reader's
convenient reference.

\begin{definition}[Graph Theory Review]
  A \textit{graph} $\gcg = (\mathcal{V}, \mathcal{E})$ is simply a
  tuple of sets respectively called \textit{nodes} and \textit{edges}.
  Throughout this paper, we have in all cases
  $V = [n] \defeq \{1, 2, \ldots, n\}$.  We will also focus solely on
  \textit{directed} graphs, where the edges
  $\mathcal{E} \subseteq V \times V$ are \textit{ordered} pairs.

  A (directed) \textit{path} (of length $r$) between nodes $i, j$,
  denoted $\gcgpath{i}{j}$, is a sequence
  $a_0, a_1, \ldots, a_{r - 1}, a_r$ with $a_0 = i$ and $a_r = j$ such
  that $\forall\ 0 \le k \le r\ (a_k, a_{k + 1}) \in \mathcal{E}$.

  A \textit{cycle} is a path of length $2$ or more between a node and
  itself.  An edge between a node and itself $(i, i)$ (which is not a
  cycle) is referred to as a \textit{loop}.

  A graph $\gcg$ is a \textit{directed acyclic graph} (DAG) if it is a
  directed graph and does not contain any cycles.
\end{definition}

\begin{definition}[Causality graph, Parents]

Remaining in the context of a collection of $n$ WSS processes
$x(t) = \big(x_1(t), \ldots, x_n(t)\big)$, we define the Granger-causality
graph $\gcg$ to be the directed graph formed on $n$ vertices where an
edge $(j, i) \in \gcg$ if and only if $x_j \gc x_i$.

We refer to the set of \textit{parents} of $x_i$ in $\gcg$ by
$\pa{i}$, that is, $j \in \pa{i} \iff x_j \gc x_i$.
\end{definition}

\begin{definition}[Grandparents, ancestors]
  The set of level $\ell$ \textit{grandparents} of node $i$, denoted
  $\gpn{\ell}{i}$, is the set such that $j \in \gpn{\ell}{i}$ if and
  only if there is a \textit{directed path} of length $\ell$ in $\gcg$
  from $j$ to $i$.  Note that a node can be it's own grandparent
  unless $\gcg$ is a DAG.  Clearly, $\pa{i} = \gpn{1}{i}$.

  Finally, the set of \textit{level $\ell$ ancestors} of $i$:
  $\ancn{\ell}{i} = \bigcup_{\lambda \le \ell}\gpn{\lambda}{i}$ is the
  set such that $j \in \ancn{\ell}{i}$ if and only if there is a
  directed path of length $\ell$ \textit{or less} in $\gcg$ from $j$
  to $i$.  The set of \textit{all ancestors} of $i$
  (i.e. $\ancn{n}{i}$) is denoted simply $\anc{i}$.

  Note that since loops are not considered paths, it is not possible
  for a node to be it's own parent.
\end{definition}

The edges of the Granger-causality graph $\gcg$ can be given a general
notion of ``weight'' by associating an edge $(j, i)$ with a
\textit{strictly causal} LTI filter

\begin{equation}
  \label{eqn:filter}
  \B_{ij}(z) = \sum_{\tau = 1}^{\infty} B_{ij}(\tau)z^{-\tau}
\end{equation}

from the $\VAR$ representation of equation
\ref{eqn:ar_representation}.  We will represent the action
(convolution) of this filter compactly by taking $z^{-1}$ as the
backshift operator on $\ell_2^n(\Omega, \mathcal{F}, \mathbb{P})$ when
convenient

\begin{equation}
  \sum_{\tau = 1}^\infty B_{ij}(\tau)x_j(t - \tau) \defeq \B_{ij}(z)x_j(t).
\end{equation}

% What if we take this as the definition of PWGC?  then, in a simply causal graph
% the "normal" definition becomes a theorem.  Or, if lem:pwgc_anc is true then
% maybe that is good enough?

From the $\VAR$ representation of $x(t)$ there is clearly a tight
relationship between each node and it's parent nodes, indeed

\begin{equation}
  \label{eqn:parent_expansion}
  x_i(t) = v_i(t) + \B_{ii}(z)x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t).
\end{equation}

% When is this guaranteed w.r.t. the full system?
Furthermore, if the filter $\B_{ii}$ is invertible, we can write

\begin{equation*}
  \begin{aligned}
    (1 - \B_{ii}(z))x_i(t) &= v_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t)\\
    \Rightarrow x_i(t) &= (1 - \B_{ii}(z))^{-1}\big[v_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t)\big]\\
    &= \wtB_{ii}(z)v_i(t) + \sum_{k \in \pa{i}}\wtB_{ik}(z)x_k(t)
  \end{aligned}
\end{equation*}

% In general, we will write $\wtB$ for arbitrary filters maintaining the
% convention that $\wtB_{ii}$ is a filter that modifies the future of
% $x_i$ from it's past and $\wtB_{ij}$ a filter modifying $x_i$ from the
% past of $x_j$.  We will use the convention that the filter
% $\widetilde{\B}$ will represent simply ``a filter'' and is not
% necessarily the same as the filters serving as edge weights in $\gcg$
% (indeed, $\wtB$ will often be $0$), and moreover, we will allow for
% $\widetilde{\B}$ to potentially change from place to place and even
% from line to line in the same sequence of calculations as it's exact
% properties or specification are not important.

We continue to develop the relationship between $x_i$ and it's family tree.

\begin{proposition}[Ancestor Expansion]
  \label{prop:parent_expanding}
  The component $x_i(t)$ of $x(t)$ can be represented as a sum of
  linear filtering operations acting on the driving noise $v_i$ of
  $x_i$ as well as the parents of $x_i$:

  \begin{equation}
    x_i(t) = \wtB_{ii}(z)v_i(t) + \sum_{k \in \pa{i}}\wtB_{ik}(z)x_k(t),
  \end{equation}

  where $\wtB_{ii}(z) = (1 - \B_{ii}(z))^{-1}$ and
  $\wtB_{ik}(z) = \wtB_{ii}(z) B_{ik}(z)$.

  Moreover, $x_i$ can be expanded in terms of it's ancestors and
  driving noise only:

  \begin{equation}
    \label{eqn:ancestor_expansion}
    x_i(t) = \A_{ii}(z)v_i(t) + \sum_{\substack{k \in \anc{i} \\ k \ne i}}\A_{ik}(z)v_k(t),
  \end{equation}

  where $\A(z) = \sum_{\tau = 0}^\infty A(\tau)z^{-\tau}$ is the filter from
  the Wold decomposition representation of $x(t)$, equation
  (\ref{eqn:wold}).
\end{proposition}

This statement is ultimately about the sparsity pattern in the Wold
decomposition matrices $A(\tau)$ since
$x_i(t) = \sum_{\tau = 0}^\infty \sum_{j = 1}^n a_{ij}(\tau)v_j(t -
\tau)$.  The proposition states that $\A_{ij}(\tau) = 0$ unless
$j \in \anc{i}$.

% \begin{proof}
%   The formula (\ref{eqn:parent_expansion}) was established in the
%   preceding paragraph and serves as a starting point for the induction
%   hypothesis:

%   \begin{equation}
%     \label{eqn:induction}
%     x_i(t) = \wtB_{ii}(z)v_i(t) + \sum_{k \in \ancn{\ell - 1}{i}\setminus \{i\}}\wtB_{ik}(z)v_k(t) + \sum_{k \in \gpn{\ell}{i} \setminus \{i\}}\wtB_{ik}(z)x_k(t).
%   \end{equation}

%   The base case follows by expanding each $x_k$ in equation \ref{eqn:induction} via equation \ref{eqn:parent_expansion}, but we skip the calculations since they are nearly identical to the following induction step:

%   \begin{align*}
%     x_i(t) &= \wtB_{ii}(z)v_i(t) + \sum_{k \in \ancn{\ell - 1}{i}\setminus \{i\}}\wtB_{ik}(z)v_k(t) + \sum_{k \in \gpn{\ell}{i} \setminus \{i\}}\wtB_{ik}(z)x_k(t)\\
%            &\overset{(a)}{=} \wtB_{ii}(z)v_i(t) + \sum_{k \in \ancn{\ell - 1}{i}\setminus \{i\}}\wtB_{ik}(z)v_k(t) + \sum_{k \in \gpn{\ell}{i} \setminus \{i\}}\wtB_{ik}(z)\big[\wtB_{kk}v_k(t)\\
%     &\ \ \ \  + \wtB_{ki}(z)x_i(t) + \sum_{h \in \pa{k}\setminus \{i\}}\wtB_{kh}(z)x_h(t)\big]\\
%     &\overset{(b)}{=} \wtB_{ii}(z)v_i(t) + \sum_{k \in \ancn{\ell}{i} \setminus \{i\}}\wtB_{ik}(z)v_k(t) + \sum_{k \in \gpn{\ell + 1}{i} \setminus \{i\}}\wtB_{ik}(z)x_k(t).
%   \end{align*}

%   Where equality $(a)$ follows by expanding each $x_k$ with equation \ref{eqn:parent_expansion} and $(b)$ requires the inversion of $\big(1 - \sum_{k \in \gpn{\ell}{i}\setminus\{i\}}\wtB_{ik}(z)\wtB_{ki}(z)\big)$ as well as using the fact that $\ancn{\ell}{i} = \ancn{\ell - 1}{i}\cup\gpn{\ell}{i}$.  We here remind the reader of our earlier established convention that $\wtB$ simply represents the existence of a filter (possibly 0) and can change from place to place.

% The induction terminates after $n$ steps since in a graph with $n$ nodes $\gpn{n + 1}{i} = \emptyset$

% % Where step $(a)$ requires the inversion of $1 - \sum_{k \in \pa{i}}\wtB_{ik}(z)\wtB_{ki}(z)$.

% %   \begin{align*}
% %     x_i(t) &= \wtB_{ii}(z)v_i(t) + \sum_{k \in \pa{i}}\wtB_{ik}(z)x_k(t)\\
% %            &= \wtB_{ii}(z)v_i(t) + \sum_{k \in \pa{i}}\wtB_{ik}(z)\big[\wtB_{kk}v_{k}(t) + \wtB_{ki}(z)x_i(t) + \sum_{h \in \pa{k}\setminus \{i\}}\wtB_{kh}(z)x_h(t) \big]\\
% %            &\overset{(a)}{=} \wtB_{ii}(z)v_i(t) + \sum_{k \in \pa{i}}\wtB_{ik}(z)v_{k}(t) + \sum_{k \in \gpn{2}{i} \setminus \{i\}}\wtB_{ik}(z)x_k(t)
% %   \end{align*}

% % Where step $(a)$ requires the inversion of $1 - \sum_{k \in \pa{i}}\wtB_{ik}(z)\wtB_{ki}(z)$.

% \end{proof}
% \textbf{this is an alternative proof, I think superior to the one above}.

The matrix $\B(z)$ is analagous to a \textit{weighted adjacency
  matrix} for the graph $\gcg$.  And, in the same way that the
$k^{\text{th}}$ power of an adjacency matrix counts the number of
paths of length $k$ between nodes, $(\B(z)^k)_{ij}$ is a filter
isolating the ``action'' of $j$ on $i$ at a time lag of $k$ steps.
Though we only need the sparsity pattern of $\B(z)^k$ for the proof.

We state the following well known theorem as a lemma, proof follows
easily by induction.

\begin{lemma}
  \label{lem:adj_matrix}
  Let $S$ be the transposed adjacency matrix of the Granger-causality
  graph $\gcg$.  Then, $(S^k)_{ij}$ is the number of paths of length
  $k$ from node $j$ to node $i$.  Evidently, if
  $\forall k \in \N,\ (S^k)_{ij} = 0$ then $j \not\in \anc{i}$.
\end{lemma}

We now prove the proposition.

\begin{proof}
  From equation (\ref{eqn:ar_representation}), which we are assuming
  throughout the paper to be invertible, we can write

  \begin{equation*}
    x(t) = (I - \B(z))^{-1} v(t),
  \end{equation*}

  where $(I - \B(z))^{-1} = \A(z)$ due to the uniqueness of
  (\ref{eqn:wold}).  Since $\B(z)$ is stable, that is
  $|\lambda_{\text{max}}(B(z))| < 1$, for every $|z| \le 1$ we have

  \begin{equation}
    \label{eqn:resolvant_inv}
    (I - \B(z))^{-1} = \sum_{k = 0}^\infty \B(z)^k.
  \end{equation}

  Now, let $S$ be a $0-1$ matrix containing the sparsity pattern of
  $\B(z)$.  Then from lemma \ref{lem:adj_matrix} $S$ is the transpose
  of the adjacency matrix for $\gcg$ and hence $(S^k)_{ij}$ is
  non-zero if and only if $j \in \gpn{k}{i}$, and therefore
  $\B(z)^k_{ij} = 0$ if $j \not \in \gpn{k}{i}$.  The Cayley-Hamilton
  theorem can then be applied to represent the infinite sum of equation
  (\ref{eqn:resolvant_inv}) in terms only of \textit{finite} powers of
  $\B(z)$, so the application of lemma \ref{lem:adj_matrix} in the
  limit is not necessary.  Finally, since $\anc{i} = \cup_{k = 1}^n\gpn{k}{i}$
  we see that $\A(z)_{ij}$ is zero if $j \not\in \anc{i}$.

  Thence

  \begin{align*}
    x_i(t) &= (I - \B(z))^{-1}v(t)\\
    &= \sum_{j = 1}^n \A_{ij}(z) v_j(t)\\
    &= \A_{ii}(z) v_i(t) + \sum_{\substack{j \in \anc{i} \\ j \ne i}} \A_{ij}(z) v_j(t)
  \end{align*}
\end{proof}

Before moving onto our next proposition, we need two lemmas formalizing
some intuitively apparent facts.

\begin{lemma}
  \label{lem:subspace_sum_projection}
  Let $x \in \H$ where $\H$ is a Hilbert space having inner product $\inner{\cdot}{\cdot}$ and let $\{\H_n\}_{n = 0}^\infty$ be a collection of subspaces of $\H$ such that $x \perp \H_0$.  Then

  \begin{equation*}
    \linE{x}{\bigvee_{n = 0}^\infty\H_n} = \linE{x}{\bigvee_{n = 1}^\infty\H_n},
  \end{equation*}

  where $\H_1 \vee \H_2 = \cl \{\alpha + \beta\ |\ \alpha \in \H_1, \beta \in \H_2 \}$ is the closed sum of subspaces.
\end{lemma}
\begin{proof}
  We can apply the Gram-schmidt process to the collection of subspaces to obtain a sequence of orthogonal subspaces $\widetilde{\H}_0, \widetilde{\H}_1, \ldots$ with $\widetilde{\H}_0 = \H_0$, $\widetilde{\H}_n \subseteq \H_n$, and $\bigvee_{n = 0}^\infty\H_n = \bigoplus_{n = 0}^\infty \widetilde{\H}_n$, where $\oplus$ indicates the orthogonal sum of subspaces (which is always closed).  Then, we have an explicit representation of the projection

  \begin{align*}
    \linE{x}{\bigvee_{n = 0}^\infty\H_n} &= \sum_{n = 0}^\infty \inner{x}{x_n} x_n\\
    &\overset{(a)}{=} \sum_{n = 1}^\infty \inner{x}{x_n} x_n,
  \end{align*}

  where $x_n \in \widetilde{\H}_n \subseteq \H_n$ and $(a)$ follows because of our assumption that $x \perp \H_0$.  This establishes that the projection is infact a member of the smaller subspace $\bigvee_{n = 1}^\infty\H_n$, and since the error can only increase by projecting onto this smaller subspace, we have the conclusion.
\end{proof}

The following lemma is reproduced from \cite{lindquist} here for convenience.

\begin{lemma}[\cite{lindquist}]
  \label{lem:conditional_orthogonality_equivalence}
  Consider three closed subspaces of a Hilbert space $\mathcal{A}$, $\mathcal{B}$, $\mathcal{X}$.  The following statements are equivalent

  \begin{enumerate}
    \item{$\mathcal{A} \perp \mathcal{B}\ |\ \mathcal{X}$}
    \item{$\linE{\beta}{\mathcal{A} \vee \mathcal{X}} = \linE{\beta}{\mathcal{X}}\ \forall \beta \in \mathcal{B}$}
    % \item{$\linE{\beta}{\mathcal{A}} = \linE{\linE{\beta}{\mathcal{X}}}{\mathcal{A}}\ \forall \beta \in \mathcal{B}}$}
  \end{enumerate}
\end{lemma}
\begin{proof}
  Proceding from the definition of conditional orthogonality we have

  \begin{align*}
    &\inner{\alpha - \linE{\alpha}{\mathcal{X}}}{\beta - \linE{\beta}{\mathcal{X}}} = 0 \ \forall \alpha \in \mathcal{A}\ \beta \in \mathcal{B}\\
    &{\iff} \inner{\alpha + x - \linE{\alpha + x}{\mathcal{X}}}{\beta - \linE{\beta}{\mathcal{X}}} = 0 \ \forall \alpha \in \mathcal{A}\ \beta \in \mathcal{B}, x \in \mathcal{X}\\
    &\overset{(a)}{\iff} \inner{\alpha + x}{\beta - \linE{\beta}{\mathcal{X}}} = 0 \ \forall \alpha \in \mathcal{A}\ \beta \in \mathcal{B}, x \in \mathcal{X}\\
    &\overset{(b)}{\iff} \linE{\beta - \linE{\beta}{\mathcal{X}}}{\mathcal{A} \vee \mathcal{X}} = 0\ \forall \beta \in \mathcal{B}\\
    &\overset{(c)}{\iff} \linE{\beta}{\mathcal{A} \vee \mathcal{X}} = \linE{\beta}{\mathcal{X}},
  \end{align*}

  where $(a)$ follows since $(\beta - \linE{\beta}{\mathcal{X}}) \perp \mathcal{X}$ and $\linE{\alpha + x}{\mathcal{X}} \in \mathcal{X}$, $(b)$ since $z \perp \mathcal{X} \vee \mathcal{A} \iff \linE{z}{\mathcal{X} \vee \mathcal{A}} = 0$, and $(d)$ since $\linE{z}{\H_2} = \linE{\linE{z}{\H_2}}{\H_1}$ for $\H_2 \subseteq \H_1$.
  
  % This is property (vi) in Lindquist
  % \begin{align*}
  %   \langle \alpha - \linE{\alpha}{\mathcal{X}}, \beta - \linE{\beta}{\mathcal{X}} \rangle &= 0 \ \forall \alpha \in \mathcal{A}\ \beta \in \mathcal{B}\\
  %   \overset{(a)}{\iff} \langle \alpha, \beta - \linE{\beta}{\mathcal{X}} \rangle &= 0\ \forall \alpha \in \mathcal{A}\ \beta \in \mathcal{B}\\
  %   \overset{(b)}{\iff} \linE{\beta - \linE{\beta}{\mathcal{X}}}{\mathcal{A}} &= 0\ \forall \beta \in \mathcal{B}
  % \end{align*}

  % where $(a)$ follows because $\linE{\alpha}{\mathcal{X}} \in \mathcal{X}$ and $\beta - \linE{\beta}{\mathcal{X}} \in \mathcal{X}$, and $(b)$ follows since $\beta \perp \mathcal{A} \iff \linE{\beta}{\mathcal{A}} = 0$.
\end{proof}

\begin{definition}[\hl{Confounder}]
  A node $k$ will be referred to as a \textit{confounder} of nodes
  $i, j$ (neither of which are equal to $k$) if
  $k \in \anc{i} \cap \anc{j}$ and there exists a path
  $\gcgpath{k}{i}$ not containing $j$, and a path $\gcgpath{k}{j}$
  not containing $i$.

  A simple example is furnished by the ``fork'' graph
  $i \leftarrow k \rightarrow j$.
\end{definition}

\begin{proposition}
  \label{prop:ancestor_properties}
  \hl{Add the stronger statements on uncorrelatedness for use on confounders later.}

  In a Granger-causality graph $\gcg$, if $j \pwgc i$ then at least
  one of the following must hold

  \begin{enumerate}
    \item{$j \in \anc{i}$}
    \item{$\anc{i} \cap \anc{j} \ne \emptyset$.}
  \end{enumerate}

  Moreover, if $j \not\in \anc{i}$ then $\exists u \in \anc{i} \cap \anc{j}$ which is a confounder, that is, such that $i$ is not contained in at least one $\gcgpath{u}{j}$ path.
\end{proposition}

\begin{remark}
  Considering the contrapositive of the first part of this theorem (which is what we actually prove below) is enlightening: it says that if $i$ is not a descendent of $j$ (i.e. $j \not \in \anc{i}$) and that there are no common ancestors (i.e. $\anc{i} \cap \anc{j} = \emptyset$) then $j \npwgc i$, or $\linE{x_i(t)}{\H_{t - 1}^{(i, j)}} = \linE{x_i(t)}{\H_{t - 1}^{(i)}}$, which is an intuitively natural result.

  The first part is used in part to prove the second (and stronger) statement, which allows common ancestors, but only if they are ``mediated'' by $i$.  For example, this covers the case of a graph $k \rightarrow i \rightarrow j$, where $\{k\} = \anc{i} \cap \anc{j}$, but clearly $j \npwgc i$.
\end{remark}

\begin{proof}
  We will first establish the contrapositive of $(1)$ and $(2)$: assume that $j \not \in \anc{i}$ and $\anc{i} \cap \anc{j} = \emptyset$, then we will show that $\forall \tau \in \Z_+,\ \E x_i(t)x_j(t - \tau) = 0$.  This follows from representation (\ref{eqn:parent_expansion}) since we have

  \begin{align*}
    \E x_i(t)x_j(t - \tau) &= \E \big(\A_{ii}(z)v_i(t)\big)\big(\A_{jj}(z)v_j(t - \tau)\big)\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\E[\big(\A_{ik}(z)v_k(t)\big)v_j(t - \tau)] + \sum_{\substack{k \in \anc{j} \\ k \ne j}}\E[v_i(t) \big(\A_{jk}(z) v_k(t - \tau)\big)]\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\sum_{\substack{\ell \in \anc{j} \\ \ell \ne j}}\E[\big(\A_{ik}(z)v_k(t)\big)\big(\A_{j\ell}(z)v_\ell(t - \tau)\big)]\\
  \end{align*}

  Keeping in mind that $v(t)$ is an isotropic and uncorrelated sequence (i.e. $\forall \tau \in \Z,\ k \ne \ell \Rightarrow \E v_k(t)v_\ell(t - \tau) = 0$ and $\forall \tau \in \Z\setminus\{0\}\ \E v_i(t)v_i(t - \tau) = 0$), we will see that each of these above four terms are 0.  The first term since $i \pwgc i$ is not possible and hence the case $j = i$ is vacuous.  The second term is 0 since $j \not\in \anc{i}$.  The fourth term above is 0 since $\anc{i} \cap \anc{j} = \emptyset$.  For the third term, it is possible that (due to cycles) $i \in \anc{j}$ which requires more care, although similarly as before we can at least see immediately that

  \begin{equation*}
    \sum_{\substack{k \in \anc{j} \\ k \ne j}}\E[v_i(t) \big(\A_{jk}(z) v_k(t - \tau)\big)] = \E[v_i(t)\big(\A_{ji}(z)v_i(t - \tau)\big)].
  \end{equation*}

  However, $k \ne \ell \Rightarrow \A_{k\ell}(z) = 0$ (i.e. $\A(z) = I + A(1)z^{-1} + \cdots$) so $\A_{ji}(z)v_i(t - \tau)$ is strictly earlier in time, for $\tau \ge 0$, than $v_i(t)$ and hence $\E[v_i(t)\big(\A_{ji}(z)v_i(t - \tau)\big)] = 0$.  This establishes the fact that $H_t^{(i)} \perp \H_t^{(j)}$ where $H_t^{(i)} = \{\alpha x_i(t)\ |\ \alpha \in \R\}$ is the Hilbert space generated by the current sample of $x_i$.  It is clear that $\H_t^{(i)} = \bigvee_{\tau = 0}^\infty H_{t - \tau}^{(i)}$.

  \hl{TODO: Separate $H_t^i \perp \H_t^j \Rightarrow \H_t^i \perp \H_{t - 1}^j\ |\ \H_{t - 1}^i$ into a lemma and consolidate it with lemma 2, 3.}

  Then for every $s \ge 0$

  \begin{align*}
    \linE{x_j(t - s)}{\H_t^{(i)}} &= \linE{x_j(t - s)}{\bigvee_{\tau = 0}^\infty H_{t - \tau}^{(i)}}\\
    &\overset{(a)}{=} \linE{x_j(t - s)}{\bigvee_{\tau = 1}^\infty H_{t - \tau}^{(i)}}\\
    &= \linE{x_j(t - s)}{\H_{t - 1}^{(i)}},
  \end{align*}

  where $(a)$ follows from lemma (\ref{lem:subspace_sum_projection}).  Lemma (\ref{lem:conditional_orthogonality_equivalence}) tells us that this is equivalent to

  \begin{equation*}
    \H_t^{(i)} \perp \H_{t - 1}^{(j)}\ |\ \H_{t - 1}^{(i)},
  \end{equation*}

  so by theorem (\ref{thm:granger_causality_equivalences}) we conclude $j \npwgc i$.

  For the second part of the theorem we will suppose that $j$ is a node having the following two properties: $(a)$ $j \not \in \anc{i}$ and $(b)$ for every $k \in \anc{i} \cap \anc{j}$ every $k \rightarrow \cdots \rightarrow j$ path contains $i$.  The first is the negation of property $(1)$ above, and the second is to setup the contradiction $j \npwgc i$, which will then establish the theorem.

  Firstly, notice that every $u \in \big(\pa{j} \setminus \{i\}\big)$ necesssarily inherits these same two properties since if $u \in \anc{i}$ then $u \in \anc{i} \cap \anc{j}$ so that every $u \rightarrow \cdots \rightarrow j$ path must contain $i$, but $u \in \pa{j}$, so this is not the case since $u \rightarrow j$ is a path that doesn't contain $i$; moreover, if we consider $w \in \anc{i} \cap \anc{u}$ then we also have $w \in \anc{i} \cap \anc{j}$ so every $w \rightarrow \cdots \rightarrow j$ path must contain $i$.  These properties therefore extend inductively to every $u \in \big(\anc{j} \setminus \{i\}\big)$.

  In order to deploy a recursive argument, define the following partition of $\pa{u}$, for some node $u$:

  \begin{align*}
    C_0(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} = \emptyset, k \ne i\}\\
    C_1(u) &= \{k \in \pa{u}\ |\ i \in \anc{k} \text{ or } k = i\}\\
    C_2(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} \ne \emptyset, k \ne i\}.
  \end{align*}

  We notice that for any $u$ having the properties $a, b$ above, we must have $C_2(u) = \emptyset$ since if $k \in C_2(u)$ then $\exists w \in \anc{i} \cap \anc{k}$ s.t. $i \not \in \anc{k}$ and therefore there must be a path $w \rightarrow \cdots \rightarrow k \rightarrow \cdots \rightarrow u$ which does not contain $i$.

  Using this partition, we will expand $x_j(t)$ in terms of it's parents, and recursively expand nodes in $C_1$ until we reach a case where $C_1 = \emptyset$.  For the first step:  %where $\A_{jj}(z) = (1 - \B_{jj}(z))^{-1}$:

  \begin{equation}
    \label{eqn:xj_partition_expansion}
    x_j(t) = \A_{jj}(z)\Big(v_j(t) + \sum_{k \in C_0(j)}\B_{ik}(z)x_k(t) + \sum_{k \in C_1(j)}\B_{ik}(z)x_k(t)\Big).
  \end{equation}

  Using this representation we choose an arbitrary $\Phi(z) x_i(t - 1) \in \H_{t - 1}^{(j)}$ and show that

  \begin{equation}
    \inner{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}{\Phi(z)x_j(t - 1) - \linE{\Phi(z)x_j(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation}

  which will imply (\hl{by lemma ???}) that $j \npwgc i$ and for which it is equivalent to show that

  \begin{equation}
    \label{eqn:sufficient_inner_prod}
    \inner{x_i(t)}{\Phi(z)x_j(t - 1) - \linE{\Phi(z)x_j(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation}

  by the orthogonality principle since $\linE{x_i(t)}{\H_{t - 1}^{(i)}} \in \H_{t - 1}^{(i)}$.  Substituting (\ref{eqn:xj_partition_expansion}) into (\ref{eqn:sufficient_inner_prod}) and starting with the first term we have

  \begin{align*}
    &\inner{x_i(t)}{\Phi(z)\A_{jj}(z)v_j(t - 1) - \linE{\Phi(z)\A_{jj}(z)v_j(t - 1)}{\H_{t - 1}^{(i)}}}\\
    \overset{(\alpha)}{=}\ &\inner{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}{\Phi(z)\A_{jj}(z)v_j(t - 1)}\\
    \overset{(\beta)}{=}\ &0,
  \end{align*}

  where $(\alpha)$ follows by expanding $x_i(t)$ with (\ref{eqn:ancestor_expansion}) and $\linE{\Phi(z)\A_{jj}(z)v_j(t - 1)}{\H_{t - 1}^{(i)}} = 0$ because $\forall \tau, s$

  \begin{equation*}
    \E v_j(t - \tau) x_i(t - s) = \E v_j(t - \tau) \sum_{k \in \anc{i} \cup \{i\}}\A_{ik}(z)v_k(t - s) = 0,
  \end{equation*}

  since $j \not \in \anc{i}$; $(\beta)$ follows similarly, that is, $j \not \in \anc{i}$.  Secondly we see that $\forall k \in C_0(j)$

  \begin{equation*}
    \inner{x_i(t)}{\Phi(z)\A_{jj}(z)\B_{ik}(z)x_k(t - 1) - \linE{\Phi(z)\A_{jj}(z)\B_{ik}(z)x_k(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation*}

  which follows from the first part of the theorem.  Finally, for $k \in C_1(j)$ the case $k = i$ is immediate, so suppose $k \ne i$.  We know from above that $k$ inherits the key properties referred to as $(a), (b)$ and therefore we can recursively expand $k$ in the same way as in equation (\ref{eqn:xj_partition_expansion}).  Continuing this recursion for each $k \in C_1(j)$ where $k \ne i$ must eventually terminate since $i \in \anc{k}$.
\end{proof}

An interesting corollary is the following:

\begin{corollary}
  If the graph $\gcg$ is acyclic and if $j \pwgc i$ and $i \pwgc j$ then $\anc{i} \cap \anc{j} \ne \emptyset$.
\end{corollary}

It seems intuitive that a converse of proposition \ref{prop:ancestor_properties}
would hold, i.e. $j \in \anc{i} \Rightarrow j \pwgc i$.  Unfortunately,
this is not the case in general, as different paths through $\gcg$ can
lead to cancellation (see example \ref{ex:diamond_cancellation}).  In
fact, we do not even have $j \in \pa{i} \Rightarrow j \pwgc i$ (see
example \ref{ex:lag_cancellation}).

\begin{example}
  \label{ex:diamond_cancellation}
  Firstly, on $n = 4$ nodes, ``diamond'' shapes can lead to cancellation on paths of length 2:

\begin{equation*}
  x(t) =
  \left[
    \begin{array}{cccc}
      0 & 0 & 0 & 0\\
      a & 0 & 0 & 0\\
      -a & 0 & 0 & 0\\
      0 & 1 & 1 & 0\\
    \end{array}
  \right] x(t - 1) + v(t),
\end{equation*}

with $\E v(t) = 0,\ \E v(t)v(t - \tau)^\T = \delta_\tau I$.

By directly calculating

\begin{align*}
  x_4(t) &= x_2(t - 1) + x_3(t - 1) + v_4(t)\\
         &= ax_1(t - 2) + av_2(t - 1) - ax_1(t - 2) -av_3(t - 1) + v_4(t)\\
         &= a(v_2(t - 1) - v_3(t - 1)) + v_4(t),
\end{align*}

we see that, since $v(t)$ is isotropic white noise, $1 \npwgc 4$.  The problem here is that there are multiple paths from $x_1$ to $x_4$.
\end{example}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\linewidth]{example1.pdf}
    \caption{Graph Corresponding to Example \ref{ex:diamond_cancellation}}
    \label{fig:diamond_cancellation}
    \centering{$j \in \anc{i} \nRightarrow j \pwgc i$}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\linewidth]{example2.pdf}
    \caption{Graph Corresponding to Example \ref{ex:lag_cancellation}}
    \label{fig:lag_cancellation}
    \centering{$j \in \pa{i} \nRightarrow j \pwgc i$}
  \end{subfigure}
\end{figure}

\begin{example}
  \label{ex:lag_cancellation}
  A second example on $n = 3$ nodes is also worth examining, in this case
  cancellation is a result of differing time lags.

\begin{equation*}
  x(t) =
  \left[
    \begin{array}{ccc}
      0 & 0 & 0\\
      -a & 0 & 0\\
      0 & 1 & 0\\
    \end{array}
  \right] x(t - 1) +
  \left[
    \begin{array}{ccc}
      0 & 0 & 0\\
      0 & 0 & 0\\
      a & 0 & 0\\
    \end{array}
  \right] x(t - 2) + v(t)
\end{equation*}

Then

\begin{align*}
  x_2(t) &= v_2(t) - ax_1(t - 1)\\
  x_3(t) &= v_3(t) + x_2(t - 1) + ax_1(t - 2)\\
  \Rightarrow x_3(t) &= v_2(t - 1) + v_3(t),
\end{align*}

and again $1 \npwgc 3$.
\end{example}

\subsection{Strongly Causal Graphs}
\label{sec:strongly_causal_graphs}
% The examples at the conclusion of the previous section seem rather
% pathological since we have simply constructed a case where different
% paths in the graph cancel exactly.  It may be possible to rid
% ourselves of these pathologies for instance by defining a notion of
% ``robust'' pairwise causality where ``$j \pwgc _R i$'' if $j \pwgc i$
% almost surely for a ``small'' random perturbation of the system
% matrix.  However, we do not believe such a modification of our theory
% can have useful practical implications since the difference between
% paths that ``nearly cancel'' rather than cancel exactly would in
% practice require inordinate amounts of data to resolve.

In this section and the next we will seek to understand when converse
statements of Proposition \ref{prop:ancestor_properties} \textit{do}
hold.  One possibility is to restrict to coefficients of the system
matrix matrix, e.g. by requiring that $B_{ij}(\tau) \ge 0$.  Instead,
we think it is more meaningful to focus on the defining feature of
time series networks, that is, the topology of $\gcg$.

\begin{definition}[Strongly Causal]
  \label{def:strongly_causal}
  We will say that a Granger-causality graph $\gcg$ is
  \textit{strongly causal} if there is at most 1 directed path between
  any two nodes.
\end{definition}

Examples of strongly causal graphs include directed trees (or
forests), DAGs where each node has at most one parent, and figure
\ref{fig:example_fig3} of this paper.  The persistence condition is
used to ensure that $\linE{x_i(t)}{\H_{t - 1}^{(i)}} \ne 0$ i.e. that
the each process has at least some amount of ``memory'' -- this can
occur either due to cycles, or non-zero $\B_{ii}(z)$ terms in equation
\eqref{eqn:ar_representation}.  It is evident that the strong causal property
is inherited by subgraphs, but persistence need not be, unless $\gcg$ is a DAG.

\hl{Not sure these are needed, but were important in my own thinking.}

\begin{lemma}
  \label{lem:still_strongly_causal}
  If $\gcg$ is a strongly causal graph, any subgraph formed by eliminating nodes
  as well as all of the in and out edges thereof is still strongly causal.
\end{lemma}
\begin{proof}
  If $\gcg$ has at most one path between any two nodes, there can only
  be fewer paths after removing nodes from $\gcg$.
\end{proof}

\begin{lemma}
  \label{lem:still_persistent}
  If $\gcg$ is a persistent DAG, any subgraph formed by eliminating nodes
  as well as all of the in and out edges thereof is still persistent.
\end{lemma}
\begin{proof}
  \hl{TODO -- is this needed?}
\end{proof}

We explore a number of properties of strongly causal graphs before
moving into the main result of section \ref{sec:theory} in section
\ref{sec:pairwise_algorithm}.  The following important property
essentially strengthens proposition \ref{prop:ancestor_properties}
for the case of strongly causal graphs.

\begin{proposition}
  \label{prop:sc_graph_common_anc}
  In a strongly causal graph if $j \in \anc{i}$ then any
  $k \in \anc{i} \cap \anc{j}$ is not a confounder, that is,
  the unique path from $k$ to $i$ contains $j$.
\end{proposition}
\begin{proof}
  Suppose that there is a path from $k$ to $i$ which does not contain
  $j$.  In this case, there are multiple paths from $k$ to $i$ (one of
  which \textit{does} go through $j$ since $j \in \anc{i}$) which
  contradicts the assumption of strong causality.
\end{proof}

% Is pwgc transitive?
  
\begin{corollary}
  \label{cor:parent_corollary}
  If $\gcg$ is a strongly causal DAG then $i \pwgc j$ and $j \in \anc{i}$ are
  \textit{alternatives}, that is $i \pwgc j \Rightarrow j \notin \anc{i}$.
\end{corollary}
\begin{proof}
  Suppose that $i \pwgc j$ and $j \in \anc{i}$.  Then since $\gcg$ is
  acyclic $i \not\in \anc{j}$, and by proposition
  \ref{prop:ancestor_properties} there is some
  $k \in \anc{i}\cap\anc{j}$ which is a confounder.  However, by
  proposition \ref{prop:sc_graph_common_anc} $k$ cannot be a
  confounder, a contradiction.
\end{proof}
% This is a direct proof
  % \begin{proof}
%   Suppose that $\gcg$ is a strongly causal DAG and that we have both
%   $i \pwgc j$ and $j \in \anc{i}$, which implies that
%   $i \not \in \anc{j}$ since $\gcg$ is a DAG.  We will establish the
%   contradiction $i \in \anc{j}$.

%   Since $j \in \anc{i}$ there is a path $\gcgpath{j}{i}$.
%   Moreover, since $i \pwgc j$ by proposition \ref{prop:pwgc_anc} there
%   must be a confounding ancestor $k \in \anc{i} \cap \anc{j}$, where the
%   node $k$ has a path to $j,\ \gcgpath{k}{j}$, as well as a path
%   to $i,\ \gcgpath{k}{i}$.

%   \hl{Double check that the ancestor properties proposition implies
%     that BOTH paths do not contain the other node.}

%   Now, since the graph is strongly causal, the only possible
%   $\gcgpath{k}{i}$ path is the one obtained by concatenating the
%   $\gcgpath{k}{j}$ path with the $\gcgpath{j}{i}$ path.
%   However, this is a contradiction since $k$ is a confounder and
%   should have a $\gcgpath{k}{i}$ path which does not contain $i$.
% \end{proof}
  
\begin{corollary}
  \label{cor:bidirectional_edge}
  If $\gcg$ is a strongly causal DAG such that $i \pwgc j$ and
  $j \pwgc i$, then $i \not\in \anc{j}$ and $j \not\in \anc{i}$.  In
  particular, a pairwise bidirectional edge indicates the absense of
  any edge in $\gcg$.
\end{corollary}
\begin{proof}
  This follows directly from applying proposition
  \ref{cor:parent_corollary} to $i \pwgc j$ and $j \pwgc i$.
% This is a direct proof
% \begin{proof}
  % By way of contradiction, suppose that $i \in \pa{j}$.  We will
  % consider the two possibilities allowed by proposition
  % \ref{prop:ancestor_properties} for $j \pwgc i$.  Firstly
  % $j \in \anc{i}$ is impossible since $\gcg$ is assumed to be acyclic.
  % Secondly, if there is some confounding node
  % $u \in \anc{i} \cap \anc{j}$ with a path
  % $u \rightarrow \cdots \rightarrow j$ which does not contain $i$ we
  % have a contradiction since there must now be multiple
  % $u \rightarrow j$ paths: the aforementioned, and a path
  % $u \rightarrow \cdots \rightarrow i \rightarrow \cdots \rightarrow
  % j$ which \textit{does} contain $i$.  We conclude that $i \in \pa{j}$
  % is impossible, and symmetrically that $j \in \pa{i}$ is as well.
% \end{proof}
\end{proof}

\begin{proposition}
  \label{prop:independent_confounders}
  If $\gcg$ is strongly causal we have distinct
  $k_1, k_2 \in \anc{i} \cap \anc{j}$ are confounders of $i, j$, then
  $k_1 \not\in \anc{k_2}$, $k_2 \not\in \anc{k_1}$ and
  $\not\exists \ell \in \anc{k_1} \cap \anc{k_2}$ confounding
  $k_1, k_2$.
\end{proposition}
\begin{proof}
  \hl{TODO}
\end{proof}

\begin{corollary}
  If $\gcg$ is strongly causal, then distinct confounders are
  uncorrelated.
\end{corollary}
\begin{proof}
  This follows directly by combining proposition
  \ref{prop:independent_confounders} with proposition \ref{prop:ancestor_properties}.  \hl{I need to update prop} \ref{prop:ancestor_properties} \hl{to note the strong non-correlation property.}
\end{proof}

% \begin{lemma}
%   If $\gcg$ is a strongly causal DAG and $j \in \anc{i}$ then 
% \end{lemma}

In light of proposition \ref{prop:sc_graph_common_anc}, the following
provides a partial converse to proposition \ref{prop:ancestor_properties}.

\begin{proposition}
  \label{prop:pwgc_anc}
  If $\gcg$ is a strongly causal DAG then $j \in \anc{i} \Rightarrow j \pwgc i$.
\end{proposition}
\begin{proof}
  We will show that for some $\psi \in \H_{t - 1}^{(j)}$ we have

  \begin{equation}
    \label{eqn:cond_ortho_proof}
    \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0
  \end{equation}

  and therefore that $H_t^{(i)} \not\perp\ \H_{t - 1}^{(j)}\ |\ \H_{t - 1}^{(i)}$, which by theorem (\ref{thm:granger_causality_equivalences}) and \hl{lemma [name the lemma]} is enough to establish that $j \pwgc i$.

  Firstly, we will establish a representation of $x_i(t)$ that involves $x_j(t)$.  Denote by $a_{r + 1} \rightarrow a_r \rightarrow \cdots \rightarrow a_1 \rightarrow a_0$ with $a_{r + 1} \defeq j$ and $a_0 \defeq i$ the \textit{unique} $\gcgpath{j}{i}$ path in $\gcg$, we will expand the representation of equation (\ref{eqn:parent_expansion}) backwards along this path:

  % Should this be written as a lemma?
  \begin{align*}
    x_i(t) &= v_i(t) + \B_{ii}(z) x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z) x_k(t)\\
           &= \underbrace{v_{a_0}(t) + \B_{a_0a_0}(z) x_i(t) + \sum_{\substack{k \in \pa{a_0} \\ k \ne a_1}}\B_{a_0 k}(z) x_k(t)}_{\defeq \wtalpha{a_0}{a_1}} + \B_{a_0a_1}(z)x_{a_1}(t)\\
           &= \wtalpha{a_0}{a_1} + \B_{a_0a_1}(z)\big[\wtalpha{a_1}{a_2} + \B_{a_1a_2}(z)x_{a_2}(t) \big]\\
           &\overset{(a)}{=} \cdots\\
           &= \sum_{\ell = 0}^r \underbrace{\Big(\prod_{m = 0}^{\ell - 1} \B_{a_m a_{m + 1}}(z) \Big)}_{\defeq F_\ell(z)} \wtalpha{a_\ell}{a_{\ell + 1}} + \Big(\prod_{m = 0}^{r}\B_{a_m a_{m + 1}}(z)\Big)x_{a_{r + 1}}(t)\\
           &= \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} + F_{r + 1}(z) x_j(t)
  \end{align*}

  where $(a)$ follows by a routine induction argument and where we define $\prod_{m = 0}^{-1} \bullet \defeq 1$ for notational convenience.

  Using this representation to expand equation (\ref{eqn:cond_ortho_proof}), we obtain the following cumbersome expression:

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}\\
    &- \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\linE{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}{\H_{t - 1}^{(i)}}}\\
    &+ \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}.
  \end{align*}

  Note that by the orthogonality principle, $\psi - \linE{\psi}{\H_{t - 1}^{(i)}} \perp \H_{t - 1}^{(i)}$, the middle term above is $0$.  Choosing now the particular value $\psi = F_{r + 1}(z)x_j(t) \in \H_{t - 1}^{(j)}$ we arrive at

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
    &= \E|F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}|^2\\
    &+ \inner{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}}},
  \end{align*}

  which by the Cauchy-Schwarz inequality is $0$ if and only if

  \begin{equation*}
    \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}} - F_{r + 1}(z)x_j(t),
  \end{equation*}

  or by rearranging and applying the representation obtained earlier, if and only if

  \begin{equation*}
    x_i(t) \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}},
  \end{equation*}

  but this is impossible since $x_i(t) \not \in \H_{t - 1}^{(i)}$.
\end{proof}

We immediately obtain the corollary, which we remind the reader is,
surprisingly, not true in a general graph.

\begin{corollary}
  \label{cor:gc_implies_pwgc}
  If $\gcg$ is a strongly causal DAG then $j \gc i \Rightarrow j \pwgc i$.
\end{corollary}

\begin{remark}
  It can be seen from the conclusion of the proof that simple facts
  about Granger causality are indeed reliant on our assumptions about
  the nature of $x(t)$ laid out in the section \ref{sec:theory}, in
  particular, the innovations process $v(t)$ must be full rank.
  
  Moreover, that strong conditions need to be placed on the topology
  of $\gcg$ in order for proposition \ref{prop:pwgc_anc} to hold can
  be seen through the earlier examples.
\end{remark}

\begin{lemma}
  If $\gcg$ is strongly causal, then any $\VAR(p)$ system on $\gcg$ is stable if and only if
  $B_{ii}(z)$ is stable for every $i = 1, \ldots, n$.
\end{lemma}
\begin{proof}
  \hl{This is certainly true, but I don't think I need to use it anywhere.}
\end{proof}


\begin{remark}
  \hl{Possibly expand on this thought}

  As in much of statistics, confounding nodes pose challenges for
  Granger-causality.  However, as opposed to Pearl's causal calculus
  \cite{pearl2000art}, pairwise Granger-causality does not suffer any
  difficulty with so-called ``colliders'', that is, the topology
  $i \rightarrow k \leftarrow j$ will never result in $i \pwgc j$ or
  $j \pwgc i$.  This is evidently an advantage of the \textit{temporal}
  nature of Granger causality -- ``information'' cannot flow backwards
  along the edges of $\gcg$.
\end{remark}

\begin{example}
  As a final remark of this subsection we note that a complete
  converse to proposition \ref{prop:ancestor_properties} is not
  possible without additional conditions.  Consider the ``fork'' system on $3$
  nodes (i.e. $2 \leftarrow 1 \rightarrow 3$) defined by

  \begin{equation*}
    x(t) =
    \left[
      \begin{array}{cccc}
        0 & 0 & 0\\
        a & 0 & 0\\
        a & 0 & 0\\
      \end{array}
    \right] x(t - 1) + v(t).
  \end{equation*}

  In this case, node $1$ is a confounder for nodes $2$ and $3$, but
  $x_3(t) = v_3(t) - v_2(t) + x_2(t)$ and $2 \npwgc 3$ (even
  though $x_2(t)$ and $x_3(t)$ are contemporaneously correlated)

  If we were to augment this system by simply adding an autoregressive
  component to $x_1(t)$ e.g.  $x_1(t) = v_1(t) + b x_1(t - 1)$ then we
  \textit{would} have $2 \pwgc 3$ since then
  $x_3(t) = v_3(t) + av_1(t - 1) - bv_2(t - 1) + bx_2(t - 1)$.
\end{example}

\subsection{Persistent Systems}
\label{sec:persistent_systems}
In section \ref{sec:strongly_causal_graphs} we obtained a converse to
the part $(a)$ of proposition \ref{prop:ancestor_properties} via the
notion of a strongly causal graph topology.  In this section, we
complete a converse by adding the additional requirement we refer to
as ``persistence''.  This condition requires that each node maintains
some ``memory'' of the past.

\begin{definition}[Lag Function]
  Given a caual filter $\B(z) = \sum_{\tau = 0}^\infty b(\tau)z^{-\tau}$
  define 

  \begin{align}
    \tau_0(\B) &= \text{min}\{\tau \in \Z_+\ |\ b(\tau) \ne 0\},\\
    \tau_{\infty}(\B) &= \text{sup}\{\tau \in \Z_+\ |\ b(\tau) \ne 0\}.\\
  \end{align}

  i.e. the ``first'' and ``last'' coefficients of the filter $\B(z)$,
  where $\tau_\infty(\B) = \infty$ if the filter has an infinite
  impulse response.
\end{definition}

\begin{definition}[Persistent]
  We will say that a WSS process $x(t)$ with Granger-causality graph
  $\gcg$ is \textit{persistent} if for every $i \in [n]$, every
  $k \in \anc{i}$ we have $\tau_\infty(\A_{ik}) = \infty$.
\end{definition}

\begin{remark}
  There are a multitude of conditions which would guarantee $x(t)$ is
  persistent, for innstance, any ARMA system with a non-zero
  autoregressive component is likely to be persistent due to recursion
  in the coefficients naturally lead to an $\mathsf{MA}(\infty)$
  representation.

  In particular, any system with a finite impulse response
  (i.e. $\mathsf{VAR}(q)$ models) is likely to be persistent.  These
  models are common in applications of Granger-causality.

  Moreover, persistence is not the weakest condition necessary for the
  results of this section, but we opt to avoid unnecessarily
  complicated definitions.
\end{remark}

\begin{lemma}
  \label{lem:time_lag_cancellation}
  Suppose $v(t)$ is a scalar sequence with unit variance and zero
  autocorrelation and let $\A(z), \B(z)$ be nonzero and strictly
  causal (i.e. $\tau_0(\A) \ge 1$) linear filters.  Then,

  \begin{equation}
    \inner{F(z)\A(z)v(t)}{\B(z)v(t)} = 0\ \forall \text{ strictly causal filters } F(z)
  \end{equation}

  if and only if $\tau_0(\A) \ge \tau_\infty(\B)$.
\end{lemma}
\begin{proof}
  We have

  \begin{align}
    \inner{\A(z)v(t)}{\B(z)v(t)} &= \sum_{\tau = 1}^\infty \sum_{s = 1}^\infty a(\tau)b(s)\E[v(t - s)v(t - \tau)]\\
    &= \sum_{\tau = \text{max}(\tau_0(\A), \tau_0(\B))}^{\text{min}(\tau_\infty(\A), \tau_\infty(\B))} a(\tau) b(\tau)\\
  \end{align}

  due to the uncorrelatedness assumptions on $v(t)$.  This expression
  is $0$ if and only if $\tau_0(\A) \ge 1 + \tau_\infty(\B)$ or if
  $\tau_0(\B) \ge 1 + \tau_\infty(\A)$ or if the coefficients are
  orthogonal along the common support.

  Specializing this fact to $\inner{F(z)\A(z)v(t)}{\B(z)v(t)} = 0$ we
  see that the coefficients cannot be orthogonal for every choice of
  $F$, and that $\text{sup}_F \tau_\infty(F\A) = \infty$, leaving only
  the possibility that

  \begin{align*}
    \tau_0(F\A) \ge 1 + \tau_\infty(\B) \forall F &\overset{(a)}{\iff} \tau_0(\A) \ge 1 + \tau_\infty(\B) - \underset{F}{\text{min }} \tau_0(F)\\
    &\overset{(b)}{\iff} \tau_0(\A) \ge \tau_\infty(\B),
  \end{align*}

  where $(a)$ follows since $\tau_0(F\A) = \tau_0(F) + \tau_0(\A)$,
  and $(b)$ since $\text{min}_F\ \tau_0(F) = 1$.
\end{proof}

\begin{corollary}
  \label{cor:time_lag_cancellation}
  For $k \in \anc{i} \cap \anc{j}$ we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})
  \end{align*}
\end{corollary}
\begin{proof}
  The final equivalence follows immediately from the Lemma.  For the first equivalence we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)A_{jk}(z)v_k(t)}{x_i(t - \tau)} &= 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{align*}

  which can be expanded by equation \eqref{eqn:ancestor_expansion} to
  obtain (after cancellinng all ancestors of $i$ other than $k$)

  \begin{equation*}
    \inner{F(z)A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t - \tau)} = 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{equation*}

  which by the Lemma is equivalent to $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$ as stated.
\end{proof}

\begin{proposition}
  Suppose $\gcg$ is a strongly causal DAG and that $x(t)$ is persistent DAG, then if $k$
  confounds $i, j$ we have $i \pwgc j$ and $j \pwgc i$.
\end{proposition}
\begin{proof}
  We will show that $j \pwgc i$, the other being symmetric.  First
  note also that by proposition \ref{prop:sc_graph_common_anc} we
  cannot have $i \in \anc{j}$ or $j \in \anc{i}$ and therefore every
  $k \in \anc{i}\cap\anc{j}$ will be a confounder.

  It is sufficient to show that $\exists \psi \in \H_{t - 1}^{(j)}$
  such that

  \begin{equation*}
    \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0.
  \end{equation*}

  To this end, let $F(z)$ be an arbitrary but strictly causal linear,
  and equation \eqref{eqn:ancestor_expansion} to $x_i(t)$ and
  $\psi \defeq F(z)x_j(t)$:

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
    &\overset{(a)}{=} \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
    &\overset{(b)}{=} \inner{\sum_{k \in \anc{j}}\big(F(z)\A_{jk}(z)v_k(t) - \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}\big)}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
    &\overset{(c)}{=} \sum_{k \in \anc{i}\cap\anc{j}}\Big(\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} - \inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{ii}v_i(t)}\\
    &- \sum_{\ell \in \anc{i}}\inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{i\ell}(z)v_\ell(t)}\Big)
  \end{align*}

  where in $(a)$ we have removed the $\linE{x_i(t)}{\H_{t - 1}^{(i)}}$
  term via the orthogonality principle, in $(b)$ there is no
  $F(z)\A_{jj}(z)v_j(t)$ term since due to $j \not\in \anc{i}$ it is
  orthogonal to $\H_t^{(i)}$.  Finally, $(c)$ follows by applying
  orthogonality properties of $v(t)$, as well as the fact that
  $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$ for
  $k \not \in \anc{i}$.  Note that
  $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} \in \H_{t - 1}^{(i)}$
  and thence there is in general no cancellation in the final term
  above for $\ell \in \anc{i}$.

  Evidently, (\hl{This is clearly not immediately evident}) this is
  $0$ for each $F$ if and only if
  $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$ and
  $\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} = 0$, which by
  Collary \ref{cor:time_lag_cancellation} occurs if and only if
  $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$, which is impossible
  since in general $\tau_0(\A_{jk}) < \infty$ and in a persistent
  graph $\tau_\infty(\A_{ik}) = \infty$.
\end{proof}

\subsection{Recovering $\gcg$ via Pairwise Tests}
\label{sec:pairwise_algorithm}
In this section we will show that if $\gcg$ is a strongly causal DAG,
then it is possible to recover $\gcg$ via pairwise tests alone.  This
is an interesting fact in and of itself, but also has some
implications for applications since pairwise testing is trivially
parallelizable.  In section \ref{sec:structure_learning} we will also
analyze the use of pairwise testing as a heuristic for general graphs.

\begin{theorem}[Pairwise Recovery]
  \label{thm:tree_recovery}
  If the Granger-Causality graph $\gcg$ for $x(t)$ is a strongly
  causal DAG then $\gcg$ can be inferred from pairwise causality
  tests alone.  The procedure can be carried out, assuming we have an
  oracle for pairwise causality, via algorithm (\ref{alg:pwgr}).

  \begin{algorithm}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \SetKwInOut{Initialize}{initialize}
    \DontPrintSemicolon

    \BlankLine
    \caption{Pairwise Graph Recovery}
    \label{alg:pwgr}
    % \TitleOfAlgo{Pairwise Graph Recovery}
    \Input{Pairwise Granger-causality relations between $n$ processes whose joint Granger-causality relations are known to form a strongly causal DAG $\gcg$.}
    \Output{Edges $E = \{(i, j) \in [n] \times [n]\ |\ i \gc j \}$ of the graph $\gcg$.}
    \Initialize{$S_0 = [n]$  \texttt{\# unprocessed nodes}\\
      $E_0 = \emptyset$  \texttt{\# edges of }$\gcg$\\
      % $P_0 = \emptyset$  \texttt{\# layer by layer driving nodes}\\
      $k = 0$ \texttt{\# a counter used only for notation}}
    \BlankLine
    $W \leftarrow \{(i, j)\ |\ i \pwgc j, j \npwgc i\}$  \texttt{\# potential edges}\\
    \While{$S_k \ne \emptyset$}{
      $P_k \leftarrow \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W\}$  \texttt{\# parents of current layer}\\
      % $P_k \leftarrow P_{k - 1} \cup P_k$ \texttt{\# Expand parent set}\\
      $S_{k + 1} \leftarrow S_k \setminus P_k$ \texttt{\# remove from } $S$\\

      \;
      $C_k \leftarrow \{j \in S_{k + 1}\ |\ \forall s \in S_{k + 1}\ (s, j) \not\in W\}$   \texttt{\# potential children of }$P_k$\\

      \For{$r = 0, \ldots, k$}
      {
        \label{alg:inner_loop}
        $D_{kr} \leftarrow \{(i, j) \in P_{k - r} \times C_k\ |\ (i, j) \in W,\ \text{no } i \rightarrow j \text{ path in } E_k \}$\\%  \texttt{\# edges incident on }$C_k$\\
        $E_k \leftarrow E_k \cup D_{kr}$ \texttt{\# add edges to }$E_k$\\
        % $W_{k + 1} \leftarrow W_k \setminus D_{kr}$  \texttt{\# remove edges from consideration}\\
      }
      % $W_{k + 1} \leftarrow W_k \setminus \big(\bigcup_{r = 0}^k D_{kr}\big)$  \texttt{\# remove edges from consideration}\\

      \;
      $k \leftarrow k + 1$
    }
    $E \leftarrow E_{k + 1}$\\
    \Return{$E$}
  \end{algorithm}
\end{theorem}

We prove the theorem by establishing the correctness of algorithm (\ref{alg:pwgr}).  The idea is to iteratively ``peel away layers'' of nodes by removing the nodes that have no parents remaining, which always exist since the graph is acyclic.  The additional requirement of strong causality ensures firstly that all actual edges of $\gcg$ manifest in some way as pairwise relations (by proposition \ref{prop:pwgc_anc}), and secondly, as we will see, that parents of $C_k$ are in $\bigcup_{r = 0}^kP_{k - r}$ and can be determined by a pairwise test.

\begin{example}
  The set $W$ collects all of the edges that may be edges of $\gcg$.
  In reference to figure \ref{fig:example_fig3}, each of the solid
  black edges, as well as the dotted red edges will be included, but
  \textit{not} the bidirectional green dash-dotted edge.  That we exclude
  bidirectional edges is important since it will provide guarantees
  that $P_k$ is non-empty.  The groupings $P_0, \ldots, P_3$
  are also indicated in figure \ref{fig:example_fig3}.

  The algorithm proceeds first with the parentless nodes $1, 2$ on the
  initial iteration where the edge $(1, 3)$ is added to $E$.  On the
  next iteration, the edges $(3, 4), (2, 4), (3, 5)$ are added, and
  the false edges $(1, 4), (1, 5)$ is excluded due to the paths
  $1 \rightarrow 3 \rightarrow 4$ and $1 \rightarrow 3 \rightarrow 5$
  in E.  Finally, edge $(4, 6)$ are added, and the false
  $(1, 6), (3, 6), (2, 6)$ edges are similarly excluded due to the
  ordering of the inner loop.
  
  \begin{figure}
    \centering
    \caption{Example graph for Algorithm \ref{alg:pwgr}}
    \footnotesize{Black arrows indicate true parent-child
      relations.  Red dotted arrows indicate pairwise causality (due to
      ancestor relations), green dash-dotted arrows indicates
      bidirectional pairwise causality (due to the confounding node
      $1$).  Blue groupings indicate each $P_k$ in Algorithm
      \ref{alg:pwgr}.}
    \label{fig:example_fig3}
    
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\linewidth]{example_algorithm.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\linewidth]{example_algorithm2.pdf}
    \end{subfigure}
  \end{figure}

  That we need to proceed backwards through $P_{k - r}$ as in the
  inner loop of line \ref{alg:inner_loop} can also be seen from this
  example, where if instead
  $D_k = \{(i, j) \in \Big(\bigcup_{r = 0}^k P_{k - r}\Big) \times
  C_k\ |\ i \pwgc j \}$ we would infer the false positive edge
  $1 \rightarrow 4$.  Moreover, the same example shows that if simply
  $D_k = \{(i, j) \in P_k \times C_k\ |\ i \pwgc j \}$, then the edge
  $1 \rightarrow 3$ would be missed.
\end{example}

\begin{lemma}
  \label{lem:inner_loop_lemma}
  At any given iteration, $k$, of Algorithm \ref{alg:pwgr}, if we have
  $e \in E$ if and only if $e \in \mathcal{E}$, then the inner loop (line \ref{alg:inner_loop})
  will add $(i, j) \in \big(\bigcup_{r = 0}^kP_{k - r}\big) \times C_k$ to $E$
  if $(i, j) \in \mathcal{E}$.  i.e. any real edge will be included.
\end{lemma}
\begin{proof}
  Let $j \in C_k$, $i \in \bigcup_{r = 0}^k P_{k - r}$, and assume
  $i \in \pa{j}$.

  First, suppose that there is no $i \rightarrow j$ path in $E$.  Then
  since $i \gc j$ we have $i \pwgc j$ and $j \npwgc i$ (i.e.
  $(i, j) \in W$) by Proposition \ref{prop:pwgc_anc} and Corollary
  \ref{cor:bidirectional_edge} so at least one of
  $\{D_{kr} \}_{r = 0}^k$ contains $(i, j)$ and hence $(i, j)$ will be
  added to $E$ upon the end of the iteration.

  Now, suppose instead that there is already an $i \rightarrow j$ path
  in $E$.  Given our assumption that $E$ contains edges if and only if
  they are edges of $\gcg$, then either we already have $(i, j) \in E$
  correctly, or there is more than one $i \rightarrow j$ path in
  $\gcg$, which is not possible since $\gcg$ is strongly causal.
\end{proof}

\begin{lemma}
  \label{lem:W_subset_E}
  $W \subseteq \mathcal{E}$
\end{lemma}
\begin{proof}
  By proposition \ref{prop:pwgc_anc} we have
  $j \in \pa{i} \Rightarrow j \pwgc i$ and by Corollary \ref{cor:bidirectional_edge}
  there are no bidirectional edges in $\gcg$.
\end{proof}

\hl{Must show that Algorithm terminates in $n$ steps.}
\begin{lemma}
  For any $i \in P_k$ we have $i \in C_{k - 1}$.  \hl{[Can't yet prove
    this part] Moreover, for each $i \in \bigcup_{k = 0}^n P_k$, if
    $(s, i) \in W$ then $(s, i) \in E$ upon the termination of
    Algorithm} \ref{alg:pwgr}.
\end{lemma}
\begin{proof}
  We prove the second part by induction, the first part being
  established along the way.  By construction $P_0$ has no parent
  nodes in $\gcg$ so the base case is immediate.  Fix now some
  $k \ge 1$, $i \in P_k$, and let $s$ be such that $(s, i) \in W$.
  For every $s$ such that $(s, i) \in W$ we have $s \not\in S_k$ since
  otherwise $i \not\in P_k$.  Therefore
  $s \in \bigcup_{r = 0}^{k - 1}P_{k - r}$, which implies
  $s \not\in S_k$, and finally that $i \in C_{k - 1}$.  Then by Lemma
  \ref{lem:W_subset_E} every edge in $\mathcal{E}$ is also present in
  $W$, and by Lemma \ref{lem:inner_loop_lemma} \hl{[We can't conclude
    here because of the strong conditions required on lemma}
    \ref{lem:inner_loop_lemma}]
\end{proof}

\begin{lemma}[Loop Invariant]
  \label{lem:loop_invariant}
  $\forall k \ge 0,\ e \in E_k \Rightarrow e \in \mathcal{E}$.
  \hl{Not sure if this is useful / the right approach.}
\end{lemma}
\begin{proof}
  We will proceed by induction.  Firstly, the base case
  $E_0 = \emptyset$ is trivial, so fix some $k \ge 1$ and consider an
  edge $(i, j) \in \big(\bigcup_{r = 0}^k P_{k - r} \big) \times C_k$
  such that $(i, j) \in W$ (if the edge is not in $W$ then it is not
  in $\mathcal{E}$ by Lemma \ref{lem:W_subset_E}).

  \hl{WIP}
\end{proof}

\begin{definition}[Depth]
  For our present purposes we will define the \textit{depth} $d(j)$ of
  a node $j$ in $\gcg$ to be the length of the \textit{longest} path
  from a node in $P_0$ to $j$, where $d(j) = 0$ if $j \in P_0$.  It is
  apparent (\hl{right?}) that such a path will always exist in a
  strongly causal graph.  For example, in Figure
  \ref{fig:example_fig3} we have $d(3) = 1$ and $d(4) = 2$.
\end{definition}

\begin{lemma}
  $i \in P_k \iff d(i) = k$ and $j \in S_k \iff d(j) \ge k$.
\end{lemma}
\begin{proof}
  We proceed by induction.  The base case $j \in P_0 \iff d(j) = 0$ is
  by definition, and $j \in S_0 \iff d(j) \ge 0$ is trivial since
  $S_0 = [n]$.  So suppose that the lemma is true up to $k - 1$.

  For $i \in P_k$ we have by construction that there is no $j \in S_k$
  such that $(j, i) \in W$ and therefore no $j \in S_k$ such that
  $j \in \pa{i}$.  Therefore, any parent of $i$ can only be in
  $S_{k - r}$ for $r \ge 1$ and by the induction hypothesis which
  implies that $d(i) \le k$.  But if $d(i) < k$ then $i \in P_{k - r}$
  for some $r \ge 1$ (again by the induction hypothesis) thence
  $d(i) = k$ since $P_k \cap P_{k - r} = \emptyset$ by construction.

  Now for $s \in S_k$ we have by induction that and the fact that
  $s \in S_k \implies s \in S_{k - 1}$ by construction we have
  $d(s) \ge k - 1$, but again by induction (this time on $P_{k - 1}$)
  we have $d(s) \ne k$ since $S_k = S_{k - 1} \setminus P_{k - 1}$
  and therefore $d(s) \ge k$.

  Conversely, suppose $i \in [n]$ is such that $d(i) = k$.  Then
  $i \in S_{k - 1}$ by the hypothesis, but also $i \not\in P_{k - 1}$
  so then $i \in S_k = S_{k - 1} \setminus P_{k - 1}$.  Now, recalling
  the definition of $P_k$

  \begin{equation*}
    P_k = \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W \},
  \end{equation*}

  if $s \in S_k$ is such that $s \pwgc i$ then by Proposition
  \ref{prop:ancestor_properties} $(a) s \in \anc{i}$ or
  $(b) \exists u \in \anc{i} \cap \anc{j}$ which is a confounder.  If
  we have $s \in \anc{i}$ then since $d(s) \ge k$ we must have
  $d(i) > k$, which is a contradiction so $s \not\in \anc{i}$.  On the
  other hand, if we have a confounding $u \in \anc{i} \cap \anc{s}$
  then we must have $d(u) < k$ (since $s \in \anc{i}$ and $d(i) = k$)
  which implies by induction that $u \in P_{k - r}$ for some $r \ge 1$,
  and therefore that $u \not\in ???$  \hl{This is wrong.}

  Now let $j \in S_k$, then $d(j) \ge k - 1$ since we
  also have $j \in S_{k - 1}$ (i.e. $S_k$ is a decreasing sequence of
  sets) \hl{Can't conclude unless $i \in P_k \iff d(i) = k$, in which
    case, we have $S_k = S_{k - 1} \setminus P_{k - 1}$ and therefore
    $d(j) \ge k$.}
\end{proof}

\begin{lemma}[Loop Invariant]
  For every $(i, j) \in \mathcal{E}$ such that $d(j) \le k$ we have
  $(i, j) \in E_k$.  Conversely, for every $(i, j) \in E_{k}$ we have
  $d(j) \le k$ and $(i, j) \in \mathcal{E}$.  That is, at iteration
  $k, E_k$ and $\mathcal{E}$ agree on the set of edges whose
  terminating node is at most $k$ steps away from $P_0$.
\end{lemma}
\begin{proof}
  We will proceed by induction.  The base case $E_0 = \emptyset$ is
  trivial, so fix some $k \ge 1$, and suppose that the lemma holds for
  all nodes of depth less than $k$.
\end{proof}
  

%   Finally, suppose that $i \not \in \pa{j}$.  If $i \npwgc j$ then
%   we will correctly leave $(i, j)$ absent from $E$, so suppose that
%   $i \pwgc j$.  Then we must have either $i \in \anc{j}$ or a confounding 
%   $k \in \anc{i} \cap \anc{j}$; suppose first the former.
%   Then, there is some $u$ s.t. $i \pwgc u \pwgc j$, so by the
%   construction of \hl{it seems} that both $(i, u)$ and $(u, j)$ should
%   have already been added to $E$, so we have an $i \rightarrow j$ path
%   in $E$ \hl{is this sound?}  Suppose then the latter case above,
%   $u \in \anc{i} \cap \anc{j}$ \hl{try to construct a counterexample
%     involving this case}.
% \end{proof}

Proof of the actual theorem...

\begin{proof}
  Denote the edges of $\gcg$ by $\mathcal{E}$, we endevour to show
  that upon the termination of Algorithm \ref{alg:pwgr} we have
  $E = \mathcal{E}$.  On the first iteration of the algorithm
  ($k = 0$) we have $S_0 = [n]$ and $E = \emptyset$.

  We will first dispense with some trivialities: if $n = 0$ or $n = 1$
  the algorithm is correct, returning on the first iteration with
  $E = \emptyset$.  Suppose then that $n > 1$.  The set
  $P_0$ is non-empty since $\gcg$ is a DAG and therefore
  there are nodes in $\gcg$ without parents (if
  $P_0 = \emptyset$ proposition
  \ref{prop:ancestor_properties} would be contradicted).  Now, if
  $P_0 = [n]$ then the graph necessarily contains no edges
  and we will correctly return $E = \emptyset$.

  Moving on from any trivial cases, notice that
  $\mathcal{E} \subseteq W$.  This follows since by Proposition
  \ref{prop:pwgc_anc} we have $j \in \pa{i} \Rightarrow j \pwgc i$ and by
  Corollary \ref{cor:bidirectional_edge} there are no bidirectional
  edges in $\gcg$.

% 1. Dispense with trivialities
% 2. Show that hat{P}_k is nonempty
% 3. Show that C_k is nonempty
% 4. Show that edges in D_k are actual edges in G
% 5. show that each edge incident on C_k is in D_k
% 6. Induction

% Given non-empty P and C...  Can we show all P -> C edges get collected in D?
  % Firstly, if $n = 0$ or $n = 1$ the algorithm is trivially correct, returning on the first iteration with $E = \emptyset$.  Suppose then that $n > 1$ and let us define

  % \begin{equation*}
  %   A_k = \{i \in [n]\ |\ \dist{i}{P_0} = k\},
  % \end{equation*}

  % where

  % \begin{equation*}
  %   \dist{i}{P} = \text{max}\{r \in [n]\ |\ \exists (j \rightarrow i) \text{ path of length } r \text{ in } \gcg \text{ for some } j \in P\}.
  % \end{equation*}
  
  % We will proceed by induction and show that when the counter has reached the value $k \ge 1$, it is guaranteed that all nodes in $\bigcup_{\ell \le k}A_\ell$ (nodes having paths of length up to $k$ between themselves and $P_0$) must have all of their incident edges included in $E$, and that every edge in $E$ is an edge of $\gcg$.  If this is the case, then it can be immediately seen that the algorithm terminates with the correct result, $\gcg = ([n], E)$, once $k = n$; since there can only be paths up to length $n - 1$ in $\gcg$, and there cannot be any nodes or edges that don't involve paths back to $P_0$.  That is, $\bigcup_{\ell \le n}A_\ell = [n]$.

  % % firstly dispensing with some trivialities, and then establishing that every edge in $\gcg$ makes an appearance in some $D_{kr}$ as well as that everything in any $D_{kr}$ is indeed an edge in $\gcg$.

  % On the first iteration of the algorithm ($k = 0$) we have $S_0 = [n]$ and $E = \emptyset$.  To quickly dispense with some trivialities: the set $P_0$ is non-empty since $\gcg$ is a DAG and therefore there are nodes in $\gcg$ without parents, thence if $P_0 = \emptyset$ proposition \ref{prop:ancestor_properties} would be contradicted.  Now, if $P_0 = [n]$ then the graph necessarily contains no edges and we will correctly return $E = \emptyset$.

  % Continuing with the non-trivial case: if $P_0 \ne [n]$ then $C_0$ must be non-empty, otherwise there would be driving nodes not included in $P_0$.  In this case it is also clear that $A_1 \ne \emptyset$, so let us consider some $j \in A_1$.  We must have $j \in C_0$, otherwise there would be a path of length $2$ from $P_0$ to $j$.  Moreover, since $\exists i \in P_0$ s.t. $i \gc j$ and therefore $i \pwgc j$ (by proposition \ref{prop:pwgc_anc}) we have $(i, j) \in D_{00}$ and therefore $(i, j) \in E$.  Therefore $A_1 \subseteq D_{00}$.  \hl{This is nonsensical because $A_1$ does not contain tuples -- need to show that $A_1 \subseteq C_0$ ?}

  % Now consdider some $(i, j) \in D_{00}$ (the same reasoning as for $C_0$ is sufficient to see that $D_{00}$ is non-empty).  Since $i$ has no parents, $\anc{i} \cap \anc{j} = \emptyset$ and therefore $i \in \anc{j}$ (proposition \ref{prop:ancestor_properties}).  Furthermore, the construction of $C_k$ implies that $i \in \pa{j}$ since otherwise, there would be some $u \in \anc{j}$ with $i \in \anc{u}$, but then by proposition \ref{prop:pwgc_anc} $i \pwgc u$ and $u \pwgc j$ so that $u \not \in P_0 \Rightarrow u \in S_0 \Rightarrow j \not\in C_0$, a contradiction.  This implies every tuple in $D_{00}$ is a bona-fide edge of $\gcg$, as well as that $(i, j) \in A_1$.  So then $A_1 = D_{00}$, [\hl{nonsensical, see earlier.}] completing the base case for induction.

  % Suppose now the algorithm has reached step $k$ and that $E = \bigcup_{\ell \le k - 1}A_\ell$.
  
% The reasoning here is (I think) correct -- but isn't directly establishing the induction hypothesis.
%   On the other hand, if $P_0 \ne [n]$ then $C_0$ must also be non-empty, otherwise there would be driving nodes not included in $P_0$.  Consider now an edge $(i, j) \in D_{00}$ ($D_{00}$ is non-empty for the same reason as $C_0$).  Since $i$ has no parents, $\anc{i} \cap \anc{j} = \emptyset$ and therefore $i \in \anc{j}$ (proposition \ref{prop:ancestor_properties}).  Furthermore, the construction of $C_k$ implies that $i \in \pa{j}$ since otherwise, there would be some $u \in \anc{j}$ with $i \in \anc{u}$, but then by proposition \ref{prop:pwgc_anc} $i \pwgc u$ and $u \pwgc j$ so that $u \not \in P_0 \Rightarrow u \in S_0 \Rightarrow j \not\in C_0$, a contradiction.  So every tuple in $D_{00}$ is a bona-fide edge of $\gcg$.  Suppose now that for some $j \in C_0$ there is an $i$ s.t. $(i, j) \in \gcg$ but $(i, j) \not\in D_{00}$.  Since $(i, j) \in \gcg$ we have $i \pwgc j$ (corollary \ref{cor:gc_implies_pwgc}), but then if $i \not\in P_0$ then $j \notin C_0$ and if $i \in P_0, j \in C_0$ then necessarily $(i, j) \in D_{00}$ since we cannot at this stage have any paths in $E$.

%   and show that on each step, every edge incident upon a node in $C_k$ is contained in $D_k$, that every edge in $D_k$ is an edge in $\gcg$, and finally that $\bigcup_{k = 1}^n C_k = [n] \setminus P_0$ and that this implies $\big([n], \bigcup_{k = 1}^n D_k\big) = \gcg$.


%  On the first iteration of the algorithm ($k = 0$) we have $S_0 = [n]$ and $E_0 = \emptyset$.  The set $P_0$ is non-empty since $\gcg$ is a DAG and therefore there are nodes in $\gcg$ without parents, thence if $P_0 = \emptyset$ proposition \ref{prop:ancestor_properties} would be contradicted.  Now, if $P_0 = [n]$ then the graph necessarily contains no edges and we will correctly return $E = \emptyset$.  On the other hand, if $P_0 \ne [n]$ then $C_0$ must also be non-empty, otherwise there would be driving nodes not included in $P_0$.  Consider now an edge $(i, j) \in D_0$ (again $D_0$ is non-empty for the same reason as $C_0$).  Since $i$ has no parents, we must have $\anc{i}\cap\anc{j} = \emptyset$ and therefore $i \in \anc{j}$, and due to the construction of $C_0$ we in fact have $i \in \pa{j}$, which establishes that every edge in $D_0$ is also an edge in $\gcg$.  Finally consider $j \in C_0$, by proposition \ref{prop:pwgc_anc} and since the parents of $C_0$ must be in $P_0$, we see that every edge incident on $j$ must be present in $D_0$.

% We also see that since nodes in $P_0$ have no parents,

% we must have $\anc{i}\cap\anc{j} = \emptyset$, and therefore by proposition \ref{prop:ancestor_properties} $i \in \anc{j}$;  moreover, the construction of $P_1$ implies that in fact $i \in \pa{j}$, so $(i, j)$ is an edge of $\gcg$.

%Finally, by similar reasoning, there cannot be an edge $(i, j) \in D_0$ which is not in $\gcg$, since $\anc{i} \cap \anc{j} = \emptyset$.

%Consider now step $K < n$ of algorithm \ref{alg:pwgr}.  By lemma \ref{lem:still_strongly_causal} the graph remains strongly causal after having removed nodes $\bigcup_{k = 0}^K P_k$ from $S$, and we assume for induction that $\bigcup_{k = 1}^K D_k$ are all edges of $\gcg$.

\hl{WIP}
\end{proof}

\begin{example}
  Being a strongly causal DAG is merely a sufficient condition.  For example, the complete directed graph with 2 nodes (e.g. $B(1) = \left[ \begin{array}{cc} 1/2 & 1 \\ 1 & 1/2 \end{array} \right]$) contains a loop but is pairwise recoverable, though not by algorithm (\ref{alg:pwgr}).  Clearly, this example is somewhat artificial since when $n = 2$ there is no difference between pairwise Granger-causality and joint Granger-causality amongst all series -- however, one can take any strongly causal DAG, and then add two additional nodes having such a loop without impacting the pairwise recoverability.
\end{example}

\section{Structure Learning}
\label{sec:structure_learning}
\subsection{Local Search Heuristics}


\subsection{Edge-Wise Grouped LASSO}
Minimize the sparsity promoting regularized least squares problem and choose the hyper-parameters against the Akaike information criteria.

\begin{equation}
  L(\lambda, p) \defeq \underset{B}{\text{minimize}}\ \frac{1}{T}\sum_{t = 1}^T||x(t) - \sum_{\tau = 1}^pB(\tau)x(t - \tau)||_2^2 + \lambda \sum_{i, j}\big[\alpha||B_{i, j}||_2 + (1 - \alpha)||B_{i, j}||_1\big]
\end{equation}

\begin{equation}
  \underset{\lambda, p}{\text{minimize}}\ L(\lambda, p) + \mathsf{AIC}(B^{(\lambda, p)})
\end{equation}

\subsection{Bayesian Posterior Thresholding}
Consider the Bayesian model

\begin{equation}
  \begin{aligned}
    (x(t)\ |\ B, \sigma_v^2, \{x(t - \tau)\}_{\tau = 1}^p) &\sim \mathcal{N}(\sum_{\tau = 1}^pB(\tau)x(t - \tau), \sigma_v^2)\\
    (B_{ij}(\tau)\ |\ G_{ij}(\tau)) &\sim G_{ij}(\tau)\mathcal{N}(0, \sigma_\beta^2) + (1 - G_{ij}(\tau))\delta_0(B_{ij}(\tau))\\
    (G_{ij}(\tau)) &\sim \mathsf{BER}(p_{ij}(\tau))\\
    \sigma_v^2 &\sim \Gamma(a_v, b_v)\\
    \sigma_\beta^2 &\sim \Gamma(a_\beta, b_\beta)
  \end{aligned}
\end{equation}

Similar models have been studied by various authors in the context of the linear regression model \hl{[(cite them)]} where it is referred to as \textit{stochastic variable selection}, or referred to as a model for Bayesian variable selection.

Since we have in mind applications where $n$ is large, sampling posterior probabilities can be prohibitively burdensome, so we can instead fit the mean-field variational approximation $p(B, G, \sigma\ |\ X) \approx q_Gq_Bq_\sigma$ and subsequently apply a thresholding operation to the posterior edge inclusion probabilities under $q_G$.

\subsection{Pairwise Minimum Error Spanning Trees}
Inspired by theorem \ref{thm:tree_recovery}, we propose the following
structure learning heuristic.

%% This algorithm should perform coordinate descent or jointly refit all the trees.

\begin{enumerate}
  \item{Set $k = 0$ and $x^0(t) = x(t)$}
  \item{Compute all pairwise causality measures $\Xi \defeq \big[\ln \frac{\xi_i}{\xi_{ij}} \big]_{i, j}$}
  \item{Find the maximum spanning arborescence\footnote{An ``arborescence'' is a french word for a tree diagram, and refers to a \textit{directed} tree in graph theory} $\mathcal{T}_k$ of a graph having edge weights $\Xi_{ij}$.}
  \item{Fit a vector LTI filter $F_k$ having graph defined by $\mathcal{T}_k$ to $x^{k - 1}(t)$ and set $x^k(t) = x^{k - 1}(t) - \hat{x}^{k - 1}(t)$}
  \item{Repeat over $k$ until a stopping criteria is met}
\end{enumerate}


\section{Application}
\section{Conclusion}

\printbibliography
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
