\documentclass{statsoc}
\usepackage{fullpage}
\usepackage{ccfonts,eulervm}
\usepackage{tabularx}
\usepackage{xr}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{definition}{section}}
\externaldocument{paper_theory}

\usepackage{geometry}

\geometry{
  textwidth=33pc,
  textheight=\dimexpr48\baselineskip+\topskip\relax,
  marginparsep=11pt,
  marginparwidth=107pt,
  footnotesep=6.65pt,
  headheight=9pt,
  headsep=9pt,
  footskip=30pt,
}
\bibliographystyle{chicago}
\usepackage{enumitem}
\usepackage{etex,etoolbox}
\usepackage{hyperref}
\usepackage{fullpage}
\setlength{\marginparwidth}{2cm}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{booktabs}

\def\gc{\overset{\text{GC}}{\rightarrow}}  % Granger causality arrow
\def\ngc{\overset{\text{GC}}{\nrightarrow}}  % Negated Granger causality arrow
\def\pwgc{\overset{\text{PW}}{\rightarrow}}  % Pairwise Granger causality arrow
\def\npwgc{\overset{\text{PW}}{\nrightarrow}}  % Negated pairwise Granger causality arrow
\def\te{\overset{\mathcal{T}}{\rightarrow}}  % Transfer entropy arrow
\def\gcg{\mathcal{G}}  % Granger-causality graph
\def\gcge{\mathcal{E}}  % Graph edges
\def\VAR{\mathsf{VAR}}  % VAR(p) model
\def\B{\mathsf{B}}  % Filter B
\def\wtB{\widetilde{\B}}  % General filter B
\def\A{\mathsf{A}}  % Filter A
\def\H{\mathcal{H}}  % Hilbert space

\newcommand{\linE}[2]{\hat{\E}[#1\ |\ #2]}  % Linear projection
\newcommand{\linEerr}[2]{\xi[#1\ |\ #2]}  % Error of linear projection
\newcommand{\pa}[1]{pa(#1)}  % Parents of a node
\newcommand{\anc}[1]{\mathcal{A}(#1)}  % Ancestors of a node
\newcommand{\ancn}[2]{\mathcal{A}_{#1}(#2)}  % nth ancestors of a node
\newcommand{\gpn}[2]{gp_{#1}(#2)}  % nth generation grandparents
\newcommand{\wtalpha}[2]{\widetilde{\alpha}(#1, #2)}  % Some notation for lem:pwgc_anc
\newcommand{\dist}[2]{\mathsf{d}(#1, #2)}  % Distance between things
\newcommand{\gcgpath}[2]{#1 \rightarrow \cdots \rightarrow #2}  % A shorter path command

% \usepackage{fullpage}
\usepackage{framed}

% Figures
\usepackage{graphicx}
\usepackage{caption}  % This is not recommended?
\usepackage{subcaption}
% \usepackage{wrapfig}
% \usepackage{svg}

% Math packages, theorem definitions and numbering
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{amsthm}
% \usepackage{mathrsfs} % Fancy scripted font
\usepackage{bm}  % Bold math

% Misc packages
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{algorithm2e} %{algorithm} environment
\usepackage{soul}  % \hl highlighting
\usepackage{color}
\usepackage{mathtools}  % For my \ceil function

% Theorems (with italics)
% \theoremstyle{plain}  % Style definition removes italics
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

% \theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{assumptions}{Assumptions}

% keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}

% General
\def\defeq{\overset{\Delta}{=}}  % Equal with triangle
\def\cl{\mathsf{cl\ }}  % Closure
\newcommand{\sgn}[1]{\mathsf{sgn}(#1)}  % sign function

% Calculus
\def\d{\mathsf{d}}  % Differential operator

% Functions
\def\ln{\mathsf{ln\ }}  % Natural logarithm
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}  % Ceiling

% Probability
\def\H{\mathcal{H}}  % Hilbert space
\def\E{\mathbb{E}}  % Expectation
\def\Var{\text{Var}}  % Variance
\def\P{\mathbb{P}}  % Probability Measure
\def\F{\mathcal{F}}  % A sigma algebra
\def\sX{\mathcal{X}}  % Another sigma algebra
\def\KL{\mathbf{D}_{KL}}  % KL divergence
\def\bF{\mathbf{F}}  % Whole F-meas space

% Standard sets
\def\Z{\mathbb{Z}}  % Set of integers
\def\R{\mathbb{R}}  % Set of real numbers
\def\C{\mathbb{C}}  % Set of complex numbers
\def\N{\mathbb{N}}  % Set of natural numbers
\def\ball{\mathbb{B}}  % Open ball
\def\clball{\overline{\ball}}  % Closed ball

% Linear algebra
\def\rk{\mathsf{rk }}  % The rank
\def\tr{\mathsf{tr }}  % The trace
\def\T{\mathsf{T}}  % Transpose notation
\def\c{\mathsf{c}}  % complement
\def\dg{\mathsf{dg }}   %  Diagonal vector of a matrix
\def\Dg{\mathsf{Dg }}   %  Diagonal matrix from a vector
\def\ind{\mathbf{1}}  % Ones vector or indicator
\def\matvec{\textbf{vec}}  % Vector operator
\def\<{\langle}  % < Inner product
\def\>{\rangle}  % > Inner product
\newcommand{\inner}[2]{\langle #1, #2 \rangle}  % Inner product
\newcommand{\innerT}[2]{#1^\T #2}  % Inner product for finite vectors

\graphicspath{{../figures/}}

\title{Graph Topological Aspects of Granger Causal Network Learning\\
  \large Supplementary Material}

\author[Author 1 {\it et al.}]{R. J. Kinnear}
\address{
  University of Waterloo,
  Waterloo,
  Canada.}
\email{Ryan@Kinnear.ca}

\author{R. R. Mazumdar}
\address{
  University of Waterloo,
  Waterloo,
  Canada.}

\begin{document}

\appendix

\section{Overview}
We restate our main results and provide detailed proofs.  Simple
Corollaries have their proofs in the main text, and are occasionally
referenced here.  The main Theorem is proven in Section
\ref{apx:proof_main_theorem}, and all of the building blocks are
established in Section \ref{apx:ancillary_results}.

We detail the methods used for our simulations and finite sample
implementation in Section \ref{sec:structure_learning} and provide
additional simulation results in Section \ref{apx:simulation}.

Code will be made available at
\url{https://github.com/RJTK/granger_causality}, as well as
accompanying this supplementary material.

\section{Proofs}
\subsection{Ancillary Results}
\label{apx:ancillary_results}
\begin{theorem}[Granger Causality Equivalences \ref{thm:granger_causality_equivalences}]
  The following are equivalent:

  \begin{enumerate}
  \item{$x_j \ngc x_i$}
  \item{$\forall \tau \in \N_+\ B_{ij}(\tau) = 0$ i.e. $\B_{ij}(z) = 0$}
  \item{$H_t^{i} \perp \H_{t - 1}^{j}\ |\ \H_{t - 1}^{-j}$}
  \item{$\linE{x_i(t)}{\H_{t - 1}^{-j}} = \linE{x_i(t)}{\H_{t - 1}}$}
  \end{enumerate}
\end{theorem}

\begin{proof}
  % Geweke uses the log of the ratio of the determinants of the residual variances
  % 
  % The equivalence $(1) \iff (3)$ is essentially a restatement of the
  % definition, but is in line with the seminal work of Geweke
  % \cite{geweke1982measurement}, \cite{geweke1984}.

  $(a) \Rightarrow (b)$ follows as a result of the uniqueness of orthogonal
  projection (i.e. the best estimate is necessarily the coefficients
  of the model).  $(b) \Rightarrow (c)$ follows since in computing
  $(y - \linE{y}{\H_{t - 1}^{-j}})$ for $y \in H_t^i$ it is sufficient
  to consider $y = x_i(t)$ by linearity, then since
  $H_{t - 1}^i \subseteq \H_{t - 1}^{-j}$ we have
  $(x_i(t) - \linE{x_i(t)}{\H_{t = 1}^{-j}}) = v_i(t)$ since
  $\B_{ij}(z) = 0$ and $v_i(t) \perp \H_{t - 1}$.  $(c) \iff (d)$ is a result
  of the equivalence in Definition
  \ref{lem:conditional_orthogonality_equivalence}.  And,
  $(d) \implies (a)$ follows directly from the Definition.
\end{proof}

\begin{lemma}
  \label{lem:adj_matrix}
  Let $S$ be the transposed adjacency matrix\footnote{\footnotesize We
    are using the convention that $\B_{ij}(z)$ is a filter with input
    $x_j$ and output $x_i$ so as to write the action of the system as
    $\B(z)x(t)$ with $x(t)$ as a column vector.  This competes with
    the usual convention for adjacency matrices where $A_{ij} = 1$ if
    there is an edge $(i, j)$.  In our case, the sparsity pattern of
    $\B_{ij}$ is the \textit{transposed} conventional adjacency
    matrix.} of the Granger-causality graph $\gcg$.  Then,
  $(S^k)_{ij}$ is the number of paths of length $k$ from node $j$ to
  node $i$.  Evidently, if $\forall k \in \N,\ (S^k)_{ij} = 0$ then
  $j \not\in \anc{i}$.
\end{lemma}
\begin{proof}
  This is a well known theorem, proof follows by induction.
\end{proof}

\begin{proposition}[Ancestor Expansion]
  \label{prop:parent_expanding}
  The component $x_i(t)$ of $x(t)$ can be represented in terms of it's
  parents in $\gcg$:

  \begin{equation}
    \label{eqn:parent_expansion}
    x_i(t) = v_i(t) + \B_{ii}(z)x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t).
  \end{equation}

  Moreover, $x_i$ can be expanded in terms of it's ancestor's $v(t)$
  components only:

  \begin{equation}
    \label{eqn:ancestor_expansion}
    x_i(t) = \A_{ii}(z)v_i(t) + \sum_{\substack{k \in \anc{i} \\ k \ne i}}\A_{ik}(z)v_k(t),
  \end{equation}

  where $\A(z) = \sum_{\tau = 0}^\infty A(\tau)z^{-\tau}$ is the filter from
  the Wold decomposition representation of $x(t)$, equation
  (\ref{eqn:wold}).
\end{proposition}

This statement is ultimately about the sparsity pattern in the Wold
decomposition matrices $A(\tau)$ since
$x_i(t) = \sum_{\tau = 0}^\infty \sum_{j = 1}^n A_{ij}(\tau)v_j(t -
\tau)$.  The proposition states that if $j \not \in \anc{i}$ then
$\A_{ij}(z) = 0$.  

\begin{proof}
  Equation \eqref{eqn:parent_expansion} is immediate from the
  $\VAR(\infty)$ representation of \eqref{eqn:ar_representation} and
  Theorem \ref{thm:granger_causality_equivalences}, we are left to
  demonstrate \eqref{eqn:ancestor_expansion}.
  
  From equation (\ref{eqn:ar_representation}), which we are assuming
  throughout the paper to be invertible, we can write

  \begin{equation*}
    x(t) = (I - \B(z))^{-1} v(t),
  \end{equation*}

  where $(I - \B(z))^{-1} = \A(z)$ due to the uniqueness of
  (\ref{eqn:wold}).  Since $\B(z)$ is stable we have

  \begin{equation}
    \label{eqn:resolvant_inv}
    (I - \B(z))^{-1} = \sum_{k = 0}^\infty \B(z)^k.
  \end{equation}

  Invoking the Cayley-Hamilton theorem allows writing the infinite sum
  of \eqref{eqn:resolvant_inv} in terms of \textit{finite} powers of
  $\B$.

  Let $S$ be a matrix with elements in $\{0, 1\}$ which represents the
  sparsity pattern of $\B(z)$, from lemma \ref{lem:adj_matrix} $S$ is
  the transpose of the adjacency matrix for $\gcg$ and hence
  $(S^k)_{ij}$ is non-zero if and only if $j \in \gpn{k}{i}$, and
  therefore $\B(z)^k_{ij} = 0$ if $j \not \in \gpn{k}{i}$.  Finally,
  since $\anc{i} = \bigcup_{k = 1}^n\gpn{k}{i}$ we see that
  $\A_{ij}(z)$ is zero if $j \not\in \anc{i}$.

  Therefore

  \begin{align*}
    x_i(t) &= [(I - \B(z))^{-1}v(t)]_i = \sum_{j = 1}^n \A_{ij}(z) v_j(t)\\
           &= \A_{ii}(z) v_i(t) + \sum_{\substack{j \in \anc{i} \\ j \ne i}} \A_{ij}(z) v_j(t)
  \end{align*}
\end{proof}

\begin{proposition}
  \label{prop:separated_ancestor_uncorrelated}
  Consider distinct nodes $i, j$ in a Granger-causality graph
  $\gcg$.  If

  \begin{enumerate}[label=(\alph*)]
  \item{$j \not\in \anc{i}$ and $i \not\in \anc{j}$}
  \item{$\anc{i}\cap\anc{j} = \emptyset$}
  \end{enumerate}

  then $\H_t^{(i)} \perp \H_t^{(j)}$, that is,
  $\forall s, \tau \in \Z_+\ \E[x_i(t - s)x_j(t - \tau)] = 0$.  Moreover,
  this means that $j \npwgc i$ and $\linE{x_j(t)}{\H_{t - 1}^i} = 0$.
\end{proposition}

\begin{proof}
  We show directly that
  $\forall s, \tau \in \Z_+\ \E[x_i(t - s)x_j(t - \tau)] = 0$.  To this end,
  fix $s, \tau \ge 0$, then by expanding with equation \eqref{eqn:ancestor_expansion} we have

  \begin{align*}
    \E x_i(t - s)x_j(t - \tau)
    &= \E \big(\A_{ii}(z)v_i(t - s)\big)\big(\A_{jj}(z)v_j(t - \tau)\big)\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\E[\big(\A_{ik}(z)v_k(t - s)\big)\big(\A_{jj}(z)v_j(t - \tau)\big)]\\
    &+ \sum_{\substack{k \in \anc{j} \\ k \ne j}}\E[\big(\A_{ii}(z)v_i(t - s)\big) \big(\A_{jk}(z) v_k(t - \tau)\big)]\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\sum_{\substack{\ell \in \anc{j} \\ \ell \ne j}}\E[\big(\A_{ik}(z)v_k(t - s)\big)\big(\A_{j\ell}(z)v_\ell(t - \tau)\big)].
  \end{align*}
  
  Keeping in mind that $v(t)$ is an isotropic and uncorrelated
  sequence we see that each of these above four terms are 0: the
  first term since $i \ne j$, the second and third since
  $j \not\in \anc{i}$ and $i \not\in \anc{j}$ and finally the fourth since
  $\anc{i} \cap \anc{j} = \emptyset$.
\end{proof}

\begin{proposition}
  \label{prop:ancestor_uncorrelated}
  Consider distinct nodes $i, j$ in a Granger-causality graph $\gcg$.
  If

  \begin{enumerate}[label=(\alph*)]
  \item{$j \not\in \anc{i}$}
  \item{$\anc{i}\cap\anc{j} = \emptyset$}
  \end{enumerate}

  then $j \npwgc i$.
\end{proposition}
\begin{proof}
  By Theorem \ref{thm:granger_causality_equivalences} it suffices to show that

  \begin{equation*}
    \inner{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^i}}{x_j(t) - \linE{x_j(t)}{\H_{t - 1}^i}} = 0.
  \end{equation*}

  which by the orthogonality principle is equivalent to

  \begin{equation}
    \label{eqn:proof_inner0}
    \inner{x_i(t)}{x_j(t) - \linE{x_j(t)}{\H_{t - 1}^i}} = 0.
  \end{equation}

  Define the disjoint sets

  \begin{align*}
    C_0(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, k \ne i \}\\
    C_1(u) &= \{k \in \pa{u}\ |\ i \in \anc{k} \text{ or } k = i \}.
  \end{align*}

  We can then expand the parents of $j$ using Equation \ref{eqn:parent_expansion} as

  \begin{equation*}
    x_j(t) = v_j(t) + \sum_{k \in C_0(j)} B_{jk}(z) x_k(t) + \sum_{k \in C_1(j)} B_{jk}(z) x_k(t),
  \end{equation*}

  which when substituted into the left hand side of Equation
  \ref{eqn:proof_inner0} results (by Proposition
  \ref{prop:separated_ancestor_uncorrelated}) in

  \begin{equation*}
    \inner{x_i(t)}{\sum_{k \in C_1(j)} B_{jk}(z) x_k(t) - \sum_{k \in C_1(j)} \linE{B_{jk}(z) x_k(t)}{\H_{t - 1}^i}}.
  \end{equation*}

  We can continue this process recursively (i.e. split each
  $k \in C_1(j)$ into $C_0(k)$ and $C_1(k)$) which must eventually
  terminate with $C_1(u) = \{i\}$.  Therefore, there exists some
  causal filter $\Phi(z)$ such that Equation \ref{eqn:proof_inner0} is
  equivalent to

  \begin{equation}
    \label{eqn:proof_inner0}
    \inner{x_i(t)}{\Phi(z)x_i(t) - \linE{\Phi(z)x_i(t)}{\H_{t - 1}^i}},
  \end{equation}

  which is $0$ since $\linE{\Phi(z)x_i(t)}{\H_{t - 1}^i} = \Phi(z)x_i(t)$.
\end{proof}

\begin{proposition}
  \label{prop:ancestor_properties}
  If in a Granger-causality graph $\gcg$ where $j \pwgc i$ then
  $j \in \anc{i}$ or $\exists k \in \anc{i} \cap\anc{j}$ which is a
  confounder of $(i, j)$.
\end{proposition}

\begin{proof}
  We will prove by way of contradiction.  To this end, suppose that
  $j$ is a node such that: $(a)$ $j \not \in \anc{i}$ and $(b)$ for
  every $k \in \anc{i} \cap \anc{j}$ every
  $k \rightarrow \cdots \rightarrow j$ path contains $i$.

  Firstly, notice that every $u \in \big(\pa{j} \setminus \{i\}\big)$
  necessarily inherits these same two properties.  This follows since
  if we also had $u \in \anc{i}$ then $u \in \anc{i} \cap \anc{j}$ so that every
  $u \rightarrow \cdots \rightarrow j$ path must contain $i$, but
  $u \in \pa{j}$, so this is not the case since $u \rightarrow j$ is a
  path that doesn't contain $i$; moreover, if we consider
  $w \in \anc{i} \cap \anc{u}$ then we also have
  $w \in \anc{i} \cap \anc{j}$ so every
  $w \rightarrow \cdots \rightarrow j$ path must contain $i$.  These
  properties therefore extend inductively to every
  $u \in \big(\anc{j} \setminus \{i\}\big)$.

  In order to deploy a recursive argument, define the following
  partition of $\pa{u}$, for some node $u$:

  \begin{align*}
    C_0(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} = \emptyset, k \ne i\}\\
    C_1(u) &= \{k \in \pa{u}\ |\ i \in \anc{k} \text{ or } k = i\}\\
    C_2(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} \ne \emptyset, k \ne i\}.
  \end{align*}

  We notice that for any $u$ having the properties $(a), (b)$ above, we
  must have $C_2(u) = \emptyset$ since if $k \in C_2(u)$ then
  $\exists w \in \anc{i} \cap \anc{k}$ s.t. $i \not \in \anc{k}$ and
  therefore there must be a path $\gcgpath{w}{k} \rightarrow u$ which
  does not contain $i$.

  Using this partition, we will expand $x_j(t)$ in terms of it's
  parents, and recursively expand nodes in $C_1$ until we reach a case
  where $C_1 = \emptyset$.  For the first step equation
  \eqref{eqn:parent_expansion} gives us:

  \begin{equation}
    \label{eqn:xj_partition_expansion}
    x_j(t) = \A_{jj}(z)\Big(v_j(t) + \sum_{k \in C_0(j)}\B_{ik}(z)x_k(t) + \sum_{k \in C_1(j)}\B_{ik}(z)x_k(t)\Big).
  \end{equation}

  Using this representation we choose an arbitrary $\Phi(z) x_i(t - 1) \in \H_{t - 1}^{(j)}$ and show that

  \begin{equation}
    \inner{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}{\Phi(z)x_j(t - 1) - \linE{\Phi(z)x_j(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation}

  which will imply (by Theorem \ref{thm:granger_causality_equivalences}) that $j \npwgc i$ and for which it is equivalent to show that

  \begin{equation}
    \label{eqn:sufficient_inner_prod}
    \inner{x_i(t)}{\Phi(z)x_j(t - 1) - \linE{\Phi(z)x_j(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation}

  by the orthogonality principle since $\linE{x_i(t)}{\H_{t - 1}^{(i)}} \in \H_{t - 1}^{(i)}$.  Substituting (\ref{eqn:xj_partition_expansion}) into (\ref{eqn:sufficient_inner_prod}) and starting with the first term we have

  \begin{align*}
    &\inner{x_i(t)}{\Phi(z)\A_{jj}(z)v_j(t - 1) - \linE{\Phi(z)\A_{jj}(z)v_j(t - 1)}{\H_{t - 1}^{(i)}}}\\
    \overset{(\alpha)}{=}\ &\inner{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}{\Phi(z)\A_{jj}(z)v_j(t - 1)}\\
    \overset{(\beta)}{=}\ &0,
  \end{align*}

  where $(\alpha)$ follows by expanding $x_i(t)$ with (\ref{eqn:ancestor_expansion}) and $\linE{\Phi(z)\A_{jj}(z)v_j(t - 1)}{\H_{t - 1}^{(i)}} = 0$ because $\forall \tau, s$

  \begin{equation*}
    \E v_j(t - \tau) x_i(t - s) = \E v_j(t - \tau) \sum_{k \in \anc{i} \cup \{i\}}\A_{ik}(z)v_k(t - s) = 0,
  \end{equation*}

  since $j \not \in \anc{i}$; $(\beta)$ follows similarly, that is, $j \not \in \anc{i}$.  Secondly we see that $\forall k \in C_0(j)$

  \begin{equation*}
    \inner{x_i(t)}{\Phi(z)\A_{jj}(z)\B_{ik}(z)x_k(t - 1) - \linE{\Phi(z)\A_{jj}(z)\B_{ik}(z)x_k(t - 1)}{\H_{t - 1}^{(i)}}} = 0,
  \end{equation*}

  which follows from Proposition \ref{prop:ancestor_uncorrelated}.  Finally,
  for $k \in C_1(j)$ the case $k = i$ is immediate (since the error in
  estimating $\Phi(z)\B_{ii}(z)x_i(t)$ given $\H_{t - 1}^{(i)}$ is
  $0$), so suppose $k \ne i$.  We know from above that $k$ inherits the
  key properties referred to as $(a)$ and $(b)$ above and therefore we
  can recursively expand $k$ in the same way as in equation
  (\ref{eqn:xj_partition_expansion}).  Continuing this recursion for
  each $k \in C_1(j)$ (where $k \ne i$) must eventually terminate since
  $i \in \anc{k}$.
\end{proof}

\begin{proposition}
  \label{prop:sc_graph_common_anc}
  In a strongly causal graph if $j \in \anc{i}$ then any
  $k \in \anc{i} \cap \anc{j}$ is not a confounder, that is,
  the unique path from $k$ to $i$ contains $j$.
\end{proposition}
\begin{proof}
  Suppose that there is a path from $k$ to $i$ which does not contain
  $j$.  In this case, there are multiple paths from $k$ to $i$ (one of
  which \textit{does} go through $j$ since $j \in \anc{i}$) which
  contradicts the assumption of strong causality.
\end{proof}

\begin{proposition}
  \label{prop:pwgc_anc}
  If $\gcg$ is a strongly causal DAG then $j \in \anc{i} \Rightarrow j \pwgc i$.
\end{proposition}
\begin{proof}
  We will show that for some $\psi \in \H_{t - 1}^{(j)}$ we have

  \begin{equation}
    \label{eqn:cond_ortho_proof}
    \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0
  \end{equation}

  and therefore that $H_t^{(i)} \not\perp\ \H_{t - 1}^{(j)}\ |\ \H_{t - 1}^{(i)}$, which by theorem (\ref{thm:granger_causality_equivalences}) is enough to establish that $j \pwgc i$.

  Firstly, we will establish a representation of $x_i(t)$ that involves $x_j(t)$.  Denote by $a_{r + 1} \rightarrow a_r \rightarrow \cdots \rightarrow a_1 \rightarrow a_0$ with $a_{r + 1} \defeq j$ and $a_0 \defeq i$ the \textit{unique} $\gcgpath{j}{i}$ path in $\gcg$, we will expand the representation of equation (\ref{eqn:parent_expansion}) backwards along this path:

  % Should this be written as a lemma?
  \begin{align*}
    x_i(t) &= v_i(t) + \B_{ii}(z) x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z) x_k(t)\\
           &= \underbrace{v_{a_0}(t) + \B_{a_0a_0}(z) x_i(t) + \sum_{\substack{k \in \pa{a_0} \\ k \ne a_1}}\B_{a_0 k}(z) x_k(t)}_{\defeq \wtalpha{a_0}{a_1}} + \B_{a_0a_1}(z)x_{a_1}(t)\\
           &= \wtalpha{a_0}{a_1} + \B_{a_0a_1}(z)\big[\wtalpha{a_1}{a_2} + \B_{a_1a_2}(z)x_{a_2}(t) \big]\\
           &\overset{(a)}{=} \sum_{\ell = 0}^r \underbrace{\Big(\prod_{m = 0}^{\ell - 1} \B_{a_m a_{m + 1}}(z) \Big)}_{\defeq F_\ell(z)} \wtalpha{a_\ell}{a_{\ell + 1}} + \Big(\prod_{m = 0}^{r}\B_{a_m a_{m + 1}}(z)\Big)x_{a_{r + 1}}(t)\\
           &= \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} + F_{r + 1}(z) x_j(t)
  \end{align*}

  where $(a)$ follows by a routine induction argument and where we define $\prod_{m = 0}^{-1} \bullet \defeq 1$ for notational convenience.

  Using this representation to expand equation (\ref{eqn:cond_ortho_proof}), we obtain the following cumbersome expression:

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}\\
    &- \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\linE{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}{\H_{t - 1}^{(i)}}}\\
    &+ \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}.
  \end{align*}

  Note that by the orthogonality principle, $\psi - \linE{\psi}{\H_{t - 1}^{(i)}} \perp \H_{t - 1}^{(i)}$, the middle term above is $0$.  Choosing now the particular value $\psi = F_{r + 1}(z)x_j(t) \in \H_{t - 1}^{(j)}$ we arrive at

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
    &= \E|F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}|^2\\
    &+ \inner{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}}},
  \end{align*}

  which by the Cauchy-Schwarz inequality is $0$ if and only if

  \begin{equation*}
    \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}} - F_{r + 1}(z)x_j(t),
  \end{equation*}

  or by rearranging and applying the representation obtained earlier, if and only if

  \begin{equation*}
    x_i(t) \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}},
  \end{equation*}

  but this is impossible since $x_i(t) \not \in \H_{t - 1}^{(i)}$.
\end{proof}

\begin{lemma}
  \label{lem:time_lag_cancellation}
  Suppose $v(t)$ is a scalar sequence with unit variance and zero
  autocorrelation and let $\A(z), \B(z)$ be nonzero and strictly
  causal (i.e. $\tau_0(\A) \ge 1$) linear filters.  Then,

  \begin{equation}
    \inner{F(z)\A(z)v(t)}{\B(z)v(t)} = 0\ \forall \text{ strictly causal filters } F(z)
  \end{equation}

  if and only if $\tau_0(\A) \ge \tau_\infty(\B)$.
\end{lemma}
\begin{proof}
  We have

  \begin{align}
    \inner{\A(z)v(t)}{\B(z)v(t)} &= \sum_{\tau = 1}^\infty \sum_{s = 1}^\infty a(\tau)b(s)\E[v(t - s)v(t - \tau)]\\
    &= \sum_{\tau = \text{max}(\tau_0(\A), \tau_0(\B))}^{\text{min}(\tau_\infty(\A), \tau_\infty(\B))} a(\tau) b(\tau)\\
  \end{align}

  due to the uncorrelatedness assumptions on $v(t)$.  This expression
  is $0$ if and only if $\tau_0(\A) \ge 1 + \tau_\infty(\B)$ or if
  $\tau_0(\B) \ge 1 + \tau_\infty(\A)$ or if the coefficients are
  orthogonal along the common support.

  Specializing this fact to $\inner{F(z)\A(z)v(t)}{\B(z)v(t)} = 0$ we
  see that the coefficients cannot be orthogonal for every choice of
  $F$, and that $\text{sup}_F \tau_\infty(F\A) = \infty$, leaving only
  the possibility that

  \begin{align*}
    \tau_0(F\A) \ge 1 + \tau_\infty(\B) \forall F &\overset{(a)}{\iff} \tau_0(\A) \ge 1 + \tau_\infty(\B) - \underset{F}{\text{min }} \tau_0(F)\\
    &\overset{(b)}{\iff} \tau_0(\A) \ge \tau_\infty(\B),
  \end{align*}

  where $(a)$ follows since $\tau_0(F\A) = \tau_0(F) + \tau_0(\A)$,
  and $(b)$ since $\text{min}_F\ \tau_0(F) = 1$.
\end{proof}

\begin{corollary}
  \label{cor:time_lag_cancellation}
  For $k \in \anc{i} \cap \anc{j}$ we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})
  \end{align*}
\end{corollary}
\begin{proof}
  The final equivalence follows immediately from Lemma \ref{lem:time_lag_cancellation}.  For the first equivalence we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)A_{jk}(z)v_k(t)}{x_i(t - \tau)} &= 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{align*}

  which can be expanded by equation \eqref{eqn:ancestor_expansion} to
  obtain (after cancelling all ancestors of $i$ other than $k$)

  \begin{equation*}
    \inner{F(z)A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t - \tau)} = 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{equation*}

  which by the Lemma is equivalent to $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$ as stated.
\end{proof}

\begin{proposition}
  \label{prop:persistence_converse}
  Suppose $\gcg$ is a strongly causal DAG and that $x(t)$ is
  persistent, then if there exists a $k$ which confounds $(i, j)$ we
  have $i \pwgc j$ and $j \pwgc i$.
\end{proposition}
\begin{proof}
  We will show that $j \pwgc i$, the other being symmetric.  First
  note also that by proposition \ref{prop:sc_graph_common_anc} we
  cannot have $i \in \anc{j}$ or $j \in \anc{i}$ and therefore every
  $k \in \anc{i}\cap\anc{j}$ will be a confounder.

  It is sufficient to show that $\exists \psi \in \H_{t - 1}^{(j)}$
  such that

  \begin{equation*}
    \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0.
  \end{equation*}

  To this end, let $F(z)$ be an arbitrary but strictly causal linear
  filter.  We apply equation \eqref{eqn:ancestor_expansion} to
  $x_i(t)$ and $\psi \defeq F(z)x_j(t)$:

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
    &\overset{(a)}{=} \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
    &\overset{(b)}{=} \inner{\sum_{k \in \anc{j}}\big(F(z)\A_{jk}(z)v_k(t) \\&- \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}\big)}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
    &\overset{(c)}{=} \sum_{k \in \anc{i}\cap\anc{j}}\Big(\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} \\&- \inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{ii}v_i(t)}\\
    &- \sum_{\ell \in \anc{i}}\inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{i\ell}(z)v_\ell(t)}\Big)
  \end{align*}

  where in $(a)$ we have removed the $\linE{x_i(t)}{\H_{t - 1}^{(i)}}$
  term via the orthogonality principle, in $(b)$ there is no
  $F(z)\A_{jj}(z)v_j(t)$ term since due to $j \not\in \anc{i}$ it is
  orthogonal to $\H_t^{(i)}$.  Finally, $(c)$ follows by applying
  orthogonality properties of $v(t)$, as well as the fact that
  $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$ for
  $k \not \in \anc{i}$.  Note that
  $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} \in \H_{t - 1}^{(i)}$
  and therefore there is in general no cancellation in the final term
  above for $\ell \in \anc{i}$.

  \todo{This is clearly not immediately evident}

  This is $0$ for every $F$ if and only if for all $F$ and
  $\forall k \in \anc{i} \cap \anc{j}$ we have
  $$\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$$ and
  $$\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} = 0,$$ which by
  Corollary \ref{cor:time_lag_cancellation} occurs if and only if
  $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$, which is impossible since by persistence
  $\tau_0(\A_{jk}) < \infty$ and  $\tau_\infty(\A_{ik}) = \infty$.
\end{proof}

\subsection{The Main Theorem}
\label{apx:proof_main_theorem}

\begin{theorem}[Pairwise Recovery]
  \label{thm:scg_recovery}
  If the Granger-causality graph $\gcg$ for persistent process $x(t)$
  is a strongly causal DAG then $\gcg$ can be inferred from pairwise
  causality tests.  The procedure can be carried out, assuming
  we have an oracle for pairwise causality, via Algorithm
  (\ref{alg:pwgr}).
\end{theorem}

\begin{algorithm}[H]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Initialize}{initialize}
  \DontPrintSemicolon

  % \BlankLine
  \caption{Pairwise Granger Causality Algorithm (PWGC)}
  \label{alg:pwgr}
  % \TitleOfAlgo{Pairwise Graph Recovery (PWGC)}
  % \Input{Pairwise Granger-causality relations between a persistent
  % process of dimension $n$ whose joint Granger-causality
  % relations are known to form a strongly causal DAG $\gcg$.}
  \Input{Pairwise Granger-causality relations}
  \Output{Edges $\gcge = \{(i, j) \in [n] \times [n]\ |\ i \gc j \}$ of
    the graph $\gcg$.}
  \Initialize{$S_0 = [n]$  \texttt{\# unprocessed nodes}\\
    $E_0 = \emptyset$  \texttt{\# edges of }$\gcg$\\
    % $P_0 = \emptyset$  \texttt{\# layer by layer driving nodes}\\
    $k = 1$ \texttt{\# a counter used only for notation}}
  \BlankLine
  $W \leftarrow \{(i, j)\ |\ i \pwgc j, j \npwgc i\}$  \texttt{\# candidate edges}\\
  $P_0 \leftarrow \{i \in S_0\ |\ \forall s \in S_0\ (s, i) \not\in W\}$  \texttt{\# parentless nodes}\\
  \While{$S_{k - 1} \ne \emptyset$}{
    $S_k \leftarrow S_{k - 1} \setminus P_{k - 1}$ \texttt{\# remove nodes with depth }$k - 1$\\
    $P_k \leftarrow \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W\}$   \texttt{\# candidate children}\\

    $D_{k0} \leftarrow \emptyset$\\
    \For{$r = 1, \ldots, k$} 
    {
      $Q \leftarrow E_{k - 1} \cup \big(\bigcup_{\ell = 0}^{r - 1} D_{k\ell}\big)$ \texttt{\# currently known edges}\\
      $D_{kr} \leftarrow \{(i, j) \in P_{k - r} \times P_k\ |\ (i, j) \in W,\ \text{no } i \rightarrow j \text{ path in } Q\}$
      % $E_k \leftarrow E_{k - 1} \cup D_{kr}$ \texttt{\# add edges to }$E_k$ \label{alg:inner_loop_end}\\
      % $W_{k + 1} \leftarrow W_k \setminus D_{kr}$  \texttt{\# remove edges from consideration}\\
    }
    $E_k \leftarrow E_{k - 1} \cup \big(\bigcup_{r = 1}^k D_{kr}\big)$ \texttt{\# update } $E_k$ \texttt{ with new edges}\\
    % \label{alg_line:inner_loop_end}\\
    % $W_{k + 1} \leftarrow W_k \setminus \big(\bigcup_{r = 0}^k D_{kr}\big)$  \texttt{\# remove edges from consideration}\\
    $k \leftarrow k + 1$
  }
  \Return{$E_{k - 1}$}
\end{algorithm}

Our proof proceeds in 5 steps stated formally as lemmas.  Firstly, we
characterize the sets $W$ and $P_k$.  Then we establish a correctness
result for the inner loop on $r$, a correctness result for the outer
loop on $k$, and finally that the algorithm terminates in a finite
number of steps.

\begin{lemma}[$W$ Represents Ancestor Relations]
  \label{lem:W_subset_E}
  In Algorithm \ref{alg:pwgr} we have
  $(i, j) \in W$ if and only if $i \in \anc{j}$.  In particular,
  $W \subseteq \gcge$.
\end{lemma}
\begin{proof}
  Let $j \in [n]$ and suppose that $i \in \anc{j}$.  Then $i \pwgc j$
  by Proposition \ref{prop:pwgc_anc}.  Proposition
  \ref{prop:sc_graph_common_anc} ensures that $(i, j)$ are not
  confounded and Corollary \ref{cor:parent_corollary} that
  $j \not\in \anc{i}$ so $j \npwgc i$ and thence by Proposition
  \ref{prop:ancestor_properties} $(i, j) \in W$.

  Conversely, suppose $(i, j) \in W$.  Then since $j \npwgc i$
  Proposition \ref{prop:persistence_converse} ensures that $(j, i)$
  are not confounded and so by Proposition \ref{prop:ancestor_properties}
  we must have $i \in \anc{j}$.
\end{proof}

\begin{definition}[Depth]
  For our present purposes we will define the \textit{depth} $d(j)$ of
  a node $j$ in $\gcg$ to be the length of the \textit{longest} path
  from a node in $P_0$ to $j$, where $d(j) = 0$ if $j \in P_0$.  It is
  apparent that such a path will always exist.  For example, in Figure
  \ref{fig:example_fig3} we have $d(3) = 1$ and $d(4) = 2$.
\end{definition}

\begin{lemma}[Depth Characterization of $P_k$]
  \label{lem:depth_lemma}
  $i \in P_k \iff d(i) = k$ and $j \in S_k \iff d(j) \ge k$.
\end{lemma}
\begin{proof}
  We proceed by induction, noting that $P_0$ is non-empty since $\gcg$
  is acyclic and therefore $\gcg$ contains nodes without parents.  The
  base case $i \in P_0 \iff d(i) = 0$ is by definition, and
  $j \in S_0 \iff d(j) \ge 0$ is trivial since $S_0 = [n]$.  So
  suppose that the lemma is true up to $k - 1$.

  ($i \in P_k \implies d(i) = k$): Let $i \in P_k$.  Suppose that
  $d(i) \ge k + 1$, then $\exists j \in \pa{i}$ such that
  $j \not\in \cup_{r \ge 1}P_{k - r}$ (otherwise $d(i) \le k$), this
  implies that $j \in S_k$ with $(j, i) \in W$ (by Lemma
  \ref{lem:W_subset_E}) which is not possible due to the construction of
  $P_k$ and therefore $d(i) \le k$.  Moreover,
  $P_k \subseteq S_k \subseteq S_{k - 1}$ implies that
  $d(i) \ge k - 1$ by the induction hypothesis, but if $d(i) = k - 1$
  then $i \in P_{k - 1}$ again by induction which is impossible since
  $i \in P_k$ and therefore $d(i) = k$.

  ($s \in S_k \implies d(s) \ge k$): Let
  $s \in S_k \subseteq S_{k - 1}$.  We have by induction that
  $d(s) \ge k - 1$, but again by induction (this time on $P_{k - 1}$)
  we have $d(s) \ne k - 1$ since $S_k = S_{k - 1} \setminus P_{k - 1}$
  and therefore $d(s) \ge k$.

  ($d(i) = k \implies i \in P_k$): Suppose $i \in [n]$ is such that
  $d(i) = k$.  Then $i \in S_{k - 1}$ by the hypothesis, but also
  $i \not\in P_{k - 1}$ so then
  $i \in S_k = S_{k - 1} \setminus P_{k - 1}$ and thus $d(i) \ge k$.
  Now, recalling the definition of $P_k$

  \begin{equation*}
    P_k = \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W \},
  \end{equation*}

  if $s \in S_k$ is such that $(s, i) \in W$ then $s \pwgc i$ and
  $i \npwgc s$ so that by persistence and Proposition
  \ref{prop:persistence_converse} there cannot be a confounder of
  $(s, i)$ (otherwise $i \pwgc s$) so then by Proposition
  \ref{prop:ancestor_properties} we have $s \in \anc{i}$.  We have
  shown that $s \in S_k \implies d(s) \ge k$ and so we must have
  $d(i) > k$, a contradiction, thence $s \not\in \anc{i}$,
  $s \npwgc i$, $(s, i) \not\in W$ and $i \in P_k$.

  ($d(j) \ge k \implies j \in S_k$): Let $j \in [n]$ such that
  $d(j) \ge k$, then by induction we have $j \in S_{k - 1}$.  This
  implies by the construction of $S_k$ that $j \not\in S_k$ only if
  $j \in P_{k - 1}$, but we have shown that this only occurs when
  $d(j) = k - 1$, but $d(j) > k - 1$ so $j \in S_k$.
\end{proof}

\begin{lemma}[Inner Loop]
  \label{lem:inner_loop_lemma}
  Fix an integer $k \ge 1$ and suppose that $(i, j) \in E_{k - 1}$ if
  and only if $(i, j) \in \gcge$ and $d(j) \le k - 1$.  Then, we have
  $(i, j) \in D_{kr}$ if and only if $(i, j) \in \gcge $, $d(j) = k$,
  and $d(i) = k - r$.
\end{lemma}
\begin{proof}
  We prove by induction on $r$, keeping in mind the results of Lemmas
  \ref{lem:W_subset_E} and \ref{lem:depth_lemma}.  For the base case,
  let $r = 1$ and suppose that $(i, j) \in \gcge$ with $d(j) = k$ and
  $d(i) = k - 1$.  Then, $(i, j) \in W$ and by our assumptions on
  $E_{k - 1}$ there is no $\gcgpath{i}{j}$ path in $E_{k - 1}$
  and therefore $(i, j) \in D_{k1}$.  Conversely, suppose that
  $(i, j) \in D_{k1}$.  Then, $d(i) = k - 1$ and $d(j) = k$ which, since
  $(i, j) \in W \implies i \in \anc{j}$ implies that
  $i \in \pa{j}$ and $(i, j) \in \gcge$.

  Now, fix $r > 1$ and suppose that the result holds up to $r - 1$.
  Let $(i, j) \in \gcge$ with $d(j) = k$ and $d(i) = k - r$.  Then,
  $(i, j) \in W$ and by induction and strong causality there cannot
  already be an $\gcgpath{i}{j}$ path in
  $E_{k - 1} \cup \big(\bigcup_{\ell = 0}^{r - 1} D_{kr}\big)$,
  therefore $(i, j) \in D_{kr}$.  Conversely, suppose
  $(i, j) \in D_{kr}$.  Then we have $d(i) = k - r$, $d(j) = k$, and
  $i \in \anc{j}$.  Suppose by way of contradiction that
  $i \not\in \pa{j}$, then there must be some $u \in \pa{j}$ such that
  $i \in \anc{u}$.  But, this implies that $d(i) < d(u)$ and by
  induction that $(u, j) \in \bigcup_{\ell = 1}^{r - 1}D_{k\ell}$.
  Moreover, since $d(u) < k$ (otherwise $d(j) > k$) each edge in
  the $\gcgpath{i}{u}$ path must already be in $E_{k - 1}$, and so
  there must be an $\gcgpath{i}{j}$ path in
  $E_{k - 1}\cup\big(\bigcup_{\ell = 0}^{r - 1}D_{kr}\big)$, which is
  a contradiction since we assumed $(i, j) \in D_{kr}$.  Therefore
  $i \in \pa{j}$ and $(i, j) \in \gcge$.
\end{proof}

\begin{lemma}[Outer Loop]
  \label{lem:outer_loop_lemma}
  We have $(i, j) \in E_k$ if and only if $(i, j) \in \gcge$ and
  $d(j) \le k$.  That is, at iteration $k, E_k$ and $\gcge$ agree on
  the set of edges whose terminating node is at most $k$ steps away
  from $P_0$.
\end{lemma}
\begin{proof}
  We will proceed by induction.  The base case $E_0 = \emptyset$ is
  trivial, so fix some $k \ge 1$, and suppose that the lemma holds for
  all nodes of depth less than $k$.

  Suppose that
  $(i, j) \in E_k = E_{k - 1}\cup \big(\bigcup_{r = 1}^k D_{rk}
  \big)$.  Then clearly there is some $1 \le r \le k$ such that
  $(i, j) \in D_{kr}$ so that by Lemma \ref{lem:inner_loop_lemma} we
  have $(i, j) \in \gcge$ and $d(j) = k$.

  Conversely, suppose that $(i, j) \in \gcge$ and $d(j) \le k$.  If
  $d(j) < k$ then by induction $(i, j) \in E_{k - 1} \subseteq E_k$ so
  suppose further than $d(j) = k$.  Since $i \in \pa{j}$ we must have
  $d(i) < k$ (else $d(j) > k$) and again by Lemma
  \ref{lem:inner_loop_lemma} $(i, j) \in \bigcup_{r = 1}^k D_{kr}$
  which implies that $(i, j) \in E_k$.  The result follows.
\end{proof}

\begin{lemma}[Finite Termination]
  Algorithm \ref{alg:pwgr} terminates and returns $E_{k^\star - 1} = \gcge$
  for some $k^\star \le n$.
\end{lemma}
\begin{proof}
  If $n = 1$, the algorithm is clearly correct, returning on the first
  iteration with $E_1 = \emptyset$.  When $n > 1$ Lemma
  \ref{lem:outer_loop_lemma} ensures that $E_k$ coincides with
  $\{(i, j) \in \gcge\ |\ d(j) \le k\}$ and since $d(j) \le n - 1$ for
  any $j \in [n]$ there is some $k^\star \le n$ such that
  $E_{k^\star - 1} = \gcge$.  We must have $S_{k^\star} = \emptyset$
  since $j \in S_{k^\star} \iff d(j) \ge k^\star$ (if $d(j) > k - 1$ then
  $E_{k^\star - 1} \ne \gcge$) and therefore the algorithm terminates.
\end{proof}

\section{Finite Sample Implementation}
\label{sec:structure_learning}
In this section we provide a review of our methods for implementing
Algorithm 1 given a \textit{finite} sample of $T$ data points.  We
apply the simplest reasonable methods in order to maintain a focus on
our main contributions (i.e. Algorithm \ref{alg:pwgr}), more
sophisticated schemes can only serve to improve the results.  Textbook
reviews of the following concepts are provided e.g. by
\cite{all_of_statistics}, \cite{murphy_mlp}, and elsewhere.

In subsection \ref{sec:pairwise_hypothesis_testing} we define pairwise
Granger-causality hypothesis tests, in subsection
\ref{sec:model_order_selection} a model order selection criteria, in
subsection \ref{sec:efficient_model_estimation} an efficient
estimation algorithm, in subsection \ref{sec:error_rate_control} the
method for choosing an hypothesis testing threshold, and finally in
subsection \ref{sec:finite_pwgc} the unified finite sample algorithm.

\subsection{Pairwise Hypothesis Testing}
\label{sec:pairwise_hypothesis_testing}
In performing pairwise checks for Granger-causality $x_j \pwgc x_i$ we
follow the simple scheme of estimating the following two linear models:

\begin{align}
  H_0:&\ \widehat{x}_i^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau),\\
  H_1:&\ \widehat{x}_i^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau) + \sum_{\tau = 1}^pb_{ij}(\tau)x_j(t - \tau).
\end{align}

We formulate the statistic 

\begin{equation}
  \label{eqn:gc_statistics}
  F_{ij}(p) = \frac{T}{p}\Big(\frac{\xi_i(p)}{\xi_{ij}(p)} - 1\Big),
\end{equation}

where $\xi_i(p)$ is the sample mean square of the
residuals\footnote{This quantity is often denoted $\widehat{\sigma}$,
  but we maintain notation from Definition
  \ref{def:granger_causality}.}  $x_i(t) - \widehat{x}^{(p)}_i(t)$,

\begin{equation*}
  \xi_i(p) = \frac{1}{T - p}\sum_{t = p + 1}^T (x_i(t) - \widehat{x}_i^{(p)}(t))^2,
\end{equation*}

and similarly for $\xi_{ij}(p)$.  We test $F_{ij}(p)$ against a
$\chi^2(p)$ distribution.

If the estimation procedure is consistent, we will have the following
convergence (in $\P$ or a.s.):

\begin{equation}
  F_{ij}(p) \rightarrow
  \left\{
    \begin{array}{ll}
      0;\ x_j \npwgc x_i\\
      \infty;\ x_j \pwgc x_i
    \end{array}
  \right. \text{ as } T \rightarrow \infty.  % the '.' after \right is necessary.
\end{equation}

In our finite sample implementation (see Algorithm
\ref{alg:finite_pwgc}) we add edges to $\widehat{\gcg}$ in order of
the decreasing magnitude of $F_{ij}$ instead of proceeding backwards
through $P_{k - r}$ in Algorithm \ref{alg:pwgr}.  This makes greater
use of the information provided by the test statistic $F_{ij}$,
moreover, if $x_i \gc x_j$ and $x_j \gc x_k$, it is expected that
$F_{kj} > F_{ki}$, thereby providing the same effect as proceeding
backwards through $P_{k - r}$.
\subsection{Model Order Selection}
\label{sec:model_order_selection}
There are a variety of methods to choose the filter order $p$ (see
e.g. \cite{lutkepohl2005new}), but we will focus in particular on the
Bayesian Information Criteria (BIC).  The BIC is substantially more
conservative than the popular alternative Akaiake Information Criteria
(the BIC is also asymptotically consistent), and since we are
searching for \textit{sparse graphs}, we therefore prefer the BIC,
where we seek to \textit{minimize} over $p$:

\begin{equation}
  \label{eqn:bic}
  \begin{aligned}
    BIC_{\text{univariate}}(p) &= \ln\ \xi_i(p) + p\frac{\ln T}{T},\\
    BIC_{\text{bivariate}}(p) &= \ln \det \widehat{\Sigma}_{ij}(p) + 4p\frac{\ln T}{T},\\
  \end{aligned}
\end{equation}

where $\widehat{\Sigma}_{ij}(p)$ is the $2 \times 2$ residual
covariance matrix for the $\VAR(p)$ model of $(x_i(t), x_j(t))$.  The
bivariate errors $\xi_{ij}(p)$ and $\xi_{ji}(p)$ are the diagonal
entries of $\widehat{\Sigma}_{ij}(p)$.

We carry this out by a simple direct search on each model order
between $0$ and some prescribed $p_\text{max}$, resulting in a
collection $p_{ij}$ of model order estimates.  In practice, it is
sufficient to pick $p_\text{max}$ ad-hoc or via some simple heuristic
e.g. plotting the sequence $BIC(p)$ over $p$, though it is not
technically possible to guarantee that the optimal $p$ is less than
the chosen $p_\text{max}$ (since there can in general be arbitrarily
long lags from one variable to another).

\subsection{Efficient Model Estimation}
\label{sec:efficient_model_estimation}
In practice, the vast majority of computational effort involved in
implementing our estimation algorithm is spent calculating the error
estimates $\xi_i(p_i)$ and $\xi_{ij}(p_{ij})$.  This requires fitting a
total of $n^2p_{\text{max}}$ autoregressive models, where the most
naive algorithm (e.g. solving a least squares problem for each model)
for this task will consume $O(n^2p_{\text{max}}^4T)$ time, it is
possible to carry out this task in a much more modest
$O(n^2p_{\text{max}}^2 ) + O(n^2p_{\text{max}}T)$ time via the
autocorrelation method
\cite{hayes_statistical_digital_signal_processing} which substitutes
the following autocovariance estimates in the Yule-Walker
equations:\footnote{The particular indexing and normalization given in
  equation \ref{eqn:covariance_estimate} is critical to ensure
  $\widehat{R}$ is positive semidefinite.  The estimate can be viewed
  as calculating the covariance sequence of a signal multiplied by a
  rectangular window.}

\begin{equation}
  \label{eqn:covariance_estimate}
  \widehat{R}_x(\tau) = \frac{1}{T}\sum_{t = \tau + 1}^T x(t) x(t - \tau)^\T;\ \tau = 0, \ldots, p_{\text{max}},
\end{equation}

It is imperative that the first index in the summation is $\tau + 1$, as
opposed perhaps to $p_\text{max}$ and that the normalization is
$1 / T$, as opposed perhaps to $1 / (T - p_\text{max})$, in order to
guarantee that $\widehat{R}_x(\tau)$ forms a valid (i.e. positive
definite) covariance sequence.  This results in some bias, however the
dramatic computational speedup is worth it for our purposes.

These covariance estimates constitute the $O(n^2p_{\text{max}}T)$
operation.  Given these particular estimates, the variances $\xi_i(p)$
for $p = 1, \ldots, p_{\text{max}}$ can be evaluated in
$O(p_{\text{max}}^2)$ time each by applying the Levinson-Durbin
recursion to $\widehat{R}_{ii}(\tau)$, which effectively estimates a
sequence of $AR$ models, producing $\xi_i(p)$ as a side-effect (see
\cite{hayes_statistical_digital_signal_processing} and
\cite{levinson_durbin_recursion}).

Similarly, the variance estimates $\widehat{\Sigma}_{ij}(p)$ (which
include $\xi_{ij}$ and $\xi_{ji}$) can be obtained by estimating
$\frac{(n + 1)n}{2}$ bivariate AR models, again in
$O(p_{\text{max}}^2)$ time via Whittle's generalized Levinson-Durbin
recursion\footnote{We have made use of standalone tailor made
  implementations of these algorithms, available at
  \textsf{github.com/RJTK/Levinson-Durbin-Recursion}.}
\cite{whittle_generalized_levinson_durbin}.

\subsection{Edge Probabilities and Error Rate Controls}
\label{sec:error_rate_control}
Denote $F_{ij}$ the Granger-causality statistic of equation
\ref{eqn:gc_statistics} with model orders chosen by the methods of
Section \ref{sec:model_order_selection}.  We assume that this
statistic is asymptotically $\chi^2(p_{ij})$ distributed (the
disturbances are Gaussian), and denote by $G$ the cumulative
distribution function thereof.  We will define the matrix

\begin{equation}
  \label{eqn:edge_inclusion_probability}
  P_{ij} = G(F_{ij}),
\end{equation}

to be the matrix of pairwise edge inclusion P-values.  This is
motivated by the hypothesis test where the hypothesis $H_0$ will be
rejected (and thence we will conclude that $x_j \pwgc x_i$) if
$P_{ij} > 1 - \delta$.

The value $\delta$ can be chosen by a variety of methods, in our case
we apply the Benjamini Hochberg criteria \cite{benjamini_hochberg}
\cite{all_of_statistics} to control the false discovery rate of
pairwise edges to a level $\alpha$ (where we generally take
$\alpha = 0.05$).

\subsection{Finite Sample Recovery Algorithm}
\label{sec:finite_pwgc}

After the graph topology $\widehat{\gcg}$ has been estimated via
Algorithm \ref{alg:finite_pwgc}, we refit the entire model with the
specified sparsity pattern directly via ordinary least squares.

We note that producing graph estimates which are not strongly causal
can potentially be achieved by performing sequential estimates
$\widehat{x}_1(t), \widehat{x}_2(t), \ldots$ estimating a strongly causal
graph with the residuals of the previous model as input, and then
refitting on the combined sparsity pattern.  We intend to consider
this heuristic in future work.

\begin{algorithm}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \SetKwInOut{Initialize}{initialize}
    \DontPrintSemicolon

    \BlankLine
    \caption{Finite Sample Pairwise Graph Recovery (PWGC)}
    \label{alg:finite_pwgc}

    \Input{Estimates of pairwise Granger-causality statistics $F_{ij}$
      (eqn \ref{eqn:gc_statistics}).  Matrix of edge probabilities $P_{ij}$ (eqn \ref{eqn:edge_inclusion_probability}).  Hypothesis testing threshold $\delta$ chosen via the Benjamini-Hochberg criterion (Section \ref{sec:error_rate_control})}
    \Output{A strongly causal graph $\widehat{\gcg}$}
    \Initialize{$S = [n]$  \texttt{\# unprocessed nodes}\\
      $E = \emptyset$  \texttt{\# edges of }$\widehat{\gcg}$\\
      $k = 1$ \texttt{\# a counter used only for notation}}

    \BlankLine

    $W_\delta \leftarrow \{(i, j)\ |\ P_{ji} > 1 - \delta, F_{ji} > F_{ij}\}$  \texttt{\# candidate edges}\\
    $\mathcal{I}_0 \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# total node incident probability}\\
    $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) < \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Nodes with fewest incident edges}\\
    \If{$P_0 = \emptyset$}{
      $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) \le \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Ensure non-empty}
    }
    \BlankLine

    \While{$S \ne \emptyset$}{
      $S \leftarrow S \setminus P_{k - 1}$ \texttt{\# remove processed nodes}\\
      % $\mathcal{I}_k \leftarrow \big(\sum_{j \in S: (j, i) \in W_\delta} F_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# intra-}$S$ \texttt{incident strength}\\
      $\mathcal{I}_k \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$\\
      $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) < \ceil{\text{min}(\mathcal{I}_k)}\}$\\
      \If{$P_k = \emptyset$}{
        $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) \le \ceil{\text{min}(\mathcal{I}_k)}\}$
      }
      \;
      \texttt{\# add strongest edges, maintaining strong causality}\\
      $U_k \leftarrow \bigcup_{r = 1}^k P_{k - r}$ \texttt{\# Include all forward edges}\\
      \For{$(i, j) \in \mathsf{sort}\Big(\{(i, j) \in U_k \times P_k\ |\ (i, j) \in W_\delta\} \mathsf{\ by\ descending\ } F_{ji}\Big)$} {
        \If{$\mathsf{is\_strongly\_causal}(E \cup \{(i, j)\})$} {
          \texttt{\# }$\mathsf{is\_strongly\_causal}$ \texttt{can be implemented by keeping}\\
          \texttt{\# track of ancestor / descendant relationships}\\
          $E \leftarrow E \cup \{(i, j)\}$
        }
      }
      $k \leftarrow k + 1$\\
    }
    \Return{$([n], E)$}
\end{algorithm}

\section{Simulation}
\label{apx:simulation}
We have implemented our empirical experiments in Python \cite{scipy},
in particular we leverage the LASSO implementation from
\texttt{sklearn} \cite{sklearn} and the random graph generators from
\texttt{networkx} \cite{networkx}.  We run experiments using two
separate graph topologies having $n = 50$ nodes. These are generated
respectively by drawing a random tree and a random Erdos Renyi graph
then creating a directed graph by directing edges from lower numbered
nodes to higher numbered nodes.

We populate each of the edges (including self loops) with random
linear filters constructed by placing $5$ transfer function poles
(i.e. $p = 5$) uniformly at random in a disc of radius $3 / 4$ (which
guarantees stability for acyclic graphs).  The resulting system is
driven by i.i.d. Gaussian random noise, each component having random
variance $\sigma_i^2 = 1/2 + r_i$ where $r_i \sim \text{exp}(1/2)$.  To ensure
we are generating data from a stationary system, we first discard
samples during a long burnin period.

For both PWGC and adaLASSO We set the maximum lag length
$p_{\text{max}} = 10$.

Results are collected in Figures
\ref{fig:simulation_results_comparison1},
\ref{fig:simulation_results_comparison2},
\ref{fig:simulation_results_scaling_and_small_T},
\ref{fig:simulation_results_dense}.

\begin{figure}
  \centering
  \caption{PWGC Compared Against AdaLASSO \cite{adaptive_lasso_zou2006} (SCG)}
  \label{fig:simulation_results_comparison1}
  \includegraphics[width=0.95\textwidth]{new_lasso_comparison_scg_pmax15_simulation.pdf}

  {\footnotesize Comparison of PWGC and LASSO for $\VAR(p)$ model
    estimation.  We make comparisons against both the MCC and the
    relative log mean-squared prediction error
    $\frac{\ln\tr \widehat{\Sigma}_v}{\ln\tr \Sigma_v}$.  Results
    in Figure \ref{fig:simulation_results_comparison1} are for systems
    guaranteed to satisfy the assumptions required for Theorem
    \ref{thm:scg_recovery}.}
\end{figure}

\begin{figure}
  \caption{PWGC vs adaLASSO (DAG, $q = \frac{2}{n}$)}
  \label{fig:simulation_results_comparison2}
  \includegraphics[width=0.95\textwidth]{new_lasso_comparison_dag_pmax15_simulation.pdf}

  {\footnotesize Figure \ref{fig:simulation_results_comparison2}
    provides results for systems which do not guarantee the
    assumptions of Theorem \ref{thm:scg_recovery}, though the graph
    has a similar level of sparsity.}
\end{figure}

\begin{figure}
  \centering
  \caption{PWGC Scaling and Small Sample Performance}
  \label{fig:simulation_results_scaling_and_small_T}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{Fixed $T$, increasing $n$ (SCG)}
    \label{fig:simulation_results_scaling}
    \includegraphics[width=\linewidth]{new_increasing_n_simulation.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{MCC Comparison for $T \le 100$}
    \label{fig:small_T_comparison}
    \includegraphics[width=\linewidth]{new_mcc_comparison001.pdf}
  \end{subfigure}

  {\footnotesize Figure \ref{fig:simulation_results_scaling} measures
    support recovery performance as the number of nodes $n$ increases,
    and the edge proportion as well as the number of samples $T$ is
    held fixed.  Remarkably, the degradation as $n$ increases is
    limited, it is primarily the graph topology (SCG or non-SCG) as
    well as the level of sparsity (measured by $q$) which are the
    determining factors for support recovery performance.

    Figure \ref{fig:small_T_comparison} provides a support recovery
    comparison for very small values of, $T$ typical for many
    applications.}
\end{figure}

\begin{figure}
  \caption{Fixed $T, n$, increasing edges $q$ (DAG)}
  \label{fig:simulation_results_dense}
  \includegraphics[width=\linewidth]{dag_increasing_q_small_T_alasso_simulation.pdf}

  {\footnotesize Figure \ref{fig:simulation_results_dense} provides a
    comparison between PWGC and AdaLASSO as the density of graph edges
    (as measured by $q$) increases.  For reference,
    $\frac{2}{n} = 0.04$ has approximately the same level of sparsity
    as the SCGs we simulated.  As $q$ increases, the AdaLASSO
    outperforms PWGC as measured by the MCC.  However, PWGC maintains
    superior performance for 1-step-ahead prediction.  We speculate
    that this is a result of fitting the sparsity pattern recovered by
    PWGC via OLS which directly seeks to optimize this metric, whereas
    the LASSO is encumbered by the sparsity inducing penalty.}
\end{figure}

In reference to figure \ref{fig:simulation_results_comparison1} it
should not be overly surprising that our PWGC algorithm performs
better than the LASSO for the case of a strongly causal graph, since
in this case the theory from which our heuristic derives is valid.
However, the performance is still markedly superior in the case of a
more general DAG.  We would conjecture that a DAG having a similar
degree of sparsity as an SCG is ``likely'' to be ``close'' to an SCG,
in some appropriate sense.

Figure \ref{fig:simulation_results_dense} illustrates the severe
(expected) degradation in performance as the number of edges increases
while the number of data samples $T$ remains fixed.  For larger values
$q$ in this plot, the number of edges in the graph is comparable to
the number of data samples.

We have also paid close attention to the performance of PWGC in the
very small sample ($T \le 100$) regime (see Figure
\ref{fig:small_T_comparison}), as this is the regime many applications
must contend with.

In regards scalability, we have observed that performing the $O(n^2)$
pairwise Granger-causality calculations consumes the vast majority
($> 90\%$) of the computation time.  Since this step is trivially
parallelizable, our algorithm also scales well with multiple cores or
multiple machines.  Figure \ref{fig:simulation_results_scaling} is a
demonstration of this scalability, where we are able to estimate
graphs having over $1500$ nodes (over $2.25 \times 10 ^6$ possible edges)
using only $T = 500$ data points, granted, an SCG on this many nodes
is extremely sparse.

\clearpage
\bibliography{\string~/Documents/academics/global_academics/global_bib}

\end{document}
