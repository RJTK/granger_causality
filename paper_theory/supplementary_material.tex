\documentclass{statsoc}
\usepackage{fullpage}
\usepackage{ccfonts,eulervm}
\usepackage{tabularx}
\usepackage{xr}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{definition}{section}}
\AtAppendix{\counterwithin{equation}{section}}
\externaldocument{graph_topology_granger_causality}

\usepackage{geometry}

\geometry{
  textwidth=33pc,
  textheight=\dimexpr48\baselineskip+\topskip\relax,
  marginparsep=11pt,
  marginparwidth=107pt,
  footnotesep=6.65pt,
  headheight=9pt,
  headsep=9pt,
  footskip=30pt,
}
\bibliographystyle{chicago}
\usepackage{enumitem}
\usepackage{etex,etoolbox}
\usepackage{hyperref}
\usepackage{fullpage}
\setlength{\marginparwidth}{2cm}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{booktabs}

\def\gc{\overset{\text{GC}}{\rightarrow}}  % Granger causality arrow
\def\ngc{\overset{\text{GC}}{\nrightarrow}}  % Negated Granger causality arrow
\def\pwgc{\overset{\text{PW}}{\rightarrow}}  % Pairwise Granger causality arrow
\def\npwgc{\overset{\text{PW}}{\nrightarrow}}  % Negated pairwise Granger causality arrow
\def\te{\overset{\mathcal{T}}{\rightarrow}}  % Transfer entropy arrow
\def\gcg{\mathcal{G}}  % Granger-causality graph
\def\gcge{\mathcal{E}}  % Graph edges
\def\VAR{\mathsf{VAR}}  % VAR(p) model
\def\B{\mathsf{B}}  % Filter B
\def\wtB{\widetilde{\B}}  % General filter B
\def\A{\mathsf{A}}  % Filter A
\def\H{\mathcal{H}}  % Hilbert space

\newcommand{\linE}[2]{\hat{\E}[#1\ |\ #2]}  % Linear projection
\newcommand{\linEerr}[2]{\xi[#1\ |\ #2]}  % Error of linear projection
\newcommand{\pa}[1]{pa(#1)}  % Parents of a node
\newcommand{\anc}[1]{\mathcal{A}(#1)}  % Ancestors of a node
\newcommand{\ancn}[2]{\mathcal{A}_{#1}(#2)}  % nth ancestors of a node
\newcommand{\gpn}[2]{gp_{#1}(#2)}  % nth generation grandparents
\newcommand{\wtalpha}[2]{\widetilde{\alpha}(#1, #2)}  % Some notation for lem:pwgc_anc
\newcommand{\dist}[2]{\mathsf{d}(#1, #2)}  % Distance between things
\newcommand{\gcgpath}[2]{#1 \rightarrow \cdots \rightarrow #2}  % A shorter path command

% \usepackage{fullpage}
\usepackage{framed}

% Figures
\usepackage{graphicx}
\usepackage{caption}  % This is not recommended?
\usepackage{subcaption}
% \usepackage{wrapfig}
% \usepackage{svg}

% Math packages, theorem definitions and numbering
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{amsthm}
% \usepackage{mathrsfs} % Fancy scripted font
\usepackage{bm}  % Bold math

% Misc packages
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{algorithm2e} %{algorithm} environment
\usepackage{soul}  % \hl highlighting
\usepackage{color}
\usepackage{mathtools}  % For my \ceil function

% Theorems (with italics)
% \theoremstyle{plain}  % Style definition removes italics
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

% \theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{assumptions}{Assumptions}

% keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}

% General
\def\defeq{\overset{\Delta}{=}}  % Equal with triangle
\def\cl{\mathsf{cl\ }}  % Closure
\newcommand{\sgn}[1]{\mathsf{sgn}(#1)}  % sign function

% Calculus
\def\d{\mathsf{d}}  % Differential operator

% Functions
\def\ln{\mathsf{ln\ }}  % Natural logarithm
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}  % Ceiling

% Probability
\def\H{\mathcal{H}}  % Hilbert space
\def\E{\mathbb{E}}  % Expectation
\def\Var{\text{Var}}  % Variance
\def\P{\mathbb{P}}  % Probability Measure
\def\F{\mathcal{F}}  % A sigma algebra
\def\sX{\mathcal{X}}  % Another sigma algebra
\def\KL{\mathbf{D}_{KL}}  % KL divergence
\def\bF{\mathbf{F}}  % Whole F-meas space

% Standard sets
\def\Z{\mathbb{Z}}  % Set of integers
\def\R{\mathbb{R}}  % Set of real numbers
\def\C{\mathbb{C}}  % Set of complex numbers
\def\N{\mathbb{N}}  % Set of natural numbers
\def\ball{\mathbb{B}}  % Open ball
\def\clball{\overline{\ball}}  % Closed ball

% Linear algebra
\def\rk{\mathsf{rk }}  % The rank
\def\tr{\mathsf{tr }}  % The trace
\def\T{\mathsf{T}}  % Transpose notation
\def\c{\mathsf{c}}  % complement
\def\dg{\mathsf{dg }}   %  Diagonal vector of a matrix
\def\Dg{\mathsf{Dg }}   %  Diagonal matrix from a vector
\def\ind{\mathbf{1}}  % Ones vector or indicator
\def\matvec{\textbf{vec}}  % Vector operator
\def\<{\langle}  % < Inner product
\def\>{\rangle}  % > Inner product
\newcommand{\inner}[2]{\langle #1, #2 \rangle}  % Inner product
\newcommand{\innerT}[2]{#1^\T #2}  % Inner product for finite vectors

\graphicspath{{../figures/}}

\title{Graph Topological Aspects of Granger Causal Network Learning\\
  \large Supplementary Material}

\author[Author 1 {\it et al.}]{R. J. Kinnear}
\address{
  University of Waterloo,
  Waterloo,
  Canada.}
\email{Ryan@Kinnear.ca}

\author{R. R. Mazumdar}
\address{
  University of Waterloo,
  Waterloo,
  Canada.}

\begin{document}

\appendix

\section{Overview}
We restate our main results and provide detailed proofs.  Simple
Corollaries have their proofs in the main text, and are occasionally
referenced here.  The main Theorem is proven in Section
\ref{apx:proof_main_theorem}, and all of the building blocks are
established in Section \ref{apx:ancillary_results}.

We detail the methods used for our simulations and finite sample
implementation in Section \ref{sec:structure_learning} and provide
additional simulation results in Section \ref{apx:simulation}.

References to equations, lemmas, etc., in this document are prefixed
with their section, whereas prefix-free equation numbers refer to the
main document.  e.g. ``Equation (1)'' refers to the first equation in
the main document, and ``Equation (A.1)'' refers to the first equation
in this document.

Code will be made available at
\url{https://github.com/RJTK/granger_causality}, as well as
accompanying this supplementary material.

\section{Proofs}
\subsection{Preparatory Results}
\label{apx:ancillary_results}
\begin{theorem}[Granger Causality Equivalences \ref{thm:granger_causality_equivalences}]
  The following are equivalent:

  \begin{enumerate}
  \item{$x_j \ngc x_i$}
  \item{$\forall \tau \in \N_+\ B_{ij}(\tau) = 0$ i.e. $\B_{ij}(z) = 0$}
  \item{$H_t^{i} \perp \H_{t - 1}^{j}\ |\ \H_{t - 1}^{-j}$}
  \item{$\linE{x_i(t)}{\H_{t - 1}^{-j}} = \linE{x_i(t)}{\H_{t - 1}}$}
  \end{enumerate}
\end{theorem}

\begin{proof}
  % Geweke uses the log of the ratio of the determinants of the residual variances
  % 
  % The equivalence $(1) \iff (3)$ is essentially a restatement of the
  % definition, but is in line with the seminal work of Geweke
  % \cite{geweke1982measurement}, \cite{geweke1984}.

  $(a) \Rightarrow (b)$ follows as a result of the uniqueness of orthogonal
  projection (i.e. the best estimate is necessarily the coefficients
  of the model).  $(b) \Rightarrow (c)$ follows since in computing
  $(y - \linE{y}{\H_{t - 1}^{-j}})$ for $y \in H_t^i$ it is sufficient
  to consider $y = x_i(t)$ by linearity, then since
  $H_{t - 1}^i \subseteq \H_{t - 1}^{-j}$ we have
  $(x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{-j}}) = v_i(t)$ since
  $\B_{ij}(z) = 0$.  The result follows since
  $v_i(t) \perp \H_{t - 1}$.  $(c) \iff (d)$ is a result of the equivalence
  in Definition \ref{lem:conditional_orthogonality_equivalence}.  And,
  $(d) \implies (a)$ follows directly from the Definition.
\end{proof}

\begin{lemma}
  \label{lem:adj_matrix}
  Let $S$ be the transposed adjacency matrix\footnote{\footnotesize We
    are using the convention that $\B_{ij}(z)$ is a filter with input
    $x_j$ and output $x_i$ so as to write the action of the system as
    $\B(z)x(t)$ with $x(t)$ as a column vector.  This competes with
    the usual convention for adjacency matrices where $A_{ij} = 1$ if
    there is an edge $(i, j)$.  In our case, the sparsity pattern of
    $\B_{ij}$ is the \textit{transposed} conventional adjacency
    matrix.} of the Granger-causality graph $\gcg$.  Then,
  $(S^k)_{ij}$ is the number of paths of length $k$ from node $j$ to
  node $i$.  Evidently, if $\forall k \in \N,\ (S^k)_{ij} = 0$ then
  $j \not\in \anc{i}$.
\end{lemma}
\begin{proof}
  This is a well known theorem, proof follows by induction.
\end{proof}

\begin{proposition}[Ancestor Expansion]
  \label{prop:parent_expanding}
  The component $x_i(t)$ of $x(t)$ can be represented in terms of it's
  parents in $\gcg$:

  \begin{equation}
    \label{eqn:parent_expansion}
    x_i(t) = v_i(t) + \B_{ii}(z)x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t).
  \end{equation}

  Moreover, $x_i$ can be expanded in terms of it's ancestor's $v(t)$
  components only:

  \begin{equation}
    \label{eqn:ancestor_expansion}
    x_i(t) = \A_{ii}(z)v_i(t) + \sum_{\substack{k \in \anc{i} \\ k \ne i}}\A_{ik}(z)v_k(t),
  \end{equation}

  where $\A(z) = \sum_{\tau = 0}^\infty A(\tau)z^{-\tau}$ is the filter from
  the Wold decomposition representation of $x(t)$, equation
  (\ref{eqn:wold}).
\end{proposition}

\begin{proof}
  Equation \eqref{eqn:parent_expansion} is immediate from the
  $\VAR(\infty)$ representation of Equation \eqref{eqn:ar_representation}
  and Theorem \ref{thm:granger_causality_equivalences}, we are left to
  demonstrate \eqref{eqn:ancestor_expansion}.
  
  From equation (\ref{eqn:ar_representation}), which we are assuming
  throughout the paper to be invertible, we can write

  \begin{equation*}
    x(t) = (I - \B(z))^{-1} v(t),
  \end{equation*}

  where necessarily $(I - \B(z))^{-1} = \A(z)$ due to the uniqueness
  of the Wold decomposition.  Since $\B(z)$ is stable we have

  \begin{equation}
    \label{eqn:resolvant_inv}
    (I - \B(z))^{-1} = \sum_{k = 0}^\infty \B(z)^k.
  \end{equation}

  Invoking the Cayley-Hamilton theorem allows writing the infinite sum
  of \eqref{eqn:resolvant_inv} in terms of \textit{finite} powers of
  $\B$.

  Let $S$ be a matrix with elements in $\{0, 1\}$ which represents the
  sparsity pattern of $\B(z)$, from lemma \ref{lem:adj_matrix} $S$ is
  the transpose of the adjacency matrix for $\gcg$ and hence
  $(S^k)_{ij}$ is non-zero if and only if $j \in \gpn{k}{i}$, and
  therefore $\B(z)^k_{ij} = 0$ if $j \not \in \gpn{k}{i}$.  Finally,
  since $\anc{i} = \bigcup_{k = 1}^n\gpn{k}{i}$ we see that
  $\A_{ij}(z)$ is zero if $j \not\in \anc{i}$.

  Therefore

  \begin{align*}
    x_i(t) &= [(I - \B(z))^{-1}v(t)]_i\\
           &= \sum_{j = 1}^n \A_{ij}(z) v_j(t)\\
           &= \A_{ii}(z) v_i(t) + \sum_{\substack{j \in \anc{i} \\ j \ne i}} \A_{ij}(z) v_j(t)
  \end{align*}
\end{proof}

\begin{proposition}
  \label{prop:separated_ancestor_uncorrelated}
  Consider distinct nodes $i, j$ in a Granger-causality graph
  $\gcg$.  If

  \begin{enumerate}[label=(\alph*)]
  \item{$j \not\in \anc{i}$ and $i \not\in \anc{j}$}
  \item{$\anc{i}\cap\anc{j} = \emptyset$}
  \end{enumerate}

  then $\H_t^{(i)} \perp \H_t^{(j)}$, that is,
  $\forall s, \tau \in \Z_+\ \E[x_i(t - s)x_j(t - \tau)] = 0$.  Moreover,
  this means that $j \npwgc i$ and $\linE{x_j(t)}{\H_t^i} = 0$.
\end{proposition}

\begin{proof}
  We show directly that
  $\forall s, \tau \in \Z_+\ \E[x_i(t - s)x_j(t - \tau)] = 0$.  To this end, fix
  $s, \tau \ge 0$, then by expanding with Equation
  \eqref{eqn:ancestor_expansion} we have

  \begin{align*}
    \E x_i(t - s)x_j(t - \tau)
    &= \E \big(\A_{ii}(z)v_i(t - s)\big)\big(\A_{jj}(z)v_j(t - \tau)\big)\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\E[\big(\A_{ik}(z)v_k(t - s)\big)\big(\A_{jj}(z)v_j(t - \tau)\big)]\\
    &+ \sum_{\substack{\ell \in \anc{j} \\ \ell \ne j}}\E[\big(\A_{ii}(z)v_i(t - s)\big) \big(\A_{j\ell}(z) v_\ell(t - \tau)\big)]\\
    &+ \sum_{\substack{k \in \anc{i} \\ k \ne i}}\sum_{\substack{\ell \in \anc{j} \\ \ell \ne j}}\E[\big(\A_{ik}(z)v_k(t - s)\big)\big(\A_{j\ell}(z)v_\ell(t - \tau)\big)].
  \end{align*}
  
  Keeping in mind that $v(t)$ is an isotropic and uncorrelated
  sequence we see that each of these above four terms are 0: the
  first term since $i \ne j$, the second and third since
  $j \not\in \anc{i}$ and $i \not\in \anc{j}$ and finally the fourth since
  $\anc{i} \cap \anc{j} = \emptyset$.
\end{proof}

\begin{lemma}
  \label{lem:vj_perp}
  Consider distinct nodes $i, j$ in a Granger-causality graph $\gcg$.
  If $j \not \in \anc{i}$, then $\H_t^{v_j} \perp \H_{t}^{i}$, and therefore
  for any causal filter $\Phi(z)$ we have

  \begin{align*}
    \linE{\Phi(z)v_j(t)}{\H_{t - 1}^i} &= 0\\
    \inner{x_i(t)}{\Phi(z)v_j(t)} & = 0.
  \end{align*}
\end{lemma}
\begin{proof}
  Fix $\tau, s \ge 0$, then by expanding with Equation \ref{eqn:ancestor_expansion}

  \begin{align*}
    \E[x_i(t - \tau)v_j(t - s)] &= \E[\big(\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t) \big) v_j(t - s)]\\
    &= 0.
  \end{align*}

  This follows since $i \ne j$ and $j \not \in \anc{i}$ and $v(t)$ is
  isotrophic and uncorrelated.
\end{proof}

\begin{proposition}
  \label{prop:ancestor_uncorrelated}
  Consider distinct nodes $i, j$ in a Granger-causality graph $\gcg$.
  If

  \begin{enumerate}[label=(\alph*)]
  \item{$j \not\in \anc{i}$}
  \item{$\anc{i}\cap\anc{j} = \emptyset$}
  \end{enumerate}

  then $j \npwgc i$.
\end{proposition}
\begin{proof}
  By Theorem \ref{thm:granger_causality_equivalences} it suffices to show that

  \begin{equation*}
    \forall \psi \in \H_{t - 1}^j\ \inner{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^i}}{\psi - \linE{\psi}{\H_{t - 1}^i}} = 0.
  \end{equation*}

  which by the orthogonality principle and by representing
  $\psi \in \H_{t - 1}^j$ via the action of some strictly causal filter
  $\Phi(z)$ on $x_j(t)$ is equivalent to

  \begin{equation}
    \label{eqn:proof_inner0}
    \inner{x_i(t)}{\Phi(z)x_j(t) - \linE{\Phi(z)x_j(t)}{\H_{t - 1}^i}} = 0.
  \end{equation}

  If we expand $x_j(t)$ using Equation \eqref{eqn:ancestor_expansion},
  the left hand side of \eqref{eqn:proof_inner0} becomes

  \begin{equation*}
    \inner{x_i(t)}{\sum_{k \in \anc{j} \cup \{j\}} \Big(\Phi(z)\A_{jk}(z)v_k(t) - \linE{\Phi(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^i}\Big)}.
  \end{equation*}

  We see that this is $0$ by Lemma \ref{lem:vj_perp} since
  $j \not \in \anc{i}$, and
  \[
    \anc{i} \cap \anc{j} = \emptyset \implies \forall k \in \anc{j}: k \not \in \anc{i}.
  \]
\end{proof}

% I believe this is an incorrect proof since it relies on inverting
% (1 - B_{jj}(z)), which need not be invertible.  For example,
% B(z) = [[r, -a], [a, 0]]z^{-1} is invertible whenever
% max(|r - root(r^2 - 4a^2)|, |r + root(r^2 - 4a^2)|) < 2
% which includes values of r > 1.

% \begin{proof}
%   By Theorem \ref{thm:granger_causality_equivalences} it suffices to show that

%   \begin{equation*}
%     \forall \psi \in \H_{t - 1}^j\ \inner{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^i}}{\psi - \linE{\psi}{\H_{t - 1}^i}} = 0.
%   \end{equation*}

%   which by the orthogonality principle and by representing
%   $\psi \in \H_{t - 1}^j$ via the action of some strictly causal filter
%   $\Phi(z)$ on $x_j(t)$ is equivalent to

%   \begin{equation}
%     \label{eqn:proof_inner0}
%     \inner{x_i(t)}{\Phi(z)x_j(t) - \linE{\Phi(z)x_j(t)}{\H_{t - 1}^i}} = 0.
%   \end{equation}

%   Given a node $u$, consider the partition of $\pa{u}$:

%   \begin{align*}
%     C_0(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, k \ne i \}\\
%     C_1(u) &= \{k \in \pa{u}\ |\ i \in \anc{k} \text{ or } k = i \}.
%   \end{align*}

%   We can then expand the parents of $j$ using Equation \ref{eqn:parent_expansion} as

%   \begin{align*}
%     x_j(t) &= v_j(t) + \B_{jj}(z)x_j(t) + \sum_{k \in C_0(j)} B_{jk}(z) x_k(t) + \sum_{k \in C_1(j)} B_{jk}(z) x_k(t)\\
%     &= \widetilde{A}_{jj}(z)\Big(v_j(t) + \sum_{k \in C_0(j)} B_{jk}(z) x_k(t) + \sum_{k \in C_1(j)} B_{jk}(z) x_k(t)\Big),
%   \end{align*}

%   % \todo{Is this true??}
%   where $(1 - B_{jj}(z))^{-1} = \widetilde{A}_{jj}(z)$, which exists
%   due to the invertibility of $\A(z)$. When this expression is
%   substituted into the left hand side of Equation
%   \ref{eqn:proof_inner0} it results in

%   \begin{equation}
%     \label{eqn:proof_inner1}
%     \begin{aligned}
%       \inner{x_i(t)}{\sum_{k \in C_1(j)} \big(\widetilde{A}_{jj}(z)\Phi(z)B_{jk}(z) x_k(t) &- \linE{\widetilde{A}_{jj}(z)\Phi(z)B_{jk}(z) x_k(t)}{\H_{t - 1}^i}\big)},
%     \end{aligned}
%   \end{equation}

%   where (by Proposition \ref{prop:separated_ancestor_uncorrelated}) we
%   have eliminated each of the terms from $C_0(u)$, and by Lemma
%   \ref{lem:vj_perp} the terms involving $v_j(t)$.

%   Now, for any $u \in \anc{j}$, we must have $u \not \in \anc{i}$, since
%   otherwise $u \in \anc{i} \cap \anc{j}$, which is empty by hypothesis.
%   Moreover, for any $u \in \anc{j}$, we have
%   $\anc{i} \cap \anc{u} = \emptyset$, since
%   $w \in \anc{i} \cap \anc{u} \implies w \in \anc{i} \cap \anc{j}$. Therefore,
%   we can continue the above process recursively splitting each
%   $k \in C_1(j)$ into $C_0(k)$ and $C_1(k)$) upwards along each path
%   from $j$ to $i$, terminating whenever we encounter $i$.  This process
%   is guaranteed to terminate since $i \in \anc{k}$.  This leaves us with
%   some strictly causal filter $\Psi(z)$ such that Expression
%   \ref{eqn:proof_inner1} is equivalent to

%   \begin{equation}
%     \inner{x_i(t)}{\Psi(z)x_i(t) - \linE{\Psi(z)x_i(t)}{\H_{t - 1}^i}},
%   \end{equation}

%   which is $0$ since $\Psi(z)x_i(t) \in \H_{t - 1}^i$ and
%   therefore $\linE{\Psi(z)x_i(t)}{\H_{t - 1}^i} = \Psi(z)x_i(t)$.
% \end{proof}

\begin{remark}
  In order to prove Proposition \ref{prop:ancestor_properties} we
  require some additional notation, as well as another representation
  theorem.  The difficulty addressed by the following Definition
  \ref{def:scc} and Lemma \ref{lem:scc_expansion} is that in the
  representation of $x_j(t)$ in terms of it's parents (i.e. Equation
  \eqref{eqn:parent_expansion})

  \[
    x_i(t) = v_i(t) + \B_{ii}(z)x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t),
  \]

  the filter $\B_{ii}(z)$ need not be stable.  That is, the inverse
  filter $(1 - \B_{ii}(z))^{-1}$ need not exist.  An example of this
  issue is furnished by

  \[
    \B(z) = \left[
    \begin{array}{cc}
      \rho & -a\\
      a & 0\\
    \end{array}
  \right] z^{-1},
  \]

  for which, depending on the value of $a$, may still be stable even
  if $|\rho| > 1$.  This implies that it is not always possible to
  represent $x_i(t)$ in terms of $v_i(t)$ and $x_k(t), k \in \pa{i}$
  alone, i.e. as

  \[
    x_i(t) = (1 - \B_{ii}(z))^{-1}\big(v_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z)x_k(t)\big).
  \]

  The difficulty presented by the non-existence of such a
  representation may become apparent upon studying the proof of
  Proposition \ref{prop:ancestor_properties}.
\end{remark}

\begin{definition}[Strongly Connected Components]
  \label{def:scc}
  In a graph $\gcg$, the \textit{ordered} (by the natural ordering on
  $\N$) subset $S \subseteq [n]$ is \textit{strongly connected} if
  $\forall i, j \in S$, $i \in \anc{j}$ and $j \in \anc{i}$.  We will
  denote by $S(j)$ (which may be the singleton $(j)$) the largest
  strongly connected component (SCC) containing $j$.  We will denote
  $x_{S(j)}(t)$ to be the vector of processes

  \[
    x_{S(j)}(t) = \big(x_s(t)\ |\ s \in S(j)\big),
  \]

  whose indices are given the same (natural) ordering as $S(j)$.
  Similarly, the sub-filter of $\B(z)$ acting on $x_{S(j)}(t)$ will be
  denoted $\B_{S(j)}(z)$.
\end{definition}

\begin{lemma}[Expansion in SCCs]
  \label{lem:scc_expansion}
  Given some $j \in [n]$, the process $x_{S(j)}(t)$ can be represented
  by
% {\substack{k \in \pa{a_0} \\ k \ne a_1}}
  \begin{equation}
    \label{eqn:scc_parent_expansion}
    x_{S(j)}(t) = v_{S(j)}(t) + \B_{S(j)}(z)x_{S(j)}(t) + \sum_{\substack{s \in S(j) \\ k \in \pa{s} \cap S(j)^\c}} B_{sk}(z)x_k(t)e_s^{S(j)},
  \end{equation}

  where $e_s^{S(j)}$ denotes the length $|S(j)|$ canonical basis
  vector with a $1$ in the component corresponding to $x_s$ in the
  vector $x_{S(j)}$, and the summation is a double sum on $s$ and $k$.

  Moreover, the filter $\B(z)$ is stable with $I - \B_{S(j)}(z)$
  invertible:

  \begin{equation}
    \label{eqn:scc_inversion}
    (I - \B_{S(j)}(z))^{-1} = \sum_{k = 0}^\infty \B_{S(j)}(z)^k,
  \end{equation}

  therefore

  \begin{equation}
    \label{eqn:scc_parent_expansion_inverted}
    x_{S(j)}(t) = (I - \B_{S(j)}(z))^{-1} \big(v_{S(j)}(t) + \sum_{\substack{s \in S(j) \\ k \in \pa{s} \cap S(j)^\c}} B_{sk}(z)x_k(t)e_s^{S(j)}\big).
  \end{equation}
  
\end{lemma}
\begin{proof}
  The representation of Equation \eqref{eqn:scc_parent_expansion}
  follows directly from the $\VAR$ representation of $x(t)$ (i.e. Equation
  \eqref{eqn:ar_representation})

  \[
    x(t) = \B(z)x(t) + v(t),
  \]

  which, when rearranged appropriately, can be written as

  \[
    \left[
      \begin{array}{c}
        x_{S(j)}(t)\\
        x_{S(j)^c}(t)
      \end{array}
    \right] =
    \left[
      \begin{array}{cc}
        \B_{S(j)}(z)& \B_{S(j), S(j)^\c}(z)\\
        \B_{S(j)^\c, S(j)}(z)& \B_{S(j)^\c}(z)
      \end{array}
    \right]
        \left[
      \begin{array}{c}
        x_{S(j)}(t)\\
        x_{S(j)^c}(t)
      \end{array}
    \right] +
    \left[
      \begin{array}{c}
        v_{S(j)}(t)\\
        v_{S(j)^c}(t)
      \end{array}
    \right].
  \]

  Theorem \ref{thm:granger_causality_equivalences} is invoked in order
  to restrict the summation to $k \in \pa{s}$ (since other elements
  are $0$).

  Now, we can partition $\gcg$ into it's maximal SCCs
  $S_1, \ldots, S_N$, (one of which is $S(j)$) and then consider the
  DAG formed on $N$ nodes with edges $I \rightarrow J$ included on the
  condition that
  $\exists j \in S_J, i \in S_I \text{ s.t. } i \in \anc{j}$.  By
  topologically sorting this DAG, we obtain an ordering $\sigma$ of
  $[n]$ such that $\B_\sigma(z)$ is block upper triangluar, with one
  of it's diagonal blocks consisting of the (possibly reordered)
  matrix $\B_{S(j)}(z)$.  So we have

  \begin{align*}
    \forall\ |z^{-1}| \le 1: \det \B(z) = \prod_{i = 1}^N \det \B_{S_i}(z) &\ne 0\\
    \implies \forall\ |z^{-1}| \le 1: \det \B_{S(j)}(z) &\ne 0,
  \end{align*}

  and therefore $\B_{S(j)}(z)$ is stable, invertible, and Equation
  \eqref{eqn:scc_inversion} holds.
\end{proof}

\begin{proposition}
  \label{prop:ancestor_properties}
  If in a Granger-causality graph $\gcg$ where $j \pwgc i$ then
  $j \in \anc{i}$ or $\exists k \in \anc{i} \cap\anc{j}$ which is a
  confounder of $(i, j)$.
\end{proposition}

\begin{proof}
  We will prove by way of contradiction.  To this end, suppose that
  $j$ is a node such that:

  \begin{enumerate}[label=(\alph*)]
    \item{$j \not \in \anc{i}$}
    \item{every $k \in \anc{i} \cap \anc{j}$ every
        $k \rightarrow \cdots \rightarrow j$ path contains $i$.}
  \end{enumerate}

  Firstly, notice that every $u \in \big(\pa{j} \setminus \{i\}\big)$
  necessarily inherits these same two properties.  This follows since
  if we also had $u \in \anc{i}$ then $u \in \anc{i} \cap \anc{j}$ so by our
  assumption every $u \rightarrow \cdots \rightarrow j$ path must contain
  $i$, but $u \in \pa{j}$ so $u \rightarrow j$ is a path that doesn't contain
  $i$, and therefore $u \not\in \anc{i}$; moreover, if we consider
  $w \in \anc{i} \cap \anc{u}$ then we also have
  $w \in \anc{i} \cap \anc{j}$ so the assumption implies that every
  $w \rightarrow \cdots \rightarrow j$ path must contain $i$.  These properties therefore
  extend inductively to every $u \in \big(\anc{j} \setminus \{i\}\big)$.

  In order to deploy a recursive argument, define the following
  partition of $\pa{u}$, for some node $u$:

  \begin{align*}
    C_0(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} = \emptyset, k \ne i\}\\
    C_1(u) &= \{k \in \pa{u}\ |\ i \in \anc{k} \text{ or } k = i\}\\
    C_2(u) &= \{k \in \pa{u}\ |\ i \not\in \anc{k}, \anc{i} \cap \anc{k} \ne \emptyset, k \ne i\}.
  \end{align*}

  We notice that for any $u$ having the properties $(a), (b)$ above,
  we must have $C_2(u) = \emptyset$ since if $k \in C_2(u)$ then
  $\exists w \in \anc{i} \cap \anc{k}$ (and
  $w \in \anc{i} \cap \anc{u}$, since $k \in \pa{u}$) such that
  $i \not \in \anc{k}$ and therefore there must be a path
  $\gcgpath{w}{k} \rightarrow u$ which does not contain $i$,
  contradicting property $(b)$.  Moreover, for any
  $u \in \big(\anc{j} \setminus \{i\}\big)$ and $k \in C_0(u)$,
  Proposition \ref{prop:ancestor_uncorrelated} shows that
  $H_t^i \perp \H_{t - 1}^j | \H_{t - 1}^i$.

  In order to establish $j \npwgc i$, choose an arbitrary element of
  $\H_{t - 1}^j$ and represent it via the action of a strictly causal
  filter $\Phi(z)$, i.e.  $\Phi(z) x_j(t) \in \H_{t - 1}^{(j)}$, by
  Theorem \ref{thm:granger_causality_equivalences} it suffices to show
  that

  \begin{equation}
    \label{eqn:sufficient_inner_prod}
    \inner{x_i(t)}{\Phi(z)x_j(t) - \linE{\Phi(z)x_j(t)}{\H_{t - 1}^{(i)}}} = 0.
  \end{equation}

  Denote $e_j \defeq e_j^{S(j)}$, we can write
  $x_j(t) = e_j^\T x_{S(j)}(t)$, and therefore from Equation
  \eqref{eqn:scc_parent_expansion_inverted} there exist strictly
  causal filters $\Gamma_s(z)$ and $\Lambda_{sk}(z)$ (defined for ease
  of notation) such that

  \[
    x_j(t) = \sum_{s \in S(j)} \Gamma_s(z)v_s(t) + \sum_{\substack{s \in S(j) \\ k \in \pa{s} \cap S(j)^\c}} \Lambda_{sk}(z)x_k(t).
  \]

  When we substitute this expression into the left hand side of
  Equation \eqref{eqn:sufficient_inner_prod}, we may cancel each term
  involving $v_s$ by Lemma \ref{lem:vj_perp}, and each $k \in C_0(s)$
  by our earlier argument, leaving us with

  \[
    \sum_{\substack{s \in S(j) \\ k \in C_1(s) \cap S(j)^\c}}\inner{x_i(t)}{\Phi(z)\Lambda_{sk}(z)x_k(t) - \linE{\Phi(z)\Lambda_{sk}(z)x_k(t)}{\H_{t - 1}^{(i)}}}.
  \]

  Since each $k \in C_1(s)$ with $k \ne i$ inherits properties $(a)$
  and $(b)$ above, we can recursively expand each $x_k$ of the above
  summation until reaching $k = i$ (which is garaunteed to terminate
  due to the definition of $C_1(u)$) which leaves us with some
  strictly causal filter $F(z)$ such that the left hand side of
  Equation \eqref{eqn:sufficient_inner_prod} is equal to

  \[
    \inner{x_i(t)}{\Phi(z)F(z)x_i(t) - \linE{\Phi(z)F(z)x_i(t)}{\H_{t - 1}^{(i)}}},
  \]

  and this is $0$ since $\Phi(z)F(z)x_i(t) \in \H_{t - 1}^i$.
  

  % Using the above partition of $\pa{j}$, we can expand $x_j(t)$ in
  % terms of it's parents, and recursively expand nodes in $C_1$
  % (cancelling all nodes in $C_0$, and recalling that
  % $C_2 = \emptyset$) until we reach $i$.

  % Using this partition, we will expand $x_j(t)$ in terms of it's
  % parents, and recursively expand nodes in $C_1$ until reaching $i$.
  % For the first step equation \eqref{eqn:parent_expansion} gives us:

  % \begin{equation}
  %   \label{eqn:xj_partition_expansion}
  %   x_j(t) = \widetilde{\A}_{jj}(z) \Big(v_j(t) + \sum_{k \in C_0(j)} \B_{ik}(z)x_k(t) + \sum_{k \in C_1(j)}\B_{ik}(z)x_k(t)\Big).
  % \end{equation}

  % Substituting (\ref{eqn:xj_partition_expansion}) into
  % (\ref{eqn:sufficient_inner_prod}) we have cancellation of all terms
  % in which $k \in C_0(j)$ by Proposition \ref{prop:ancestor_properties},
  % and cancellation of the term involving $v_j(t)$ by Lemma \ref{lem:vj_perp}.

  % We are therefore left with

  % \begin{equation}
  %   \label{eqn:refwornv}  % An arbitrary code for reference in just one place
  %   \inner{x_i(t)}{\sum_{k \in C_1(j)} \big(\widetilde{\A}_{jj}(z)\Phi(z) \B_{ik}(z)x_k(t) - \linE{\widetilde{\A}_{jj}(z)\Phi(z) \B_{ik}(z)x_k(t)}{\H_{t - 1}^i}\big)}.
  % \end{equation}

  % Since each such $k$ inherits the needed properties $(a), (b)$ we can
  % again (as in Proposition \ref{prop:ancestor_properties}) recursively
  % expand this expression until we reach $i$, obtaining a strictly
  % causal $\Psi(z)$ such that Expression \eqref{eqn:refwornv} is equal to

  % \begin{equation}
  %   \inner{x_i(t)}{\Psi(z)x_i(t) - \linE{\Psi(z)x_i(t)}{\H_{t - 1}^i}} = 0.
  % \end{equation}
\end{proof}

\begin{proposition}
  \label{prop:sc_graph_common_anc}
  In a strongly causal graph if $j \in \anc{i}$ then any
  $k \in \anc{i} \cap \anc{j}$ is not a confounder, that is,
  the unique path from $k$ to $i$ contains $j$.
\end{proposition}
\begin{proof}
  Suppose that there is a path from $k$ to $i$ which does not contain
  $j$.  In this case, there are multiple paths from $k$ to $i$ (one of
  which \textit{does} go through $j$, since $j \in \anc{i}$) which
  contradicts the assumption of strong causality.
\end{proof}

\begin{proposition}
  \label{prop:pwgc_anc}
  If $\gcg$ is a strongly causal DAG then $j \in \anc{i} \Rightarrow j \pwgc i$.
\end{proposition}
\begin{proof}
  We will show that for some $\psi \in \H_{t - 1}^{(j)}$ we have

  \begin{equation}
    \label{eqn:cond_ortho_proof}
    \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0
  \end{equation}

  and therefore that $H_t^{(i)} \not\perp\ \H_{t - 1}^{(j)}\ |\ \H_{t - 1}^{(i)}$, which by theorem (\ref{thm:granger_causality_equivalences}) is enough to establish that $j \pwgc i$.

  Firstly, we will establish a representation of $x_i(t)$ that involves $x_j(t)$.  Denote by $a_{r + 1} \rightarrow a_r \rightarrow \cdots \rightarrow a_1 \rightarrow a_0$ with $a_{r + 1} \defeq j$ and $a_0 \defeq i$ the \textit{unique} $\gcgpath{j}{i}$ path in $\gcg$, we will expand the representation of equation (\ref{eqn:parent_expansion}) backwards along this path:

  \begin{align*}
    x_i(t) &= v_i(t) + \B_{ii}(z) x_i(t) + \sum_{k \in \pa{i}}\B_{ik}(z) x_k(t)\\
           &= \underbrace{v_{a_0}(t) + \B_{a_0a_0}(z) x_i(t) + \sum_{\substack{k \in \pa{a_0} \\ k \ne a_1}}\B_{a_0 k}(z) x_k(t)}_{\defeq \wtalpha{a_0}{a_1}} + \B_{a_0a_1}(z)x_{a_1}(t)\\
           &= \wtalpha{a_0}{a_1} + \B_{a_0a_1}(z)\big[\wtalpha{a_1}{a_2} + \B_{a_1a_2}(z)x_{a_2}(t) \big]\\
           &\overset{(a)}{=} \sum_{\ell = 0}^r \underbrace{\Big(\prod_{m = 0}^{\ell - 1} \B_{a_m a_{m + 1}}(z) \Big)}_{\defeq F_\ell(z)} \wtalpha{a_\ell}{a_{\ell + 1}} + \Big(\prod_{m = 0}^{r}\B_{a_m a_{m + 1}}(z)\Big)x_{a_{r + 1}}(t)\\
           &= \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} + F_{r + 1}(z) x_j(t)
  \end{align*}

  where $(a)$ follows by a routine induction argument and where we define $\prod_{m = 0}^{-1} \bullet \defeq 1$ for notational convenience.

  Using this representation to expand equation (\ref{eqn:cond_ortho_proof}), we obtain the following cumbersome expression:

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}\\
    &- \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\linE{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}{\H_{t - 1}^{(i)}}}\\
    &+ \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z)\wtalpha{a_\ell}{a_{\ell + 1}}}.
  \end{align*}

  Note that by the orthogonality principle, $\psi - \linE{\psi}{\H_{t - 1}^{(i)}} \perp \H_{t - 1}^{(i)}$, the middle term above is $0$.  Choosing now the particular value $\psi = F_{r + 1}(z)x_j(t) \in \H_{t - 1}^{(j)}$ we arrive at

  \begin{align*}
    &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
    &= \E|F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}|^2\\
    &+ \inner{F_{r + 1}(z)x_j(t) - \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}}{\sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}}}.
  \end{align*}

  Now since $F_{r + 1}(z) \ne 0$ by Theorem
  \ref{thm:granger_causality_equivalences}, and
  $F_{r + 1}(z)x_j(t) \not\in \H_{t - 1}^i$, we have by the
  Cauchy-Schwarz inequality that this expression is equal to $0$ if
  and only if

  \begin{equation*}
    \sum_{\ell = 0}^r F_\ell(z) \wtalpha{a_\ell}{a_{\ell + 1}} \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}} - F_{r + 1}(z)x_j(t),
  \end{equation*}

  or by rearranging and applying the representation for $x_i(t)$
  obtained earlier, if and only if

  \begin{equation*}
    x_i(t) \overset{\text{a.s.}}{=} \linE{F_{r + 1}(z)x_j(t)}{\H_{t - 1}^{(i)}}.
  \end{equation*}

  But, this is impossible since $x_i(t) \not \in \H_{t - 1}^{(i)}$.
\end{proof}

\begin{example}
  Consider a process $x(t)$ generated by the $\VAR(1)$
  model\footnote{Recall that any $\VAR(p)$ model with $p < \infty$ can
    be written as a $\VAR(1)$ model, so we lose little generality in
    considering this case.}  having $\B(z) = Bz^{-1}$.  If $B$ is
  diagonalizable, and has at least $2$ distinct eigenvalues, then
  $x(t)$ is persistent.

  Pick any $i \in [n], j \in \anc{i} \setminus \{i\}$.  Then the
  stability of $B$ allows us to write

  \begin{equation*}
    \A(z) = \sum_{k = 0}^\infty B^k z^{-k},
  \end{equation*}

  whereby we see that $\exists k > 0$ such that $[B^k]_{ij} \ne 0$
  (since $j \in \anc{i}$).  Then consider

  \begin{equation*}
    \begin{aligned}
      [B^{rk}]_{ij} &= e_i^\T B^{rk} e_j \\
      &\overset{(a)}{=} \big((P^\T e_i)^\T J^{rk} P^{-1}e_j\big)\\
      &= \tr [(P^\T e_i)^\T J^{rk} P^{-1}e_j]\\
      &\overset{(b)}{=} \tr [(J^{rk}) (v u^\T)],
    \end{aligned}
  \end{equation*}

  where $(a)$ utilizes the Jordan Normal Form of $B$, and $(b)$
  denotes $u = P^\T e_i$ and $v = P^{-1}e_j$.  In order for
  $\tau_\infty(\A_{ij}) < \infty$, there must be some $N > 1$ such
  that $\forall r \ge N$, the above term is $0$.  This may be the case
  for instance if $B$ is a nilpotent matrix.  

  Using the supposition that $B$ is diagonalizable (i.e. $J$ is a
  diagonal matrix) with at least $2$ distinct eigenvalues (in this
  case $B$ is \textit{not} nilpotent), we can then rewrite the above
  as

  \begin{equation*}
    f(r) \defeq \tr [(J^{rk}) (v u^\T)] = \sum_{\nu = 1}^n \lambda_\nu ^{rk} v_\nu u_\nu \defeq \sum_{\nu = 1}^n \lambda_\nu^{rk} \beta_\nu
  \end{equation*}

  where $\lambda_\nu$ denotes the eigenvalues of $B$ and
  $\beta_\nu = u_\nu v_\nu$.  Note that $f(0) = 0$ since $i \ne j$ and
  $u$ is a row of $P$ and $v$ is a column of $P^{-1}$.  Moreover,
  $f(1) \ne 0$ by hypothesis.  But, in order for
  $f(r) = 0\ \forall r \ge N$, it would need to be the case that

  \begin{equation*}
    \Dg(\bm{\lambda})^r \bm{\lambda} = Vz
  \end{equation*}

  had a solution in $z$ for every $r \ge N$, where $V$ is an
  $n \times n - 1$ full-rank matrix whose columns span the nullspace of $\beta$,
  and $\bm{\lambda} = (\lambda_1, \ldots, \lambda_n)$. That is,
  iterates of $\Dg(\bm{\lambda})$ applied to $\bm{\lambda}$ would need to remain
  inside $\beta$'s nullspace.  This would imply that

  \begin{equation*}
    VV^\dagger \bm{\lambda}^{r + 1} = \bm{\lambda}^{r + 1},
  \end{equation*}

  i.e. that $\bm{\lambda}^{r + 1}$ is an eigenvector of $VV^\dagger$
  for an infinite number of integers $r$ (the exponentiation is to be
  understood as a point wise operation).  However, since there can only
  be a finite number of (unit length) eigenvectors, this cannot be the
  case unless every eigenvalue $(\lambda_1, \ldots, \lambda_n)$ were
  equal.

  We see from this example that the collection of $\VAR(1)$ systems
  which are not persistent are pathological, in the sense that their
  system matrices have zero measure when viewed as a subset of $\R^{n^2}$.
\end{example}

\begin{lemma}
  \label{lem:time_lag_cancellation}
  Suppose $v(t)$ is a scalar process with unit variance and zero
  autocorrelation and let $\A(z), \B(z)$ be nonzero and strictly
  causal (i.e. $1 \le \tau_0(\A) < \infty$,
  $1 \le \tau_0(\B) < \infty$) linear filters.  Then,

  \begin{equation}
    \inner{F(z)\A(z)v(t)}{\B(z)v(t)} = 0\ \forall \text{ strictly causal filters } F(z)
  \end{equation}

  if and only if $\tau_0(\A) \ge \tau_\infty(\B)$.
\end{lemma}
\begin{proof}
  We have

  \begin{align}
    \inner{\A(z)v(t)}{\B(z)v(t)} &= \sum_{\tau = 1}^\infty \sum_{s = 1}^\infty a(\tau)b(s)\E[v(t - s)v(t - \tau)]\\
    &= \sum_{\tau = \text{max}(\tau_0(\A), \tau_0(\B))}^{\text{min}(\tau_\infty(\A), \tau_\infty(\B))} a(\tau) b(\tau),\\
  \end{align}

  since $\E[v(t - s)v(t - \tau)] = \delta_{s - \tau}$.  This expression is
  $0$ if and only if $\tau_0(\A) \ge 1 + \tau_\infty(\B)$ or if
  $\tau_0(\B) \ge 1 + \tau_\infty(\A)$ or if the coefficients are orthogonal along
  the common support.

  Specializing this fact to $\inner{F(z)\A(z)v(t)}{\B(z)v(t)}$ we
  see that the coefficients cannot be orthogonal for every choice of
  $F$, and that $\text{sup}_F \tau_\infty(F\A) = \infty$, leaving only
  the possibility that

  \begin{align*}
     \forall F\ \tau_0(F\A) \ge 1 + \tau_\infty(\B) &\overset{(a)}{\iff} \tau_0(\A) \ge 1 + \tau_\infty(\B) - \underset{F}{\text{min }} \tau_0(F)\\
    &\overset{(b)}{\iff} \tau_0(\A) \ge \tau_\infty(\B),
  \end{align*}

  where $(a)$ follows since $\tau_0(F\A) = \tau_0(F) + \tau_0(\A)$,
  and $(b)$ since $\text{min}_F\ \tau_0(F) = 1$.
\end{proof}

\begin{corollary}
  \label{cor:time_lag_cancellation}
  For $k \in \anc{i} \cap \anc{j}$ we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})
  \end{align*}
\end{corollary}
\begin{proof}
  The final equivalence follows immediately from Lemma \ref{lem:time_lag_cancellation}.  For the first equivalence we have

  \begin{align*}
    \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} &= 0\ \forall \text{ strictly causal } F(z)\\
    \iff \inner{F(z)A_{jk}(z)v_k(t)}{x_i(t - \tau)} &= 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{align*}

  which can be expanded by equation \eqref{eqn:ancestor_expansion} to
  obtain (after cancelling all ancestors of $i$ other than $k$)

  \begin{equation*}
    \inner{F(z)A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t - \tau)} = 0\ \forall \tau \ge 1, \text{ strictly causal } F(z),
  \end{equation*}

  which by the Lemma is equivalent to $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$ as stated.
\end{proof}

\begin{proposition}
  \label{prop:persistence_converse}
  Fix $i, j \in [n]$ and suppose $\exists k \in \anc{i} \cap \anc{j}$
  which confounds $i, j$.  Then, if $T_{ij}(z)$ is not causal we have
  $j \pwgc i$, and if $T_{ij}(z)$ is not anti-causal we have
  $i \pwgc j$.  Moreover, if Assumption \ref{ass:T_causality} is
  satisfied, then $j \pwgc i \iff i \pwgc j$.
  % Suppose that Assumption \ref{ass:T_causality} is satisfied.  Then if
  % there exists a $k$ which confounds $(i, j)$ we have
  % $i \pwgc j \implies j \pwgc i$.  Moreover, if $T_{ij}(z)$ is not
  % constant, then $i \pwgc j$.
\end{proposition}

% QUESTION: Does $\gcg$ need to be a strongly causal DAG for this proposition?

% \begin{proposition}
%   \label{prop:persistence_converse}
%   Suppose $\gcg$ is a strongly causal DAG and that $x(t)$ is
%   persistent, then if there exists a $k$ which confounds $(i, j)$ we
%   have $i \pwgc j \implies j \pwgc i$.
% \end{proposition}
\begin{proof}
  Recalling Theorem \ref{thm:granger_causality_equivalences}, consider
  some $\psi \in \H_{t - 1}^j$ and represent it as
  $\psi(t) = F(z)x_j(t)$ for some strictly causal filter $F(z)$.
  Then

  \begin{align*}
    &\inner{\psi(t) - \linE{\psi(t)}{\H_{t - 1}^i}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^i}}\\
    &\overset{(a)}{=} \inner{F(z)x_j(t)}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^i}}\\
    &\overset{(b)}{=} \inner{F(z)\big(\A_{jj}(z)v_j(t) + \sum_{k \in \anc{j}}\A_{jk}(z)v_k(t)\big)}{(1 - H_i(z))\big(\A_{ii}(z)v_i(t) + \sum_{\ell \in \anc{i}}\A_{i\ell}(z)v_\ell(t)\big)}\\
    &\overset{(c)}{=} \sum_{k \in \anc{i}\cap\anc{j}}\inner{F(z)\A_{jk}(z)v_k(t)}{(1 - H_i(z))\A_{ik}(z)v_k(t)},
  \end{align*}

  where $(a)$ applies the orthogonality principle, $(b)$ expands with
  Equation \eqref{eqn:ancestor_expansion} with
  $H_i(z)x_i(t) = \linE{x_i(t)}{H_{t - 1}^i}$, and $(c)$ follows by
  performing cancellations of $v_k(t) \perp v_\ell(t)$ and noting that
  by the contrapositive of Proposition \ref{prop:sc_graph_common_anc}
  we cannot have $i \in \anc{j}$ or $j \in \anc{i}$.

% , $(c)$ is obtained through cancellations from Lemma \ref{lem:vj_perp}, $(d)$ follows since $\H_i$ is self-adjoint, $(e)$ follows by definition of $\H_i$: $\exists H_i(z)$ such that $$H_i(z)x_i(t) = \H_i \sum_{\ell \in \anc{i}}\A_{i\ell}v_\ell(t),$$ $(f)$ follows by again expanding $x_i(t)$ with Equation \eqref{eqn:ancestor_expansion}, and finally $(g)$ by cancelling orthogonal components of $v(t)$.

  Through symmetric calculation, we can obtain the expression relevant
  to the determination of $i \pwgc j$ for $\phi \in \H_{t - 1}^i$ represented by the strictly causal filter $G(z): \phi(t) = G(z)x_i(t)$
  \begin{align*}
    &\inner{\phi(t) - \linE{\phi(t)}{\H_{t - 1}^j}}{x_j(t) - \linE{x_j(t)}{\H_{t - 1}^j}}\\
    &= \sum_{k \in \anc{i} \cap \anc{j}}\inner{G(z)\A_{ik}(z)v_k(t)}{(1 - H_j(z))\A_{jk}(z)v_k(t)},
  \end{align*}

  where $H_j(z)x_j(t) = \linE{x_j(t)}{\H_{t - 1}^j}.$

  We have therefore

  \begin{align}
      &(j \pwgc i): \exists F(z) \text{ s.t. } \sum_{k \in \anc{i} \cap \anc{j}}\inner{F(z)\A_{jk}(z)v_k(t)}{(1 - H_i(z))\A_{ik}(z)v_k(t)} \ne 0,\\
      &(i \pwgc j): \exists G(z) \text{ s.t. } \sum_{k \in \anc{i} \cap \anc{j}}\inner{G(z)\A_{ik}(z)v_k(t)}{(1 - H_j(z))\A_{jk}(z)v_k(t)} \ne 0.
  \end{align}

  The persistence condition, by Corollary
  \ref{cor:time_lag_cancellation}, ensures that for each
  $k \in \anc{i}\cap\anc{j}$ there is some $F(z)$ and some $G(z)$ such
  that at least one of the above terms constituting the sum over $k$
  is non-zero.  It remains to eliminate the possibility of
  cancellation in the sum.

  The adjoint of a linear filter $C(z)$ is simply $C(z^{-1})$, which
  recall is strictly anti-causal if $C(z)$ is strictly causal.  Using
  this, we can write

  \begin{align*}
    &\sum_{k \in \anc{i} \cap \anc{j}}\inner{F(z)\A_{jk}(z)v_k(t)}{(1 - H_i(z))\A_{ik}(z)v_k(t)}\\
    = &\sum_{k \in \anc{i} \cap \anc{j}}\inner{\A_{ik}(z^{-1})(1 - H_i(z^{-1}))F(z)\A_{jk}(z)v_k(t)}{v_k(t)}.\\
  \end{align*}

  Moreover, it is sufficient to find some strictly causal $F(z)$ of
  the form $F(z)(1 - H_j(z))$ (abusing notation) since $1 - H_j(z)$ is
  causal.  Similarly for $G(z)$, this leads to symmetric expressions
  for $j \pwgc i$ and $i \pwgc j$ respectively:

  \begin{equation}
    \label{eqn:T_F}
    \sum_{k \in \anc{i} \cap \anc{j}}\inner{\A_{ik}(z^{-1})(1 - H_i(z^{-1}))F(z)(1 - H_j(z))\A_{jk}(z)v_k(t)}{v_k(t)},
  \end{equation}
  \begin{equation}
    \label{eqn:T_G}
    \sum_{k \in \anc{i} \cap \anc{j}}\inner{\A_{ik}(z^{-1})(1 - H_i(z^{-1}))G(z^{-1})(1 - H_j(z))\A_{jk}(z)v_k(t)}{v_k(t)}.
  \end{equation}

  Recall the filter from Assumption \ref{ass:T_causality}

  \begin{equation}
    T_{ij}(z) = \sum_{k \in \anc{i} \cap \anc{j}} \sigma_k^2\A_{ik}(z^{-1})(1 - H_i(z^{-1}))(1 - H_j(z))\A_{jk}(z).
  \end{equation}

  Since each $v_k(t)$ is uncorrelated through time,
  $\inner{T_{ij}(z)v_k(t)}{v_k(t)} = \sigma_k^2T_{ij}(0)$, and
  therefore we have $j \pwgc i$ if $T_{ij}(z)$ is \textit{not} causal
  and $i \pwgc j$ if $T_{ij}(z)$ it \textit{not} anti-causal.
  Moreover, we have $i \npwgc j$ \textit{and} $j \pwgc i$ if
  $T_{ij}(z)$ is a constant.  Therefore, under Assumption
  \ref{ass:T_causality} $j \pwgc i \iff i \pwgc j$.

  This follows since if $T_{ij}(z)$ is not causal then
  $\exists k > 0$ such that the $z^k$ coefficient of $T_{ij}(z)$ is
  non-zero, and we can choose strictly causal $F(z) = z^{-k}$ such
  that \eqref{eqn:T_F} is non-zero and therefore $j \pwgc i$.
  
  Similarly, if $T_{ij}(z)$ is not anti-causal, then
  $\exists k > 0$ such that the $z^{-k}$ coefficient of $T_{ij}(z)$ is
  non-zero, and we can choose strictly causal $G(z)$ so that
  $G(z^{-1}) = z^k$, and then \refeq{eqn:T_G} is non-zero and therefore
  $i \pwgc j$.
\end{proof}

% This proof is not correct -- there can be cancellation
% \begin{proof}
%   We will show that $j \pwgc i$, the other being symmetric.  First
%   note that by the contrapositive of proposition
%   \ref{prop:sc_graph_common_anc} we cannot have $i \in \anc{j}$ or
%   $j \in \anc{i}$ (else $k$ would not be a confounder) and therefore
%   \textit{every} $u \in \anc{i}\cap\anc{j}$ will be a confounder.

%   It is sufficient to show that $\exists \psi \in \H_{t - 1}^{(j)}$
%   such that

%   \begin{equation*}
%     \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}} \ne 0.
%   \end{equation*}

%   To this end, let $F(z)$ be an arbitrary but strictly causal linear
%   filter.  We apply equation \eqref{eqn:ancestor_expansion} to
%   $x_i(t)$ and $\psi \defeq F(z)x_j(t)$:

%   \begin{align*}
%     &\inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{x_i(t) - \linE{x_i(t)}{\H_{t - 1}^{(i)}}}\\
%     &\overset{(a)}{=} \inner{\psi - \linE{\psi}{\H_{t - 1}^{(i)}}}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
%     &\overset{(b)}{=} \inner{\sum_{k \in \anc{j}}\big(F(z)\A_{jk}(z)v_k(t) \\&- \linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}\big)}{\A_{ii}(z)v_i(t) + \sum_{k \in \anc{i}}\A_{ik}(z)v_k(t)}\\
%     &\overset{(c)}{=} \sum_{k \in \anc{i}\cap\anc{j}}\Big(\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} \\&- \inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{ii}v_i(t)}\\
%     &- \sum_{\ell \in \anc{i}}\inner{\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}}}{\A_{i\ell}(z)v_\ell(t)}\Big)
%   \end{align*}

%   where in $(a)$ we have removed the $\linE{x_i(t)}{\H_{t - 1}^{(i)}}$
%   term via the orthogonality principle, in $(b)$ there is no
%   $F(z)\A_{jj}(z)v_j(t)$ term since due to $j \not\in \anc{i}$ it is
%   orthogonal to $\H_t^{(i)}$.  Finally, $(c)$ follows by applying
%   orthogonality properties of $v(t)$, as well as the fact that
%   $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$ for
%   $k \not \in \anc{i}$.  Note that
%   $\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} \in \H_{t - 1}^{(i)}$
%   and therefore there is in general no cancellation in the final term
%   above for $\ell \in \anc{i}$.

%   \todo{This is clearly not immediately evident}

%   This is $0$ for every $F$ if and only if for all $F$ and
%   $\forall k \in \anc{i} \cap \anc{j}$ we have
%   $$\linE{F(z)\A_{jk}(z)v_k(t)}{\H_{t - 1}^{(i)}} = 0$$ and
%   $$\inner{F(z)\A_{jk}(z)v_k(t)}{\A_{ik}(z)v_k(t)} = 0,$$ which by
%   Corollary \ref{cor:time_lag_cancellation} occurs if and only if
%   $\tau_0(\A_{jk}) \ge \tau_\infty(\A_{ik})$, which is impossible since by persistence
%   $\tau_0(\A_{jk}) < \infty$ and  $\tau_\infty(\A_{ik}) = \infty$.
% \end{proof}

\subsection{The Main Theorem}
\label{apx:proof_main_theorem}

\begin{theorem}[Pairwise Recovery]
  \label{thm:scg_recovery}
  If the Granger-causality graph $\gcg$ for the process $x(t)$ is a
  strongly causal DAG and Assumption \ref{ass:T_causality} holds, then
  $\gcg$ can be inferred from pairwise causality tests.  The procedure
  can be carried out, assuming we have an oracle for pairwise
  causality, via Algorithm (\ref{alg:pwgr}).
\end{theorem}

\begin{algorithm}[H]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Initialize}{initialize}
  \DontPrintSemicolon

  % \BlankLine
  \caption{Pairwise Granger Causality Algorithm (PWGC)}
  \label{alg:pwgr}
  % \TitleOfAlgo{Pairwise Graph Recovery (PWGC)}
  % \Input{Pairwise Granger-causality relations between a persistent
  % process of dimension $n$ whose joint Granger-causality
  % relations are known to form a strongly causal DAG $\gcg$.}
  \Input{Pairwise Granger-causality relations}
  \Output{Edges $\gcge = \{(i, j) \in [n] \times [n]\ |\ i \gc j \}$ of
    the graph $\gcg$.}
  \Initialize{$S_0 = [n]$  \texttt{\# unprocessed nodes}\\
    $E_0 = \emptyset$  \texttt{\# edges of }$\gcg$\\
    % $P_0 = \emptyset$  \texttt{\# layer by layer driving nodes}\\
    $k = 1$ \texttt{\# a counter used only for notation}}
  \BlankLine
  $W \leftarrow \{(i, j)\ |\ i \pwgc j, j \npwgc i\}$  \texttt{\# candidate edges}\\
  $P_0 \leftarrow \{i \in S_0\ |\ \forall s \in S_0\ (s, i) \not\in W\}$  \texttt{\# parentless nodes}\\
  \While{$S_{k - 1} \ne \emptyset$}{
    $S_k \leftarrow S_{k - 1} \setminus P_{k - 1}$ \texttt{\# remove nodes with depth }$k - 1$\\
    $P_k \leftarrow \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W\}$   \texttt{\# candidate children}\\

    $D_{k0} \leftarrow \emptyset$\\
    \For{$r = 1, \ldots, k$} 
    {
      $Q \leftarrow E_{k - 1} \cup \big(\bigcup_{\ell = 0}^{r - 1} D_{k\ell}\big)$ \texttt{\# currently known edges}\\
      $D_{kr} \leftarrow \{(i, j) \in P_{k - r} \times P_k\ |\ (i, j) \in W,\ \text{no } i \rightarrow j \text{ path in } Q\}$
      % $E_k \leftarrow E_{k - 1} \cup D_{kr}$ \texttt{\# add edges to }$E_k$ \label{alg:inner_loop_end}\\
      % $W_{k + 1} \leftarrow W_k \setminus D_{kr}$  \texttt{\# remove edges from consideration}\\
    }
    $E_k \leftarrow E_{k - 1} \cup \big(\bigcup_{r = 1}^k D_{kr}\big)$ \texttt{\# update } $E_k$ \texttt{ with new edges}\\
    % \label{alg_line:inner_loop_end}\\
    % $W_{k + 1} \leftarrow W_k \setminus \big(\bigcup_{r = 0}^k D_{kr}\big)$  \texttt{\# remove edges from consideration}\\
    $k \leftarrow k + 1$
  }
  \Return{$E_{k - 1}$}
\end{algorithm}

Our proof proceeds in 5 steps stated formally as lemmas.  Firstly, we
characterize the sets $W$ and $P_k$.  Then we establish a correctness
result for the inner loop on $r$, a correctness result for the outer
loop on $k$, and finally that the algorithm terminates in a finite
number of steps.

\begin{lemma}[$W$ Represents Ancestor Relations]
  \label{lem:W_subset_E}
  In Algorithm \ref{alg:pwgr} we have
  $(i, j) \in W$ if and only if $i \in \anc{j}$.  In particular,
  $W \subseteq \gcge$.
\end{lemma}
\begin{proof}
  Let $j \in [n]$ and suppose that $i \in \anc{j}$.  Then $i \pwgc j$
  by Proposition \ref{prop:pwgc_anc}.  Proposition
  \ref{prop:sc_graph_common_anc} ensures that $(i, j)$ are not
  confounded and Corollary \ref{cor:parent_corollary} that
  $j \not\in \anc{i}$ so $j \npwgc i$ by Proposition and therefore
  \ref{prop:ancestor_properties} $(i, j) \in W$.

  Conversely, suppose $(i, j) \in W$.  Then since $j \npwgc i$,
  Proposition \ref{prop:persistence_converse} ensures that $(j, i)$
  are not confounded and so by Proposition \ref{prop:ancestor_properties}
  we must have $i \in \anc{j}$.
\end{proof}

\begin{definition}[Depth]
  For our present purposes we will define the \textit{depth} $d(j)$ of
  a node $j$ in $\gcg$ to be the length of the \textit{longest} path
  from a node in $P_0$ to $j$, where $d(j) = 0$ if $j \in P_0$.  It is
  apparent that such a path will always exist.  For example, in Figure
  \ref{fig:example_fig3} we have $d(3) = 1$ and $d(4) = 2$.
\end{definition}

\begin{lemma}[Depth Characterization of $P_k$ and $S_k$]
  \label{lem:depth_lemma}
  $i \in P_k \iff d(i) = k$ and $j \in S_k \iff d(j) \ge k$.
\end{lemma}
\begin{proof}
  We proceed by induction, noting that $P_0$ is non-empty since $\gcg$
  is acyclic and therefore $\gcg$ contains nodes without parents.  The
  base case $i \in P_0 \iff d(i) = 0$ is by definition, and
  $j \in S_0 \iff d(j) \ge 0$ is trivial since $S_0 = [n]$.  So
  suppose that the lemma is true up to $k - 1$.

  ($i \in P_k \implies d(i) = k$): Let $i \in P_k$.  Suppose that
  $d(i) \ge k + 1$, then $\exists j \in \pa{i}$ such that
  $j \not\in \cup_{r \ge 1}P_{k - r}$ (otherwise $d(i) \le k$), this
  implies that $j \in S_k$ with $(j, i) \in W$ (by Lemma
  \ref{lem:W_subset_E}) which is not possible due to the construction of
  $P_k$ and therefore $d(i) \le k$.  Moreover,
  $P_k \subseteq S_k \subseteq S_{k - 1}$ implies that
  $d(i) \ge k - 1$ by the induction hypothesis, but if $d(i) = k - 1$
  then $i \in P_{k - 1}$ again by induction which is impossible since
  $i \in P_k$ and therefore $d(i) = k$.

  ($s \in S_k \implies d(s) \ge k$): Let
  $s \in S_k \subseteq S_{k - 1}$.  We have by induction that
  $d(s) \ge k - 1$, but again by induction (this time on $P_{k - 1}$)
  we have $d(s) \ne k - 1$ since $S_k = S_{k - 1} \setminus P_{k - 1}$
  and therefore $d(s) \ge k$.

  ($d(i) = k \implies i \in P_k$): Suppose $i \in [n]$ is such that
  $d(i) = k$.  Then $i \in S_{k - 1}$ by the hypothesis, but also
  $i \not\in P_{k - 1}$ so then
  $i \in S_k = S_{k - 1} \setminus P_{k - 1}$.  Now, recalling the
  definition of $P_k$

  \begin{equation*}
    P_k = \{i \in S_k\ |\ \forall s \in S_k\ (s, i) \not\in W \},
  \end{equation*}

  if $s \in S_k$ is such that $(s, i) \in W$ then $s \pwgc i$ and
  $i \npwgc s$ so that by Proposition \ref{prop:persistence_converse}
  there cannot be a confounder of $(s, i)$ (otherwise $i \pwgc s$) so
  then by Proposition \ref{prop:ancestor_properties} we have
  $s \in \anc{i}$.  We have shown that $s \in S_k \implies d(s) \ge k$
  and so we must have $d(i) > k$, a contradiction, therefore there is
  no such $s \in S_k$ so $i \in P_k$.

  ($d(j) \ge k \implies j \in S_k$): Let $j \in [n]$ such that
  $d(j) \ge k$, then by induction we have $j \in S_{k - 1}$.  This
  implies by the construction of $S_k$ that $j \not\in S_k$ only if
  $j \in P_{k - 1}$, but we have shown that this only occurs when
  $d(j) = k - 1$, but $d(j) > k - 1$ so $j \in S_k$.
\end{proof}

\begin{lemma}[Inner Loop]
  \label{lem:inner_loop_lemma}
  Fix an integer $k \ge 1$ and suppose that $(i, j) \in E_{k - 1}$ if
  and only if $(i, j) \in \gcge$ and $d(j) \le k - 1$.  Then, we have
  $(i, j) \in D_{kr}$ if and only if $(i, j) \in \gcge $, $d(j) = k$,
  and $d(i) = k - r$.
\end{lemma}
\begin{proof}
  We prove by induction on $r$, keeping in mind the results of Lemmas
  \ref{lem:W_subset_E} and \ref{lem:depth_lemma}.  For the base case,
  let $r = 1$ and suppose that $(i, j) \in \gcge$ with $d(j) = k$ and
  $d(i) = k - 1$.  Then, by Corollary \ref{cor:gc_implies_pwgc}
  $(i, j) \in W$ and by our assumptions on $E_{k - 1}$ there is no
  $\gcgpath{i}{j}$ path in $E_{k - 1}$ and therefore
  $(i, j) \in D_{k1}$.  Conversely, suppose that $(i, j) \in D_{k1}$.
  Then, $d(i) = k - 1$ and $d(j) = k$ which, since
  $(i, j) \in W \implies i \in \anc{j}$ implies that $i \in \pa{j}$
  and $(i, j) \in \gcge$.

  Now, fix $r > 1$ and suppose that the result holds up to $r - 1$.
  Let $(i, j) \in \gcge$ with $d(j) = k$ and $d(i) = k - r$.  Then,
  $(i, j) \in W$ and by induction and strong causality there cannot
  already be an $\gcgpath{i}{j}$ path in
  $E_{k - 1} \cup \big(\bigcup_{\ell = 0}^{r - 1} D_{kr}\big)$,
  therefore $(i, j) \in D_{kr}$.  Conversely, suppose
  $(i, j) \in D_{kr}$.  Then we have $d(i) = k - r$, $d(j) = k$, and
  $i \in \anc{j}$.  Suppose by way of contradiction that
  $i \not\in \pa{j}$, then there must be some $u \in \pa{j}$ such that
  $i \in \anc{u}$.  But, this implies that $d(i) < d(u)$ and by
  induction that $(u, j) \in \bigcup_{\ell = 1}^{r - 1}D_{k\ell}$.
  Moreover, since $d(u) < k$ (otherwise $d(j) > k$) each edge in
  the $\gcgpath{i}{u}$ path must already be in $E_{k - 1}$, and so
  there must be an $\gcgpath{i}{j}$ path in
  $E_{k - 1}\cup\big(\bigcup_{\ell = 0}^{r - 1}D_{kr}\big)$, which is
  a contradiction since we assumed $(i, j) \in D_{kr}$.  Therefore
  $i \in \pa{j}$ and $(i, j) \in \gcge$.
\end{proof}

\begin{lemma}[Outer Loop]
  \label{lem:outer_loop_lemma}
  We have $(i, j) \in E_k$ if and only if $(i, j) \in \gcge$ and
  $d(j) \le k$.  That is, at iteration $k, E_k$ and $\gcge$ agree on
  the set of edges whose terminating node is at most $k$ steps away
  from $P_0$.
\end{lemma}
\begin{proof}
  We will proceed by induction.  The base case $E_0 = \emptyset$ is
  trivial, so fix some $k \ge 1$, and suppose that the lemma holds for
  all nodes of depth less than $k$.

  Suppose that
  $(i, j) \in E_k = E_{k - 1}\cup \big(\bigcup_{r = 1}^k D_{rk}
  \big)$.  Then clearly there is some $1 \le r \le k$ such that
  $(i, j) \in D_{kr}$ so that by Lemma \ref{lem:inner_loop_lemma} we
  have $(i, j) \in \gcge$ and $d(j) = k$.

  Conversely, suppose that $(i, j) \in \gcge$ and $d(j) \le k$.  If
  $d(j) < k$ then by induction $(i, j) \in E_{k - 1} \subseteq E_k$ so
  suppose further than $d(j) = k$.  Since $i \in \pa{j}$ we must have
  $d(i) < k$ (else $d(j) > k$) and again by Lemma
  \ref{lem:inner_loop_lemma} $(i, j) \in \bigcup_{r = 1}^k D_{kr}$
  which implies that $(i, j) \in E_k$.
\end{proof}

\begin{lemma}[Finite Termination]
  Algorithm \ref{alg:pwgr} terminates and returns the set
  $E_{k^\star - 1} = \gcge$ for some $k^\star \le n$.
\end{lemma}
\begin{proof}
  If $n = 1$, the algorithm is clearly correct, returning on the first
  iteration with $E_1 = \emptyset$.  When $n > 1$ Lemma
  \ref{lem:outer_loop_lemma} ensures that $E_k$ coincides with
  $\{(i, j) \in \gcge\ |\ d(j) \le k\}$ and since $d(j) \le n - 1$ for
  any $j \in [n]$ there is some $k^\star \le n$ such that
  $E_{k^\star - 1} = \gcge$.  We must have $S_{k^\star} = \emptyset$
  since $j \in S_{k^\star} \iff d(j) \ge k^\star$ (if $d(j) > k - 1$ then
  $E_{k^\star - 1} \ne \gcge$) and therefore the algorithm terminates.
\end{proof}

\section{Finite Sample Implementation}
\label{sec:structure_learning}
In this section we provide a review of our methods for implementing
Algorithm 1 given a \textit{finite} sample of $T$ data points.  We
apply the simplest reasonable methods in order to maintain a focus on
our main contributions (i.e. Algorithm \ref{alg:pwgr}), more
sophisticated schemes can only serve to improve the results.  Textbook
reviews of the following concepts are provided e.g. by
\cite{all_of_statistics}, \cite{murphy_mlp}, and elsewhere.

In subsection \ref{sec:pairwise_hypothesis_testing} we define pairwise
Granger-causality hypothesis tests, in subsection
\ref{sec:model_order_selection} a model order selection criteria, in
subsection \ref{sec:efficient_model_estimation} an efficient
estimation algorithm, in subsection \ref{sec:error_rate_control} the
method for choosing an hypothesis testing threshold, and finally in
subsection \ref{sec:finite_pwgc} the unified finite sample algorithm.

\subsection{Pairwise Hypothesis Testing}
\label{sec:pairwise_hypothesis_testing}
In performing pairwise checks for Granger-causality $x_j \pwgc x_i$ we
follow the simple scheme of estimating the following two linear models:

\begin{align}
  H_0:&\ \widehat{x}_i^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau),\\
  H_1:&\ \widehat{x}_{i|j}^{(p)}(t) = \sum_{\tau = 1}^{p} b_{ii}(\tau)x_i(t - \tau) + \sum_{\tau = 1}^pb_{ij}(\tau)x_j(t - \tau).
\end{align}

We formulate the statistic 

\begin{equation}
  \label{eqn:gc_statistics}
  F_{ij}(p) = \frac{T}{p}\Big(\frac{\xi_i(p)}{\xi_{ij}(p)} - 1\Big),
\end{equation}

where $\xi_i(p)$ is the sample mean square of the
residuals\footnote{This quantity is often denoted $\widehat{\sigma}$,
  but we maintain notation from Definition
  \ref{def:granger_causality}.}  $x_i(t) - \widehat{x}^{(p)}_i(t)$,

\begin{equation*}
  \xi_i(p) = \frac{1}{T - p}\sum_{t = p + 1}^T (x_i(t) - \widehat{x}_i^{(p)}(t))^2,
\end{equation*}

and similarly for $\xi_{ij}(p)$.  We test $F_{ij}(p)$ against a
$\chi^2(p)$ distribution.

If the estimation procedure is consistent, we will have the following
convergence (in $\P$ or a.s.):

\begin{equation}
  F_{ij}(p) \rightarrow
  \left\{
    \begin{array}{ll}
      0;\ x_j \npwgc x_i\\
      \infty;\ x_j \pwgc x_i
    \end{array}
  \right. \text{ as } T \rightarrow \infty.  % the '.' after \right is necessary.
\end{equation}

In our finite sample implementation (see Algorithm
\ref{alg:finite_pwgc}) we add edges to $\widehat{\gcg}$ in order of
the decreasing magnitude of $F_{ij}$ instead of proceeding backwards
through $P_{k - r}$ in Algorithm \ref{alg:pwgr}.  This makes greater
use of the information provided by the test statistic $F_{ij}$,
moreover, if $x_i \gc x_j$ and $x_j \gc x_k$, it is expected that
$F_{kj} > F_{ki}$, thereby providing the same effect as proceeding
backwards through $P_{k - r}$.
\subsection{Model Order Selection}
\label{sec:model_order_selection}
There are a variety of methods to choose the filter order $p$ (see
e.g. \cite{lutkepohl2005new}), but we will focus in particular on the
Bayesian Information Criteria (BIC).  The BIC is substantially more
conservative than the popular alternative Akaiake Information Criteria
(the BIC is also asymptotically consistent), and since we are
searching for \textit{sparse graphs}, we therefore prefer the BIC,
where we seek to \textit{minimize} over $p$:

\begin{equation}
  \label{eqn:bic}
  \begin{aligned}
    BIC_{\text{univariate}}(p) &= \ln\ \xi_i(p) + p\frac{\ln T}{T},\\
    BIC_{\text{bivariate}}(p) &= \ln \det \widehat{\Sigma}_{ij}(p) + 4p\frac{\ln T}{T},\\
  \end{aligned}
\end{equation}

where $\widehat{\Sigma}_{ij}(p)$ is the $2 \times 2$ residual
covariance matrix for the $\VAR(p)$ model of $(x_i(t), x_j(t))$.  The
bivariate errors $\xi_{ij}(p)$ and $\xi_{ji}(p)$ are the diagonal
entries of $\widehat{\Sigma}_{ij}(p)$.

We carry this out by a simple direct search on each model order
between $0$ and some prescribed $p_\text{max}$, resulting in a
collection $p_{ij}$ of model order estimates.  In practice, it is
sufficient to pick $p_\text{max}$ ad-hoc or via some simple heuristic
e.g. plotting the sequence $BIC(p)$ over $p$, though it is not
technically possible to guarantee that the optimal $p$ is less than
the chosen $p_\text{max}$ (since there can in general be arbitrarily
long lags from one variable to another).

\subsection{Efficient Model Estimation}
\label{sec:efficient_model_estimation}
In practice, the vast majority of computational effort involved in
implementing our estimation algorithm is spent calculating the error
estimates $\xi_i(p_i)$ and $\xi_{ij}(p_{ij})$.  This requires fitting a
total of $n^2p_{\text{max}}$ autoregressive models, where the most
naive algorithm (e.g. solving a least squares problem for each model)
for this task will consume $O(n^2p_{\text{max}}^4T)$ time, it is
possible to carry out this task in a much more modest
$O(n^2p_{\text{max}}^2 ) + O(n^2p_{\text{max}}T)$ time via the
autocorrelation method
\cite{hayes_statistical_digital_signal_processing} which substitutes
the following autocovariance estimates in the Yule-Walker
equations:\footnote{The particular indexing and normalization given in
  equation \ref{eqn:covariance_estimate} is critical to ensure
  $\widehat{R}$ is positive semidefinite.  The estimate can be viewed
  as calculating the covariance sequence of a signal multiplied by a
  rectangular window.}

\begin{equation}
  \label{eqn:covariance_estimate}
  \widehat{R}_x(\tau) = \frac{1}{T}\sum_{t = \tau + 1}^T x(t) x(t - \tau)^\T;\ \tau = 0, \ldots, p_{\text{max}},
\end{equation}

It is imperative that the first index in the summation is $\tau + 1$, as
opposed perhaps to $p_\text{max}$ and that the normalization is
$1 / T$, as opposed perhaps to $1 / (T - p_\text{max})$, in order to
guarantee that $\widehat{R}_x(\tau)$ forms a valid (i.e. positive
definite) covariance sequence.  This results in some bias, however the
dramatic computational speedup is worth it for our purposes.

These covariance estimates constitute the $O(n^2p_{\text{max}}T)$
operation.  Given these particular estimates, the variances $\xi_i(p)$
for $p = 1, \ldots, p_{\text{max}}$ can be evaluated in
$O(p_{\text{max}}^2)$ time each by applying the Levinson-Durbin
recursion to $\widehat{R}_{ii}(\tau)$, which effectively estimates a
sequence of $AR$ models, producing $\xi_i(p)$ as a side-effect (see
\cite{hayes_statistical_digital_signal_processing} and
\cite{levinson_durbin_recursion}).

Similarly, the variance estimates $\widehat{\Sigma}_{ij}(p)$ (which
include $\xi_{ij}$ and $\xi_{ji}$) can be obtained by estimating
$\frac{(n + 1)n}{2}$ bivariate AR models, again in
$O(p_{\text{max}}^2)$ time via Whittle's generalized Levinson-Durbin
recursion\footnote{We have made use of standalone tailor made
  implementations of these algorithms, available at
  \textsf{github.com/RJTK/Levinson-Durbin-Recursion}.}
\cite{whittle_generalized_levinson_durbin}.

\subsection{Edge Probabilities and Error Rate Controls}
\label{sec:error_rate_control}
Denote $F_{ij}$ the Granger-causality statistic of equation
\ref{eqn:gc_statistics} with model orders chosen by the methods of
Section \ref{sec:model_order_selection}.  We assume that this
statistic is asymptotically $\chi^2(p_{ij})$ distributed (e.g. the
disturbances are Gaussian), and denote by $G$ the cumulative
distribution function thereof.  We will define the matrix

\begin{equation}
  \label{eqn:edge_inclusion_probability}
  P_{ij} = G(F_{ij}),
\end{equation}

to be the matrix of pairwise edge inclusion P-values.  This is
motivated by the hypothesis test where the hypothesis $H_0$ will be
rejected (and thence we will conclude that $x_j \pwgc x_i$) if
$P_{ij} > 1 - \delta$.

The value $\delta$ can be chosen by a variety of methods, in our case
we apply the Benjamini Hochberg criteria \cite{benjamini_hochberg}
\cite{all_of_statistics} to control the false discovery rate of
pairwise edges to a level $\alpha$ (where we generally take
$\alpha = 0.05$).

\subsection{Finite Sample Recovery Algorithm}
\label{sec:finite_pwgc}

After the graph topology $\widehat{\gcg}$ has been estimated via
Algorithm \ref{alg:finite_pwgc}, we refit the entire model with the
specified sparsity pattern directly via ordinary least squares.

We note that producing graph estimates which are not strongly causal
can potentially be achieved by performing sequential estimates
$\widehat{x}_1(t), \widehat{x}_2(t), \ldots$ estimating a strongly causal
graph with the residuals of the previous model as input, and then
refitting on the combined sparsity pattern.  We intend to consider
this heuristic in future work.

\begin{algorithm}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \SetKwInOut{Initialize}{initialize}
    \DontPrintSemicolon

    \BlankLine
    \caption{Finite Sample Pairwise Graph Recovery (PWGC)}
    \label{alg:finite_pwgc}

    \Input{Estimates of pairwise Granger-causality statistics $F_{ij}$
      (eqn \ref{eqn:gc_statistics}).  Matrix of edge probabilities $P_{ij}$ (eqn \ref{eqn:edge_inclusion_probability}).  Hypothesis testing threshold $\delta$ chosen via the Benjamini-Hochberg criterion (Section \ref{sec:error_rate_control})}
    \Output{A strongly causal graph $\widehat{\gcg}$}
    \Initialize{$S = [n]$  \texttt{\# unprocessed nodes}\\
      $E = \emptyset$  \texttt{\# edges of }$\widehat{\gcg}$\\
      $k = 1$ \texttt{\# a counter used only for notation}}

    \BlankLine

    $W_\delta \leftarrow \{(i, j)\ |\ P_{ji} > 1 - \delta, F_{ji} > F_{ij}\}$  \texttt{\# candidate edges}\\
    $\mathcal{I}_0 \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# total node incident probability}\\
    $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) < \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Nodes with fewest incident edges}\\
    \If{$P_0 = \emptyset$}{
      $P_0 \leftarrow \{i \in S\ |\ \mathcal{I}_0(i) \le \ceil{\text{min}(\mathcal{I}_0)}\}$ \texttt{\# Ensure non-empty}
    }
    \BlankLine

    \While{$S \ne \emptyset$}{
      $S \leftarrow S \setminus P_{k - 1}$ \texttt{\# remove processed nodes}\\
      % $\mathcal{I}_k \leftarrow \big(\sum_{j \in S: (j, i) \in W_\delta} F_{ij}, \mathsf{\ for\ }i \in S \big)$ \texttt{\# intra-}$S$ \texttt{incident strength}\\
      $\mathcal{I}_k \leftarrow \big(\sum_{j\in S: (j, i) \in W_\delta} P_{ij}, \mathsf{\ for\ }i \in S \big)$\\
      $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) < \ceil{\text{min}(\mathcal{I}_k)}\}$\\
      \If{$P_k = \emptyset$}{
        $P_k \leftarrow \{i \in S\ |\ \mathcal{I}_k(i) \le \ceil{\text{min}(\mathcal{I}_k)}\}$
      }
      \;
      \texttt{\# add strongest edges, maintaining strong causality}\\
      $U_k \leftarrow \bigcup_{r = 1}^k P_{k - r}$ \texttt{\# Include all forward edges}\\
      \For{$(i, j) \in \mathsf{sort}\Big(\{(i, j) \in U_k \times P_k\ |\ (i, j) \in W_\delta\} \mathsf{\ by\ descending\ } F_{ji}\Big)$} {
        \If{$\mathsf{is\_strongly\_causal}(E \cup \{(i, j)\})$} {
          \texttt{\# }$\mathsf{is\_strongly\_causal}$ \texttt{can be implemented by keeping}\\
          \texttt{\# track of ancestor / descendant relationships}\\
          $E \leftarrow E \cup \{(i, j)\}$
        }
      }
      $k \leftarrow k + 1$\\
    }
    \Return{$([n], E)$}
\end{algorithm}

\section{Simulation}
\label{apx:simulation}
We have implemented our empirical experiments in Python \cite{scipy},
in particular we leverage the LASSO implementation from
\texttt{sklearn} \cite{sklearn} and the random graph generators from
\texttt{networkx} \cite{networkx}.  We run experiments using two
separate graph topologies having $n = 50$ nodes. These are generated
respectively by drawing a random tree and a random Erdos Renyi graph
then creating a directed graph by directing edges from lower numbered
nodes to higher numbered nodes.

We populate each of the edges (including self loops) with random
linear filters constructed by placing $5$ transfer function poles
(i.e. $p = 5$) uniformly at random in a disc of radius $3 / 4$ (which
guarantees stability for acyclic graphs).  The resulting system is
driven by i.i.d. Gaussian random noise, each component having random
variance $\sigma_i^2 = 1/2 + r_i$ where $r_i \sim \text{exp}(1/2)$.  To ensure
we are generating data from a stationary system, we first discard
samples during a long burnin period.

For both PWGC and adaLASSO We set the maximum lag length
$p_{\text{max}} = 10$.

Results are collected in Figures
\ref{fig:simulation_results_comparison1},
\ref{fig:simulation_results_comparison2},
\ref{fig:simulation_results_scaling_and_small_T},
\ref{fig:simulation_results_dense}.

\begin{figure}
  \centering
  \caption{PWGC Compared Against AdaLASSO \cite{adaptive_lasso_zou2006} (SCG)}
  \label{fig:simulation_results_comparison1}
  \includegraphics[width=0.95\textwidth]{new_lasso_comparison_scg_pmax15_simulation.pdf}

  {\footnotesize Comparison of PWGC and LASSO for $\VAR(p)$ model
    estimation.  We make comparisons against both the MCC and the
    relative log mean-squared prediction error
    $\frac{\ln\tr \widehat{\Sigma}_v}{\ln\tr \Sigma_v}$.  Results
    in Figure \ref{fig:simulation_results_comparison1} are for systems
    guaranteed to satisfy the assumptions required for Theorem
    \ref{thm:scg_recovery}.}
\end{figure}

\begin{figure}
  \caption{PWGC vs adaLASSO (DAG, $q = \frac{2}{n}$)}
  \label{fig:simulation_results_comparison2}
  \includegraphics[width=0.95\textwidth]{new_lasso_comparison_dag_pmax15_simulation.pdf}

  {\footnotesize Figure \ref{fig:simulation_results_comparison2}
    provides results for systems which do not guarantee the
    assumptions of Theorem \ref{thm:scg_recovery}, though the graph
    has a similar level of sparsity.}
\end{figure}

\begin{figure}
  \centering
  \caption{PWGC Scaling and Small Sample Performance}
  \label{fig:simulation_results_scaling_and_small_T}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{Fixed $T$, increasing $n$ (SCG)}
    \label{fig:simulation_results_scaling}
    \includegraphics[width=\linewidth]{new_increasing_n_simulation.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \caption{MCC Comparison for $T \le 100$}
    \label{fig:small_T_comparison}
    \includegraphics[width=\linewidth]{new_mcc_comparison001.pdf}
  \end{subfigure}

  {\footnotesize Figure \ref{fig:simulation_results_scaling} measures
    support recovery performance as the number of nodes $n$ increases,
    and the edge proportion as well as the number of samples $T$ is
    held fixed.  Remarkably, the degradation as $n$ increases is
    limited, it is primarily the graph topology (SCG or non-SCG) as
    well as the level of sparsity (measured by $q$) which are the
    determining factors for support recovery performance.

    Figure \ref{fig:small_T_comparison} provides a support recovery
    comparison for very small values of, $T$ typical for many
    applications.}
\end{figure}

\begin{figure}
  \caption{Fixed $T, n$, increasing edges $q$ (DAG)}
  \label{fig:simulation_results_dense}
  \includegraphics[width=\linewidth]{dag_increasing_q_small_T_alasso_simulation.pdf}

  {\footnotesize Figure \ref{fig:simulation_results_dense} provides a
    comparison between PWGC and AdaLASSO as the density of graph edges
    (as measured by $q$) increases.  For reference,
    $\frac{2}{n} = 0.04$ has approximately the same level of sparsity
    as the SCGs we simulated.  As $q$ increases, the AdaLASSO
    outperforms PWGC as measured by the MCC.  However, PWGC maintains
    superior performance for 1-step-ahead prediction.  We speculate
    that this is a result of fitting the sparsity pattern recovered by
    PWGC via OLS which directly seeks to optimize this metric, whereas
    the LASSO is encumbered by the sparsity inducing penalty.}
\end{figure}

In reference to figure \ref{fig:simulation_results_comparison1} it
should not be overly surprising that our PWGC algorithm performs
better than the LASSO for the case of a strongly causal graph, since
in this case the theory from which our heuristic derives is valid.
However, the performance is still markedly superior in the case of a
more general DAG.  We would conjecture that a DAG having a similar
degree of sparsity as an SCG is ``likely'' to be ``close'' to an SCG,
in some appropriate sense.

Figure \ref{fig:simulation_results_dense} illustrates the severe
(expected) degradation in performance as the number of edges increases
while the number of data samples $T$ remains fixed.  For larger values
$q$ in this plot, the number of edges in the graph is comparable to
the number of data samples.

We have also paid close attention to the performance of PWGC in the
very small sample ($T \le 100$) regime (see Figure
\ref{fig:small_T_comparison}), as this is the regime many applications
must contend with.

In regards scalability, we have observed that performing the $O(n^2)$
pairwise Granger-causality calculations consumes the vast majority
($> 90\%$) of the computation time.  Since this step is trivially
parallelizable, our algorithm also scales well with multiple cores or
multiple machines.  Figure \ref{fig:simulation_results_scaling} is a
demonstration of this scalability, where we are able to estimate
graphs having over $1500$ nodes (over $2.25 \times 10 ^6$ possible edges)
using only $T = 500$ data points, granted, an SCG on this many nodes
is extremely sparse.

\clearpage
% \bibliography{\string~/Documents/academics/global_academics/global_bib}
\bibliography{paper_bib}
\end{document}
